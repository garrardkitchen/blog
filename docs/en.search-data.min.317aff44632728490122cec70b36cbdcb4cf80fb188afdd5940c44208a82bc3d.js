'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/posts/permission-denied-while-trying-to-connect-to-the-docker-daemon-socket/','title':"Permission Denied While Trying to Connect to the Docker Daemon Socket",'content':"Out of the blue today, first day back after Christmas break, I got this when running a GH Actions Workflow on one of our Self-Hosted linux Runners üò±:\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \u0026ldquo;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json\u0026quot;: dial unix /var/run/docker.sock: connect: permission denied\r\rWe have several GitHub Self-Hosted Runners running on Linux and Windows O/S that produce, amongst other artefacts, Linux and Windows images. These images are pushed to ACR. We\u0026rsquo;re in the process of migrating our on-premise real-estate - IIS Web apps, Windows Services, Docker swarm containers - to AKS as well as migrating our SQL Server AG to Azure. We\u0026rsquo;re using Self-Hosted Runners as we have spare compute capacity and some of our applications have a dependency on a legacy NuGet server which requires our CI pipelines to run in our network. We are in the process of also migrating these legacy packages to our Azure DevOps NuGet Feed as part of our modernization initiative. This modernization initiative encompasses upgrading our runtimes to .NET Framework 4.8 and .NET 6.0.\r\rIt had been running fine prior to my break so what gives? I started to investigate\u0026hellip;\nI logged in linux VM with same creds I used when I installed the self-hosted runner. I used the following command just to confirm the same outcome:\ndocker ps Yup, same thing.\nFirst thing I wanted to check was that this user was a member of the docker group so I entered:\nsudo groups \u0026lt;user\u0026gt; Ah, that\u0026rsquo;s odd. This user wasn\u0026rsquo;t a member and therefore begs the question, how did this ever work in the first place?!!\nI added this user using this command:\nsudo usermod -a -G docker \u0026lt;user\u0026gt; I ran docker ps again but still now dice. ü§î.\nI then checked the status of docker service using this command:\nsudo systemctl status docker It reported:\nActive: active (running) since Thu 2021-09-16 14:13:04 UTC; 3 months 20 days ago\r\rOk, what next? ü§î\nI decided to restart the self-hosted service so I entered these commands:\ncd actions-runner sudo ./svc.sh start This is when I saw these failures:\nDec 19 22:01:15 redacted runsvc.sh[291703]: 2021-12-19 22:01:15Z: Runner connect error: The HTTP request timed out after 00:01:00.. Retrying unt‚Ä¶econnected.\nDec 19 22:02:35 redacted runsvc.sh[291703]: 2021-12-19 22:02:35Z: Runner reconnected.\nJan 06 14:42:21 redacted runsvc.sh[291703]: 2022-01-06 14:42:21Z: Running job: deploy\nJan 06 14:42:41 redacted runsvc.sh[291703]: 2022-01-06 14:42:41Z: Job deploy completed with result: Failed\nJan 06 14:46:19 redacted runsvc.sh[291703]: 2022-01-06 14:46:19Z: Running job: deploy\n\rI restarted the self-hosted runner using these commands:\nsudo ./svc.sh stop sudo ./svc.sh start Then I logged out and back in again to confirm docker access docker ps and finished off by re-running failed GH Action Workflow. ü•≥ Equilibrium is once again restored. As per protocol, I shared issue and resolution with our IT Team in case this crops up again when I\u0026rsquo;m not online to help.\n"});index.add({'id':1,'href':'/posts/my-first-outing-with-dapr/','title':"My First Outing With Dapr",'content':"TL;DR: Not as forgiving as I\u0026rsquo;d have liked \u0026hellip;\nChallenge #1 This took a little longer than I\u0026rsquo;d have liked!\nI was using the internal DNS to resolve the port of my redis service. My Redis single instance was deployed via a deployment manifest, along with a LoadBalancer Service - purely to give me remote access.\nI\u0026rsquo;d first create a secret, by typing:\n$ kubectl create secret generic db-passwords --from-literal=redis-password=\u0026#39;\u0026lt;password\u0026gt;\u0026#39; This is the deployment manifest:\napiVersion: v1 kind: Service metadata: name: redis-svc namespace: meetup-dapr-demo labels: run: redis-svc spec: type: LoadBalancer ports: - port: 6379 targetPort: 6379 protocol: TCP selector: run: redis --- apiVersion: apps/v1 # for k8s versions before 1.9.0 use apps/v1beta2 and before 1.8.0 use extensions/v1beta1 kind: Deployment metadata: name: redis namespace: meetup-dapr-demo spec: selector: matchLabels: run: redis replicas: 1 template: metadata: labels: run: redis spec: containers: - name: cache image: redis args: [\u0026#34;redis-server\u0026#34;, \u0026#34;--requirepass\u0026#34;, $(PASSWORD) ] env: - name: PASSWORD valueFrom: secretKeyRef: name: db-passwords key: redis-password resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 This deployed correctly.\nI then deployed my Dapr state store component:\napiVersion: dapr.io/v1alpha1 kind: Component metadata: name: mystore namespace: meetup-dapr-demo spec: type: state.redis metadata: - name: redisHost value: redis.meetup-dapr-demo.svc.cluster.local:6379 - name: redisPassword value: \u0026#34;********\u0026#34; However, I could not for the life of me give my application access to the state store!\n$ dapr logs -a http-api -k -n meetup-dapr-demo ... time=\u0026#34;2020-11-06T09:47:02.218770653Z\u0026#34; level=error msg=\u0026#34;process component mystore error, redis store: error connecting to redis at redis.meetup-dapr-demo.svc.cluster.local:6379: dial tcp: lookup redis.meetup-dapr-demo.svc.cluster.local on 10.0.0.10:53: no such host\u0026#34; app_id=http-api instance=http-api-6bc44f8957-q2lvn scope=dapr.runtime type=log ver=0.11.3 Having trying every permutation known to non-gender-specific-person-entity I remembered I was kaing it available behind a service. So, I\u0026rsquo;d been using redis.meetup-dapr-demo.svc.cluster.local:6379 when I should have used  redis-svc.meetup-dapr-demo.svc.cluster.local:6379.\nOnce I\u0026rsquo;d corrected my mistake, it connected without error.\napiVersion: dapr.io/v1alpha1 kind: Component metadata: name: mystore namespace: meetup-dapr-demo spec: type: state.redis metadata: - name: redisHost value: redis-svc.meetup-dapr-demo.svc.cluster.local:6379 - name: redisPassword value: \u0026#34;********\u0026#34; Challenge #2 secrets!\nYou\u0026rsquo;re application is going to report something similar to this - NOAUTH Authentication required - if you\u0026rsquo;re Dapr is deployed to a different namespace to that of your application:\ntime=\u0026#34;2020-11-06T11:19:06.985273661Z\u0026#34; level=error msg=\u0026#34;process component mystore error, redis store: error connecting to redis at redis-svc.meetup-dapr-demo.svc.cluster.local:6379: NOAUTH Authentication required.\u0026#34; app_id=http-api instance=http-api-7d49cf59d5-9blwf scope=dapr.runtime type=log ver=0.11.3 To circumvent this, you must create a role and binding this to the default ServiceAccount. This role secret-reader allows a get of the secrets resource within the meetup-depr-demo namespace. An example manifest is here:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: secret-reader namespace: meetup-dapr-demo rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: dapr-secret-reader namespace: meetup-dapr-demo subjects: - kind: ServiceAccount name: default roleRef: kind: Role name: secret-reader apiGroup: rbac.authorization.k8s.io Once deployed, you\u0026rsquo;ll see something similar to this in your dapr logs:\ntime=\u0026#34;2020-11-06T11:23:20.529658232Z\u0026#34; level=info msg=\u0026#34;component loaded. name: mystore, type: state.redis\u0026#34; app_id=http-api instance=http-api-7d49cf59d5-kszdz scope=dapr.runtime type=log ver=0.11.3  This post was created some time ago. Now, we\u0026rsquo;re using the Secrets Store CSI Driver to map Azure KeyVault secrets to containers running in our AKS clusters.\nRef: https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver\n\r"});index.add({'id':2,'href':'/posts/how-to-use-kubernetes-configmap/','title':"How to Use Kubernetes Configmap",'content':"There\u0026rsquo;s a ton of material out there on how to use a ConfigMap. In this post I will provide a recap on the basics then I drill into how to protect your secrets!\nThere are a few ways to create a configMap. Here, I cover just two of these ways;--from-env-file and \u0026ndash;from-literal. I won\u0026rsquo;t cover options like from volume.\nHow to create a ConfigMap from a literal To create a configMap from literals and from the command line, you would type this:\n$ kubectl create configmap config-demo-lit --from-literal=user.name=garrardkitchen --from-literal=user.type=admin To confirm the values, you would type this:\nkubectl get cm config-demo-lit -o yaml apiVersion: v1 data: user.name: garrardkitchen user.type: admin kind: ConfigMap metadata: creationTimestamp: \u0026#34;2020-11-02T16:06:30Z\u0026#34; name: config-demo-lit namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-lit uid: ****  How to create a ConfigMap from an .env file From the command line To create a configMap from the command line, you would type this:\n$ kubectl create configmap demo-config --from-env-file=config/.env.prod To confirm the values, you would type this:\n$ kubectl cm config-demo-1 -o yaml apiVersion: v1 data: foo: baa name: garrard kind: ConfigMap metadata: creationTimestamp: \u0026#34;2020-11-02T15:44:52Z\u0026#34; name: config-demo-1 namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-1 uid: **** üëÜ cm is shorthand for configmap\nFrom a Kubernetes Manifest file To create a configMap from a manifest, you would create a yml|yaml file using the kind: ConfigMap like this:\napiVersion: v1 kind: ConfigMap metadata: name: config-demo-2 namespace: dapr-demo data: foo: baa name: garrard  To confirm the values, you would type this:\n$ kubectl cm config-demo-2 -o yaml apiVersion: v1 data: foo: baa name: garrard kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;foo\u0026#34;:\u0026#34;baa\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;garrard\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;config-demo-2\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;dapr-demo\u0026#34;}} creationTimestamp: \u0026#34;2020-11-02T15:49:48Z\u0026#34; name: config-demo-2 namespace: dapr-demo resourceVersion: ***** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-2 uid: **** How to use this in a pod Here, I\u0026rsquo;m setting up environment variables from different ConfigMaps. config-demo-2 is set up from manifest file and config-demo-lit is set up from literals.\nThis is an example pod manifest called pod-demo.yml\napiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-cache image: k8s.gcr.io/busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34;] env: - name: NAME valueFrom: configMapKeyRef: name: config-demo-2 key: name - name: ROLE valueFrom: configMapKeyRef: name: config-demo-lit key: user.type restartPolicy: Never All that this üëÜ does, is output to STDOUT, a list of environment variables.\nTo apply this manifest, type:\nkubectl.exe apply -f .\\pod-demo.yml To confirm the 2 environment variables have been set, type:\n$ kubectl logs test-pod ... HOSTNAME=test-pod NAME=garrard ROLE=admin ... How to use Secrets TBC\nHow to stop people from finding out your secrets. At the end of the day, the secrets are only Base64 encoded. Anyone with the appropriate level of permissions will be able to see your secrets. One way to stop users from seeing your secrets is by only allow particular groups of people access.\nTo create a secret, type:\n$ kubectl create secret generic db-passwords --from-literal=mongodb-password=\u0026#39;mypassword\u0026#39; To see what secrets we have, type:\n$ kubectl get secrets NAME TYPE DATA AGE dapr-operator-token-mgdqs kubernetes.io/service-account-token 3 2d13h dapr-sidecar-injector-cert Opaque 2 2d13h dapr-trust-bundle Opaque 3 2d13h dashboard-reader-token-j9rcg kubernetes.io/service-account-token 3 2d13h db-passwords Opaque 1 5s default-token-xl2rz kubernetes.io/service-account-token 3 2d14h sh.helm.release.v1.dapr.v1 helm.sh/release.v1 1 2d13h To see the actual password, type:\n$ kubectl get secrets db-passwords -o yaml apiVersion: v1 data: mongodb-password: bXlwYXNzd29yZA== kind: Secret metadata: creationTimestamp: \u0026#34;2020-11-03T10:00:14Z\u0026#34; name: db-passwords namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/secrets/db-passwords uid: **** This is just a Base64 encoded string of mypassword. This is not secure enough. We need another way of to protect our sensitive information/passwords.\r\rSo, what do we do?\nHere\u0026rsquo;s a link to how Kubernetes deals with secrets\nTo use this secret with a deployment, save this to aks-deploy-mongodb-demo.yml:\nPlease note, this is not a production configuration\napiVersion: v1 kind: Service metadata: name: mongodb-svc namespace: dapr-demo labels: run: mongodb-svc spec: type: LoadBalancer ports: - port: 27017 targetPort: 27017 protocol: TCP  selector: run: mongodb --- apiVersion: apps/v1 # for k8s versions before 1.9.0 use apps/v1beta2 and before 1.8.0 use extensions/v1beta1 kind: Deployment metadata: name: mongodb namespace: dapr-demo spec: selector: matchLabels: run: mongodb replicas: 1 template: metadata: labels: run: mongodb  spec: containers: - name: mongodb  image: mongo resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 27017 env: - name: MONGO_USERNAME valueFrom: configMapKeyRef: name: config-demo-lit key: user.name  - name: MONGO_INITDB_ROOT_USERNAME valueFrom: configMapKeyRef: name: config-demo-lit key: user.name  - name: MONGO_INITDB_ROOT_PASSWORD valueFrom: secretKeyRef: name: db-passwords key: mongodb-password  - name: MONGO_DBNAME value: \u0026#34;orders\u0026#34; To deploy the above üëÜ, type this:\n$ kubectl apply -f .\\aks-deploy-mongodb-demo.yml "});index.add({'id':3,'href':'/posts/nodejs-container-restart-policy/','title':"Nodejs Container Restart Policy",'content':"If by accident to deploy a solution using the Node.js Cluster API and do not fork exited processes then the following docker-compose restart_policy will not help you:\ndeploy: restart_policy: condition: on-failure If you\u0026rsquo;re using the Cluster API to schedule tasks across your processes, and all forked processes die, then the docker engine will just assume you\u0026rsquo;ve gracefully shutdown.\nTake this code for example, you will see that it doesn\u0026rsquo;t fork another process and therefore, at some point it will no longer process any anything:\nimport { Injectable } from \u0026#39;@nestjs/common\u0026#39;; import * as cluster from \u0026#39;cluster\u0026#39;; import * as os from \u0026#39;os\u0026#39; @Injectable() export class ClusterService { static clusterize(numCPUs: number, callback: () =\u0026gt; void): void { if (cluster.isMaster ) { const procs = (numCPUs \u0026gt; os.cpus().length) ? os.cpus().length : numCPUs console.log(`GOING TO USE ${procs}PROCESSES`) console.log(`MASTER SERVER (${process.pid}) IS RUNNING `); console.log(`MASTER SERVER (${process.pid}) IS RUNNING `); console.log(`SCHED_NONE: ${cluster.SCHED_NONE}`) console.log(`SCHED_RR: ${cluster.SCHED_RR}`) console.log(`CLUSTER SCHEDULING POLICY: ${cluster.schedulingPolicy}`) for (let i = 0; i \u0026lt; procs; i++) { const worker = cluster.fork(); console.log(`CREATING PROCESS ${worker.process.pid}`); } cluster.on(\u0026#39;exit\u0026#39;, (worker, code, signal) =\u0026gt; { console.log(`worker ${worker.process.pid}died ${signal || code}`); }); cluster.on(\u0026#39;disconnect\u0026#39;, (worker) =\u0026gt; { console.log(`worker ${worker.process.pid}disconnected`); }) } else { callback() } } } To mitigate this, you simply fork another process within the exit event handler like this:\ncluster.on(\u0026#39;exit\u0026#39;, (worker, code, signal) =\u0026gt; { console.log(`worker ${worker.process.pid}died ${signal || code}, restarting...`); const newWorker = cluster.fork(); console.log(`CREATING PROCESS ${newWorker.process.pid}`); });  To avoid the container not restarting due to lack of process availability to deal with demand in the above scenario, you can\u0026rsquo;t use the on-failure condition in the restart_policy. You must use the \u0026lsquo;any\u0026rsquo; condition. This section incidentally replaces the old restart sub-option.\n deploy: replicas: 1 resources: limits: cpus: \u0026#34;2\u0026#34; memory: 512M update_config: order: start-first parallelism: 1 restart_policy: condition: any delay: 5s window: 120s placement: constraints: - node.role == worker Caution: You can\u0026rsquo;t use max_attempts: 3 in combination with condition: any\n Additionally, I found one further interesting facts when looking into this issue.\nIf you\u0026rsquo;re using Docker Stack Deploy (think stack in Portainer) using a docker-compose file to deploy to your swarm and you\u0026rsquo;re using restart: always, then beware, the restart is not supported.\nref: üëÜ compose-file\n "});index.add({'id':4,'href':'/posts/apache-ignite-running-agent-with-dotnetcore-server-node/','title':"How to run the Apache Ignite Agent with an Ignite.NET Core Server Node",'content':"I\u0026rsquo;ve recently been researching into Apache Ignite. Apache Ignite is an in-memory, memory-centric, distributed database, caching and processing platform for transactional, analytical, and streaming workloads.\nSo why the post? Well, with using .NET Core, I have run into one or two challenges that I have had to work through. One of which involves the Agent. I feel it is important to share with you how I get beyond this issue. It may save you a lot of time if you\u0026rsquo;re an Apache Ignite noob like me.\nYou use the Agent when you want to execute queries, SQL DML \u0026amp; DDL amongst other actions, from within the Web Console app. The Agent acts as a proxy. The Agent must connect to both the Web Console and your Server node or Thick Client node.\nAs I say above, this Agent acts as a proxy between the Web Console (UI to configure clusters, execute SQL DML \u0026amp; DDL, and query KV stores including visuals on caches etc‚Ä¶) and a Server node (aka, data node) to execute SQL \u0026amp; KV stores.\nWith all previous efforts, I was not able connect the Agent to the data node when the data node was created using a .NET runtime. When running the same configuration but using Java instead, it would connect without issue. Based on being able to connect to data node when created via java, I was confident that I would find a way to get this to work.\nAfter many hours of twawling the internet for answers and failed attempts, I figured out what the issue was. The Apache.Ignite nuget package does not include the ignite-rest-http module (contains many Jars), and this is what the Agent needs to communicate with the data node. So, what you need to do is download the entire Ignite binary package, and extract the ignite-rest-http folder. Then you use the following code to add a list of comma-separated file names of the HTTP Jar files to the IgniteConfiguration.JvmClasspath property:\nvar cfg = new IgniteConfiguration { JvmClasspath = Directory.GetFiles(pathToIgniteRestHttpJars) .Aggregate((x, y) =\u0026gt; x + \u0026#34;;\u0026#34; + y) }; using (var ignite = Ignition.Start(cfg)) { ... From what I could find, there\u0026rsquo;s no plans on including the ignite-rest-http module in the Apache.Ignite nuget package.\nI will share a GitHub repo to ease you into this shortly.\n"});index.add({'id':5,'href':'/posts/terraform-get-values/','title':"Terraform Get Values",'content':"With not wanting to have hard coded values pushed to a project\u0026rsquo;s code repository, and an antiquated way to derive Azure Service Principal credentials, I set about exploring ways on accomplishing this with this in mind using Terraform.\nAttempt 1 Here ins my first attempt, I load all permutations into map variables. I use an environment variable as an indexer to the appropriate map value:\nprovider \u0026#34;azurerm\u0026#34; { version = \u0026#34;=2.17\u0026#34; ... subscription_id = var.azure_subscription_id[var.environment] client_id = var.azure_client_id[var.environment] ... } variable \u0026#34;azure_subscription_id\u0026#34; { type = map default = { \u0026#34;dev\u0026#34; = \u0026#34;********-****-****-****-************\u0026#34; \u0026#34;prod\u0026#34;= \u0026#34;********-****-****-****-************\u0026#34; } } variable \u0026#34;azure_client_id\u0026#34; { type = map default = { \u0026#34;dev\u0026#34; = \u0026#34;********-****-****-****-************\u0026#34; \u0026#34;prod\u0026#34;= \u0026#34;********-****-****-****-************\u0026#34; } } ... Attempt 2 This second and more efficient approach, I used jsondecode function to load the entire credentials JSON to access the subscriptionId property:\nprovider \u0026#34;azurerm\u0026#34; { version = \u0026#34;=2.17\u0026#34; ... subscription_id = var.environment == \u0026#34;dev\u0026#34; ? jsondecode(var.azure_sp_dev).subscriptionId : jsondecode(var.azure_sp_prod).subscriptionId client_id = var.environment == \u0026#34;dev\u0026#34; ? jsondecode(var.azure_sp_dev).clientId : jsondecode(var.azure_sp_prod).clientId ... } variable \u0026#34;azure_sp_dev\u0026#34; { type = string default = \u0026lt;\u0026lt;EOT{ \u0026#34;clientId\u0026#34;: \u0026#34;********-****-****-****-************\u0026#34;, \u0026#34;clientSecret\u0026#34;: \u0026#34;********-****-****-****-************\u0026#34;, \u0026#34;subscriptionId\u0026#34;: \u0026#34;********-****-****-****-************\u0026#34;, \u0026#34;tenantId\u0026#34;: \u0026#34;********-****-****-****-************\u0026#34;, \u0026#34;activeDirectoryEndpointUrl\u0026#34;: \u0026#34;https://login.microsoftonline.com\u0026#34;, \u0026#34;resourceManagerEndpointUrl\u0026#34;: \u0026#34;https://management.azure.com/\u0026#34;, \u0026#34;activeDirectoryGraphResourceId\u0026#34;: \u0026#34;https://graph.windows.net/\u0026#34;, \u0026#34;sqlManagementEndpointUrl\u0026#34;: \u0026#34;https://management.core.windows.net:8443/\u0026#34;, \u0026#34;galleryEndpointUrl\u0026#34;: \u0026#34;https://gallery.azure.com/\u0026#34;, \u0026#34;managementEndpointUrl\u0026#34;: \u0026#34;https://management.core.windows.net/\u0026#34; } EOT } variable \u0026#34;azure_sp_prod\u0026#34; { type = string ... Obviously, none of the above deals with not pushing these credentials into the code repository.\nSo, in my opinion, there are 2 options available here. The first option is to use a GitHub Secret and to inject this secret into a script file. It could even be passed as a parameter to Terraform (e.g. terrafor apply -var credentials={...} ). Or, the second option is to obtain this key using the GitHub Azure/get-keyvault-secrets@v1.0 Action. This method will then allow you to obtain the Service Principal credentials from an Azure KeyVault. This latter approach means that we never need to expose these secrets outside of Azure, which we would have to do if we cut \u0026amp; paste them into a GitHub Secret.\n"});index.add({'id':6,'href':'/posts/digital-certificates/','title':"Digital Certificates",'content':"I\u0026rsquo;ve been wanting to put some notes down on digital certificates, signing and JWT for some time now. I find there are plenty of confusing terms involved in this area, plus a few nuances that have added to my personal confusion. I feel it now important to document these before I forget and move on [to another project].\nSo, what\u0026rsquo;s triggered this post? Well, one of many tasks I\u0026rsquo;m involved in [juggling] evolves SSO (single sign on). Albeit, mainly focused on the architecture on this task, I have compiled a few PoCs where I\u0026rsquo;m using digital certificates for authentication. In particular, SSOing into Twilio Flex and using an identity field returned from their I.AM service, to seamlessly log into our internal CRM, securely using a digital certificate.\nTerms Ok, let\u0026rsquo;s start with a few terms. I\u0026rsquo;ll slowly integrate these terms in the following examples.\n  Keys\nA key is something that is used to encrypt a piece of data (think JWT payload). It can be phrase (series of characters) or a public/private key held in a digital certificate.\n  Hash\nA hash is a piece of data, that cannot be reengineered to reveal it\u0026rsquo;s original content, also referred to as a digest or one-way hash.\n  SHA (Secure Hashing Algorithm)\nIt is for cryptographic security. It is used to produce an irreversible and unique hash.\n  Encryption\nThe process of converting something to gobbledygook and only be able to read it when you have the key used when it was encrypted.\n  Digital signature\nThe encrypted hash, that proves the data has not been tampered with in-flighted AND verifies the identity of the entity presenting it.\n  Signing\nThe process of creating the digital signature.\n  Base64\nThe more efficient was of encoding and sending data over a network.\n  Cipher algorithm\nA cipher algorithm is a mathematical formula designed specifically to obscure the value and content of data. Most valuable cipher algorithms use a key as part of the formula. This key is used to encrypt the data, and either that key or a complementary key is needed to decrypt the data back to a useful form.\n  RSA (Rivest‚ÄìShamir‚ÄìAdleman)\nRSA is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and distinct from the decryption key which is kept secret (private).\n  Symmetric Encryption\nSymmetric encryption is a type of encryption where only one key (a secret key) is used to both encrypt and decrypt electronic information.\n  Asymmetric Encryption\nAsymmetric Encryption is a form of Encryption where keys come in pairs. What one key encrypts, only the other can decrypt.\n  X.509\nIs a standard format for public key certificates. Each X.509 certificate includes a public key, identifying information, and a digital signature.\n  Of course, if this [digital signature] is new to you, the above won\u0026rsquo;t (yet) make much sense.\nI\u0026rsquo;m going to walk you through an example, well 2 actually. One that used a phrase as a key(aka keyphrase), and the other that used a public/private key found in a digital certificate (albeit, self-signed). I am going to use a tool call openssl, not may have heard of it?\nSymmetric encyrption Encryption using a keyphrase\nIn this first example, I\u0026rsquo;m going to encrypt a message with a keyphrase.\nBefore I begin, I\u0026rsquo;m going to write the content of my secret message to a file called msg.txt.\nNext, I\u0026rsquo;m going to encrypt this file it using a keyphrase of abc123 and output the encrypted file to msg.txt.enc:\n$ openssl enc -e -aes256 -k abc123 -in ./msg.txt -out ./msg.txt.enc above you\u0026rsquo;ll see -aes256. This is the cipher algorithm we\u0026rsquo;re using\nThe encrypted file looks something like this:\nSalted__BmFÔøΩj‘ø≈ÄaÔøΩÔøΩ1ÔøΩ\u0001\u001cÔøΩ\\X ÔøΩÔøΩÔøΩ{\u0026#39;VÔøΩ\u001bd\u001aFq\u0018\u0007ÔøΩ\u0026amp;ÔøΩÔøΩÔøΩLÔøΩ8:ÔøΩ\u0017\u0002ÔøΩÔøΩÔøΩÔøΩ\u0008ÔøΩT Pure gobbledygook!\nNow, I\u0026rsquo;m going to decrypt this encrypted file msg.txt.enc and output the encrypted file to msg.txt.dec. It is imperative that I used the same keyphrase:\n$ openssl enc -d -aes256 -k abc123 -in ./msg.txt.enc -out ./msg.txt.dec The decrypted file msg.txt.dec looks like:\nmy secret message If I had omitted the keyphrase, I will have been prompt for it which will have looked like this:\n$ openssl enc -d -e -aes256 -in ./msg.txt.enc -out ./msg.txt.dec enter aes-256-cbc decryption password: Or, if I had used the incorrect `keyphrase, I will have seen something like this:\nbad decrypt 140120216352064:error:06065064:digital envelope routines:EVP_DecryptFinal_ex:bad decrypt:../crypto/evp/evp_enc.c:583: A good simple illustration of how to encrypt and decrypt a message using openssl enc command.\nAsymmetric encryption Encryption using a public/private key\nIn this section I\u0026rsquo;m going to:\n Generate a RSA private key Extract the public key from the private key Generate a hash of the data I want to send, as well as signing it (using private key) Encrypt the data I want to send Decrypt the data I have received Verify the signature of the data received - ensure it data wasn\u0026rsquo;t tampered with in-flight  Let\u0026rsquo;s first generate the message I want to securely transmit:\n$ echo \u0026#39;my secret message\u0026#39; \u0026gt; msg Create a RSA private key Here I\u0026rsquo;m using the genrsa command. This command generates an RA private key:\n$ openssl genrsa -out private.pem 4096 $ openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 Extract public key $ openssl rsa -in private.pem -pubout -out public.pem Generate a digital signature using dgst Here, I\u0026rsquo;m generating a hash (digest) of the message as well as signing it with the private key\n$ openssl dgst -sha256 -sign private.pem -out msg.signature msg using rsautl rsautl, unlike dgst, does not create a hash or ASN1 encoding.\nAs rsautl uses the RSA algorithm directly, it can only be used to sign, or verify, small pieces of data:\r\r$ openssl rsautl -sign -in msg -inkey private.pem -out msg.sig Encrypt the message The rsautl command can be used to sign, verify, encrypt and decrypt data using the RSA algorithm.\n$ openssl rsautl -encrypt -inkey public.pem -pubin -in msg -out msg.enc By including the -pubin switch, you\u0026rsquo;re telling the command that the input key file (-inkey) is an RSA public key. Withou this, it assumed you\u0026rsquo;re using a private key\nDecrypt the message $ openssl rsautl -decrypt -inkey private.pem -in msg.enc -out msg.dec Verify signature using dgst This uses the public key to decrypt the Hash of the original msg:\npseudo logic:\nhash_1 = Hash ( msg ) hash_2 = Dec ( Key -\u0026gt; Hash ) IsVarified = hash_1 == hash_2 $ openssl dgst -sha256 -verify public.pem -signature msg.signature msg Verified OK using rsautl This verifies the original message using the signature and outputs it:\n$ openssl rsautl -verify -inkey private.pem -in msg.sig my secret message Digital Certificates To verify the identity of the entity presenting it\nSo far, we\u0026rsquo;ve covered hashes, key pairs, digital signatures and encryption and decryption. This section is where I cover, briefly, digital certificates. I\u0026rsquo;m using a digital certificate to replace the key pair as covered above and to give the capability of using additional information to verify that the identity of the entity presenting this message.\nLet\u0026rsquo;s start by creating a self-signed certificate. Type:\n# create self-signed certificate  openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:4096 -keyout myserver.pem -out myserver.crt -subj \u0026#34;/C=UK/OU=IT/CN=myserver.com\u0026#34; We can inspect the content of the certificate by typing:\nopenssl x509 -in myserver.crt -text -noout For bravity, I\u0026rsquo;m using the same commands as used above to; extract public key, generate digital signature, encrypt/decrypt then verify the signature. I\u0026rsquo;ve included all the statements in one block:\n# extract public key from self-signed certificate openssl rsa -in myserver.pem -pubout -out server-public.pem # generate hash and sign (digital signature) $ openssl dgst -sha256 -sign myserver.pem -out msg.server-signature msg # encrypt my message openssl rsautl -encrypt -inkey server-public.pem -pubin -in msg -out msg.enc2 # decrypt my encrypted message openssl rsautl -decrypt -inkey myserver.pem -in msg.enc2 -out msg.dec2 # vertify signature openssl dgst -sha256 -verify server-public.pem -signature msg.server-signature msg This results in:\nVerified OK JWT So, how does the digital signature relate to the Signature verification against a JWT Token?\nIn this section I will be using the jwt.io website. From this site I can choose which cipher algorithm. I will be using a RSA (widely used for secure data transmission and public-key cryptography) cipher as I\u0026rsquo;m simulating the sending and receiving of a JWT Token over HTTP.\nA JWT signature will use RSA SHA (irreversible hash) of the header and payload. This algorithm is set in the header so we have all the information we need to decrypt the encrypted data. However, we\u0026rsquo;re not able to verify these points yet: (a) has the message been tampered with inflight and (b) the identity of the entity presenting this message.\nIn actual fact, you will see this if you copied the a JWT token without keys into jwt.io (selecting RSA256 algorithm). It will show Invalid Signature. So, to verify these points, you need to provide the public and private key. It will use the private key to obtain the original Hash (hash of the original data) then decrypt this. If once decrypted, this equates to the RSASHA246 HMAC, then the signature is verified.\nAll I\u0026rsquo;ve done is added a tenant property to the claims (payload). I\u0026rsquo;ve doing this prove that I\u0026rsquo;ve changed the claim and will become apparent shortly why I\u0026rsquo;ve done this:\n{ \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;admin\u0026#34;: true, \u0026#34;iat\u0026#34;: 1516239022, \u0026#34;tenant\u0026#34;: \u0026#34;foobaa\u0026#34; } Next, I copy in the public and private key into the verify signature area. This all looks like this:\nI copy the encoded token (will paste it back in, in a moment) then refresh the page. The page is recent and defaults loaded (observe the changed payload):\nI now copy in my encoded token:\nYou will see the Invalid Signature near the bottom, but, the correct payload is back! This is because the certificates key pair in this default screen are different to my digital certificate\u0026rsquo;s key pair.\nSo, if I removed the entire encoded signature from the encoded token, we\u0026rsquo;ll still see the decrypted payload:\nSo, how\u0026rsquo;s it validating the signature? \u0026hellip;\nJWT Token format Let\u0026rsquo;s remind ourselves of the structure of a JWT Token is:\n{header}.{payload}.{signature}\nThe signature, with using the RS246 cipher, is a RSA SHA of the header (just cipher algorithm \u0026amp; type, which is JWT) AND payload. This simply means it is using a public-key (from our digital certificate) to encrypt a one-way hash of our original message (header + payload).\nHow is the signature verified? Token From encoded token\nAlgorithm (HASH_1):\nENCRYPT ( KEY -\u0026gt; ( HASH ( base64 (header) + \u0026#34;.\u0026#34; + base64 (payload) ) ) ) The Signature is RSA SHA of ( base64(header) + \u0026ldquo;.\u0026rdquo; + base64(payload)).\nHere, in the jwt.io site, it is recalculated after each valid payload change.\n\rKey pair Comparison using key pair\nAlgorithm (HASH_2):\nDECRYPT ( KEY -\u0026gt; ( HASH ( base64 (header) + \u0026#34;.\u0026#34; + base64 (payload) ) ) ) It base64 encodes the header + payload using the key pair, then encrypts it. If this matches the signature in the the encoded token then the signature is verified:\nVERIFIED = HASH_1 == HASH_2\n\r\rAs soon as I paste in my public and private keys, it correctly verifies the digital signature:\nReferences  openssl genrsa openssl rsautl openssl examples the difference rsautl -sign AND dgst -sign openssl  "});index.add({'id':7,'href':'/posts/jest-fs-readFileSync/','title':"Unit testing and mocking fs.ReadFileSync",'content':"I\u0026rsquo;d just ran npm run test in a newly created package I\u0026rsquo;d added to a monorepo (lerna) I\u0026rsquo;d created for a project I was working on that integrates with Twilio Sync, RabbitMQ, Twilio TaskRouter and MSSQL, and I go this:\n*******************************consumers\\packages\\eda [CRMBROK-233 +0 ~2 -0 !]\u0026gt; npm run test \u0026gt; @cf247/eda@1.0.2 test *******************************consumers\\packages\\eda \u0026gt; jest FAIL __tests__/eda.test.js ‚óè Test suite failed to run ENOENT: no such file or directory, open \u0026#39;.env\u0026#39; 2 | const fs = require(\u0026#39;fs\u0026#39;) 3 | const dotenv = require(\u0026#39;dotenv\u0026#39;) \u0026gt; 4 | const envConfig = dotenv.parse(fs.readFileSync(`.env`)) | ^ 5 | for (const k in envConfig) { 6 | process.env[k] = envConfig[k] 7 | } at Object.\u0026lt;anonymous\u0026gt; (lib/setenv.js:4:35) at Object.\u0026lt;anonymous\u0026gt; (lib/eda.js:1:1) Test Suites: 1 failed, 1 total Tests: 0 total Snapshots: 0 total Time: 1.772 s Ran all test suites. npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! @cf247/eda@1.0.2 test: `jest` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the @cf247/eda@1.0.2 test script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm WARN Local package.json exists, but node_modules missing, did you mean to install? npm ERR! A complete log of this run can be found in: npm ERR! *******************************\\npm-cache\\_logs\\2020-05-28T08_04_32_271Z-debug.log *******************************consumers\\packages\\eda [CRMBROK-233 +0 ~3 -0 !]\u0026gt; Not great but hey, first run and all!\nThe error message tells me everything I need to know:\nENOENT: no such file or directory, open \u0026#39;.env\u0026#39; 2 | const fs = require(\u0026#39;fs\u0026#39;) 3 | const dotenv = require(\u0026#39;dotenv\u0026#39;) \u0026gt; 4 | const envConfig = dotenv.parse(fs.readFileSync(`.env`)) Which is that it can\u0026rsquo;t find an .env file. And it wouldn\u0026rsquo;t. Later refactoring would remove this file dependency but for now, all I want to do is to get my test working.\nThis was the unit test code:\n\u0026#39;use strict\u0026#39; const eda = require(\u0026#39;..\u0026#39;) describe(\u0026#39;@cf247/eda\u0026#39;, () =\u0026gt; { it(\u0026#39;no tests\u0026#39;, () =\u0026gt; { }) }) This is the code from the module it was importing via the require('..') statement:\nrequire(\u0026#39;./setenv\u0026#39;) const amqp = require(\u0026#39;amqplib/callback_api\u0026#39;); module.exports = (io, emitter) =\u0026gt; { ... The top line is importing code from this file:\nI\u0026rsquo;ve highlighted the problematic line of code\n1 2 3 4 5 6  const fs = require(\u0026#39;fs\u0026#39;) const dotenv = require(\u0026#39;dotenv\u0026#39;) const envConfig = dotenv.parse(fs.readFileSync(`.env`)) for (const k in envConfig) { process.env[k] = envConfig[k] }    The quickest (IMO) way to deal with this and move forward is to Mock the fs class. I did this by included a jest module mock into my unit test file:\nI\u0026rsquo;ve highlighted the mock related code\n1 2 3 4 5 6 7 8 9 10 11 12 13  \u0026#39;use strict\u0026#39; const fs = require(\u0026#39;fs\u0026#39;) const eda = require(\u0026#39;..\u0026#39;) jest.mock(\u0026#39;fs\u0026#39;, () =\u0026gt; ({ readFileSync: jest.fn((file_name) =\u0026gt; { return [] }) }))  describe(\u0026#39;@cf247/eda\u0026#39;, () =\u0026gt; { it(\u0026#39;no tests\u0026#39;, () =\u0026gt; { }) });    What this does is, when the readFileSync class function is called, it always returns an empty array []. As the unit code does not have a dependency on environment variables, this mocked response will work fine.\n"});index.add({'id':8,'href':'/posts/kubernetes-on-windows/','title':"Kubernetes on Windows",'content':"This post is a reminder to me of what needs to be installed in order for a pod, created from a local image, that is to be served up via a kubernetes cluster, to be run from your local development environment.\nWhat is Kubernetes and why is it so important? \u0026ldquo;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\u0026rdquo;\nSo why is Kubernetes important? \u0026ldquo;Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn‚Äôt it be easier if this behavior was handled by a system?\nThat‚Äôs how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system.\u0026rdquo;\nKubernetes is the community\u0026rsquo;s (has a much larger community than that of Swarm's community) choice of container orchestrators.\nSome important notices \r.header {\rcolor: white;\rfont-weight: bold;\rpadding-left: 10px;\rpadding-top: 5px;\rpadding-bottom: 5px;\rborder-top-right-radius: 5px;\rborder-top-left-radius: 5px;\rfont-size: smaller;\r}\r.info {\rbackground-color: #336699;\r}\r.warning {\rbackground-color: orange;\r}\r.error {\rbackground-color: red;\r}\r.note-panel {\rbackground-color: #c2f5f5;\rpadding: 10px;\rborder-radius: 5px;\r}\r.panel-info {\rborder: 1px solid #336699;\rbackground-color: rgba(51, 102, 153, 0.2);\r}\r.panel-warning {\rborder: 1px solid orange;\rbackground-color: rgba(255, 166, 0, 0.2);\r}\r.panel-error {\rborder: 1px solid red;\rbackground-color: rgba(255, 0, 0, 0.2);\r}\r.panel-header {\rborder-top-left-radius: 0px !important;\rborder-top-right-radius: 0px !important;\r}\r\r\r\rPermissions\r\r\r-- \r\r-- Permissions\nTo install kubectl and minikube you must start Powershell with Administrator permissions\n\r\r.header {\rcolor: white;\rfont-weight: bold;\rpadding-left: 10px;\rpadding-top: 5px;\rpadding-bottom: 5px;\rborder-top-right-radius: 5px;\rborder-top-left-radius: 5px;\rfont-size: smaller;\r}\r.info {\rbackground-color: #336699;\r}\r.warning {\rbackground-color: orange;\r}\r.error {\rbackground-color: red;\r}\r.note-panel {\rbackground-color: #c2f5f5;\rpadding: 10px;\rborder-radius: 5px;\r}\r.panel-info {\rborder: 1px solid #336699;\rbackground-color: rgba(51, 102, 153, 0.2);\r}\r.panel-warning {\rborder: 1px solid orange;\rbackground-color: rgba(255, 166, 0, 0.2);\r}\r.panel-error {\rborder: 1px solid red;\rbackground-color: rgba(255, 0, 0, 0.2);\r}\r.panel-header {\rborder-top-left-radius: 0px !important;\rborder-top-right-radius: 0px !important;\r}\r\r\r\rShell\r\r\r\r\r\r-- Shell\nThese settings will only viable for the current shell, if you need to run another shell, ensure the minikube docker-env commands in the Steps to take to configure your environment section are also executed in the new shell. As minikube is the tool that runs a local cluster in your development environment, we need to tell it to use it\u0026rsquo;s built-in docker daemon and have images pulled from there, and not from a container registry.\n\rHow do I install kubectl (and what the heck is it)? kubectl is a CLI (command line interface) tool for controlling Kubernetes clusters. You can use this tool to deploy applications, inspect and manage cluster resources and view logs.\nTo ease the installation process, use chocolatey to install kubernetes-cli, run:\nPS C:\\\u0026gt; choco install kubernetes-cli How do I install minikube (and what the heck is it)? Minikube implements a local Kubernetes cluster and is deemed the best tool for local Kubernetes application development.\nTo ease the installation process, use chocolatey to install minikube, run:\nPS C:\\\u0026gt; choco install minikube Before you start, you must ensure that you have a platform virtualisation system available. Platform virtualisation software provides the mechanism to run virtual machines and containers in isolation and exposes them to one or more networks. It is within a virtual machine that your Kubernetes cluster will run. Windows 10 comes with a virtualisation hypervisor feature called hyper-v. You need to ensure it is running first. To do this, run:\nPS C:\\\u0026gt; Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All PS C:\\\u0026gt; minikube start --driver hyperv * minikube v1.9.1 on Microsoft Windows 10 Enterprise 10.0.18363 Build 18363 * Using the hyperv driver based on user configuration * Downloading VM boot image ... \u0026gt; minikube-v1.9.0.iso.sha256: 65 B / 65 B [--------------] 100.00% ? p/s 0s  \u0026gt; minikube-v1.9.0.iso: 174.93 MiB / 174.93 MiB [ 100.00% 1.03 MiB p/s 2m51s * Starting control plane node m01 in cluster minikube * Creating hyperv VM (CPUs=2, Memory=6000MB, Disk=20000MB) ... * Preparing Kubernetes v1.18.0 on Docker 19.03.8 ... * Enabling addons: default-storageclass, storage-provisioner * Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; Steps to take to configure your environment To set up your minikube environment, run:\nPS C:\\\u0026gt; minikube docker-env $Env:DOCKER_TLS_VERIFY = \u0026#34;1\u0026#34; $Env:DOCKER_HOST = \u0026#34;tcp://192.168.75.126:2376\u0026#34; $Env:DOCKER_CERT_PATH = \u0026#34;C:\\Users\\garrard.kitchen\\.minikube\\certs\u0026#34; $Env:MINIKUBE_ACTIVE_DOCKERD = \u0026#34;minikube\u0026#34; # To point your shell to minikube\u0026#39;s docker-daemon, run: # \u0026amp; minikube -p minikube docker-env | Invoke-Expression To point your shell to minikube\u0026rsquo;s docker-daemon, run:\nPS C:\\\u0026gt; minikube docker-env | Invoke-Expression To get access to minikube\u0026rsquo;s dashboard, run:\nPS C:\\\u0026gt; minikube.exe dashboard * Verifying dashboard health ... * Launching proxy ... * Verifying proxy health ... * Opening http://127.0.0.1:54553/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser... Here\u0026rsquo;s some sample nodejs (server.js) code. It starts a server on port 8080:\nvar http = require(\u0026#39;http\u0026#39;); var handleRequest = function (request, response) { console.log(\u0026#39;Received request for URL: \u0026#39; + request.url); response.writeHead(200); response.end(\u0026#39;Hello World!\u0026#39;); }; console.log(\u0026#34;started\u0026#34;) var www = http.createServer(handleRequest); www.listen(8080); Here\u0026rsquo;s a Dockerfile for the above nodejs server. Please observe that it exposes port 8080. This ensures that network TCP traffic can be received by the container via port 8080.\nFROMnode:13.5.0EXPOSE8080COPY server.js .CMD [ \u0026#34;node\u0026#34;, \u0026#34;server.js\u0026#34; ]To build a image of the above Dockerfile, run:\nPS C:\\\u0026gt; docker build -t hello-world:1 . \r.header {\rcolor: white;\rfont-weight: bold;\rpadding-left: 10px;\rpadding-top: 5px;\rpadding-bottom: 5px;\rborder-top-right-radius: 5px;\rborder-top-left-radius: 5px;\rfont-size: smaller;\r}\r.info {\rbackground-color: #336699;\r}\r.warning {\rbackground-color: orange;\r}\r.error {\rbackground-color: red;\r}\r.note-panel {\rbackground-color: #c2f5f5;\rpadding: 10px;\rborder-radius: 5px;\r}\r.panel-info {\rborder: 1px solid #336699;\rbackground-color: rgba(51, 102, 153, 0.2);\r}\r.panel-warning {\rborder: 1px solid orange;\rbackground-color: rgba(255, 166, 0, 0.2);\r}\r.panel-error {\rborder: 1px solid red;\rbackground-color: rgba(255, 0, 0, 0.2);\r}\r.panel-header {\rborder-top-left-radius: 0px !important;\rborder-top-right-radius: 0px !important;\r}\r\r\r\rInclude a build tag\r\r\r\r\r\r-- Include a build tag\nYou must specify a version tag and it has to be something other than latest. Here, I have used 1. If you don\u0026rsquo;t follow these instructions, minikube will attempt to pull the image from a docker registry (normally DockerHub).\n\rTo check that the image exists in Minikube\u0026rsquo;s built-in Docker daemon, run:\nPS C:\\\u0026gt; minikube ssh $ docker images You should see something similar to this:\n$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/\u0026#39;) _ _ | |_ __ /\u0026#39; _ ` _ `\\| |/\u0026#39; _ `\\| || , \u0026lt; ( ) ( )| \u0026#39;_`\\ /\u0026#39;__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/\u0026#39;(_,__/\u0026#39;`\\____) $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world 1 55f40b7f5c32 13 days ago 660MB hello-world latest 50c4285f25a5 13 days ago 660MB nginx latest ed21b7a8aee9 2 weeks ago 127MB k8s.gcr.io/kube-proxy v1.18.0 43940c34f24f 3 weeks ago 117MB k8s.gcr.io/kube-scheduler v1.18.0 a31f78c7c8ce 3 weeks ago 95.3MB k8s.gcr.io/kube-apiserver v1.18.0 74060cea7f70 3 weeks ago 173MB k8s.gcr.io/kube-controller-manager v1.18.0 d3e55153f52f 3 weeks ago 162MB kubernetesui/dashboard v2.0.0-rc6 cdc71b5a8a0e 5 weeks ago 221MB k8s.gcr.io/pause 3.2 80d28bedfe5d 2 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 2 months ago 43.8MB kindest/kindnetd 0.5.3 aa67fec7d7ef 5 months ago 78.5MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 5 months ago 288MB kubernetesui/metrics-scraper v1.0.2 3b08661dc379 5 months ago 40.1MB gcr.io/k8s-minikube/storage-provisioner v1.8.1 4689081edb10 2 years ago 80.8MB To run this image as a pod, run:\nPS C:\\\u0026gt; kubectl run hello-world --image=hello-world:1 --port=8080 --image-pull-policy=never pod/hello-world created The --image-pull-policy=never is telling Kubectl to use the local image and not one from a container registry (Docker, ACR, ECR, GCP)\nTo expose this port for external access (from browser) from outside of the cluster, run:\nPS C:\\\u0026gt; kubectl expose pod hello-world --type=LoadBalancer service \u0026#34;hello-world\u0026#34; exposed To confirm your service is running and to get the port number of this exposed service, run:\nPS C:\\\u0026gt; kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-world LoadBalancer 10.111.126.10 \u0026lt;pending\u0026gt; 8080:31589/TCP 45h kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2d16h You will see the \u0026lt;pending\u0026gt; state of your LoadBalancer if you do not have not a Load Balancer integrated with your cluster. For your local development environment, it is nothing to worry about.\r\rYou will see that the hello-world service is accessible via port 8080. However, we still don\u0026rsquo;t know behind what IPv4 address, this services is available. To get the IPv4 address of your cluster, you type:\nPS C:\\\u0026gt; minikube ip 192.168.75.126 Finally, to access your service, run the cURL command, using the minikube ip address and the TCP port as listed in the kubectl get services output:\nPS C:\\\u0026gt; curl \u0026#34;http://192.168.75.126:31589\u0026#34; -UseBasicParsing StatusCode : 200 StatusDescription : OK Content : {72, 101, 108, 108...} RawContent : HTTP/1.1 200 OK Connection: keep-alive Transfer-Encoding: chunked Date: Mon, 06 Apr 2020 13:05:42 GMT Hello World! Headers : {[Connection, keep-alive], [Transfer-Encoding, chunked], [Date, Mon, 06 Apr 2020 13:05:42 GMT]} RawContentLength : 12 You can also use minikube to obtain your service\u0026rsquo;s url. To do this, run:\nPS C:\\\u0026gt; minikube service hello-world --url http://192.168.75.126:31589 Useful kubectl commands This first command is important. Some background first\u0026hellip;a context is a group of access parameters. Each context contains a Kubernetes cluster, a user, and a namespace. When you are working with multiple contexts off of your development machine, you may run into compatibility issues due to your client version not being compatible with the server API version. All kubectl commands will run against the current context. To check your client version, run:\nPS C:\\\u0026gt; kubectl version --client Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;18\u0026#34;, GitVersion:\u0026#34;v1.18.0\u0026#34;, GitCommit:\u0026#34;9e991415386e4cf155a24b1da15becaa390438d8\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-03-25T14:58:59Z\u0026#34;, GoVersion:\u0026#34;go1.13.8\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;windows/amd64\u0026#34;} To ascertain your current context, run:\nPS C:\\\u0026gt; kubectl config current-context minikube To list all of your configured contexts, run:\nPS C:\\\u0026gt; kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-desktop docker-desktop docker-desktop docker-for-desktop docker-desktop docker-desktop * minikube minikube minikube The * next to minikube indicates that minikube is your current context.\nIf you are configured to access a cluster hosted from a cloud provider such as Azure, then this context will also be listed.\nTo use a specific context, run:\nPS C:\\\u0026gt; kubectl config use-context docker-for-desktop References  Install Kubernetes Install Minikube Kubectl Cheatsheet Minikube\u0026rsquo;s built-in Docker daemon  "});index.add({'id':9,'href':'/posts/modern-javascript/','title':"Modern-ish Javascript",'content':"This post includes a few notes on ECMA language features that I like as well as some info on memory leaking. I have no doubt that it will read disjointed; I started this eons ago and only now have I decided to publish it.\nA simple reminder of what Node.js is \u0026hellip;it is a set of APIs wrapped around the V8 Engine (written in c++) and is a high-performance JavaScript and WebAssembly engine.\nES2015 (ES6) Class I find Javascript messy at the best of times. When things are messy, personally I find it difficult to see the forest through the trees, and by this I mean, have I coded for all the *-cases (use/edge/corner)? Or worse, can I see the existing defects or bug breaders?! Then there\u0026rsquo;s the lack of readability.\nI\u0026rsquo;ve discussed the use of classes with many Engineers and I have had a mixed reception but in the main, most said they preferred the simplicity of arrow functions. Not sure if there is a right or wrong answer to this (bit like the tabs or spaces\u0026hellip;tabs, obvs!)\u0026hellip; and at one time I will have agreed with the majority. Now though is a different story. Like so many others, I too have drank the cool-aid on TypeScript and now the only reason I can see myself opting for Javascript in the future is mainly for legacy reasons.\nComing from an OOP background, I naturally gravitate towards constructs like classes:\nclass Admin extends User { constructor (name) { super(name) this.initialize() } initialize = () =\u0026gt; {} } Destructuring const getProfile = () =\u0026gt; { return {firstname: \u0026#34;garrard\u0026#34;, lastname: \u0026#34;kitchen\u0026#34;, married: true, children: 2} } const {firstname, lastname, ...family} = getProfile() console.log(`firstname: ${firstname}`) console.log(`lastname: ${lastname}`) console.log(family) This would result in:\nArrow function Arrow functions are a great addition to the ES spec! Their scope is purely inside of it\u0026rsquo;s closure and is not affected by the this context which may hoisted functions fall victum of.\ninitialize = () =\u0026gt; {} ES2016 (ES7) Language Features The Decorator Awesome addition to the EMCA family!\nI\u0026rsquo;ve used this with great affect with Typescript, and mostly with NestJS solutions.\nThis is a contrived example on how you can use very basic decorator on a class function:\nYou must have configured your solution to use babel\nexample\rclass Content { @link(\u0026#39;nodejs\u0026#39;, \u0026#34;\u0026lt;a href=\u0026#39;https://nodejs.org/en/\u0026#39;\u0026gt;Node.js\u0026lt;/a\u0026gt;\u0026#34;) html() { return `This server language is called nodejs!` } } function link(_find, _replace) { return function(target, key, descriptor) { var old = descriptor.value() descriptor.value = () =\u0026gt; { var n = old.replace(_find, _replace) return n } } } const m = new Content() console.log(m.html()) output:\n[nodemon] restarting due to changes... [nodemon] starting `babel-node index.js` This server language is called \u0026lt;a href=\u0026#39;https://nodejs.org/en/\u0026#39;\u0026gt;Node.js\u0026lt;/a\u0026gt;! [nodemon] clean exit - waiting for changes before restart babel\rpackage.json:\n... \u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;nodemon --exec babel-node index.js\u0026#34; }, ... \u0026#34;devDependencies\u0026#34;: { \u0026#34;@babel/core\u0026#34;: \u0026#34;^7.12.3\u0026#34;, \u0026#34;@babel/node\u0026#34;: \u0026#34;^7.12.1\u0026#34;, \u0026#34;nodemon\u0026#34;: \u0026#34;^2.0.6\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;@babel/plugin-proposal-decorators\u0026#34;: \u0026#34;^7.12.1\u0026#34; } ... .babelrc:\n{ \u0026#34;plugins\u0026#34;: [ [\u0026#34;@babel/plugin-proposal-decorators\u0026#34;, { \u0026#34;legacy\u0026#34;: true }] ] } \rES2018 (ES9) Spread I was reminded of something useful this morning (on the morning I wrote this, originally!) from a youtube video I was watching. JS passes objects (non-primitives) by reference, ergo, memory pointers, so it is possible to effect an object outside of it\u0026rsquo;s closure. So, imagine you return an array of objects (e.g. from a service to a controller). It is possible, to effect this array of objects from within the controller. One way I have found to avoid this is by using the spread syntax:\nprivate readonly list: string[] getList() { return list } you can do this:\ngetList() { return [...list] } Obvs, the üëÜ is using an array but you can do this same with an object too {\u0026hellip;list}\nMemory Functions arguments passed by value; always See also spread üëÜ to for advance on how to avoid memory leakage.\nFurther to the above, JS always passes by value (not reference) ALL augments to a function. This means that, if you pass in an argument (primitive or object) into a function, the closure is honoured and therefore any changes made to this value inside the closure is not reflected outside, example:\nlet v: string = \u0026#34;A\u0026#34; getValue(v){ v = v + \u0026#34;B\u0026#34; } let result = getValue(v) console.log(result) // output: AB console.log(v) // output: A "});index.add({'id':10,'href':'/posts/mentoring/','title':"How do I mentor?",'content':"I have written this post to document my experiences of mentoring. I have mentored front-end engineers, back-end engineers and UX designers. I have had the pleasure of helping others as well as learning one of two things about myself along this journey too. If ever you get the opportunity to be a mentor, I recommend you jump at the opportunity. It is a self-rewarding experience.\nSo, what is mentoring?\u0026hellip;\nThe definition of Mentoring is the act of advising or training (someone, especially a younger colleague).\nIn her book The Manager\u0026rsquo;s Path, Camille Fournier talks about Mentoring. She writes:\n\u0026quot;The first act of people management for many engineers is often unofficial.\u0026quot;\r This has always been the case for me too. I am currently employed as a Principal Engineer, before this, a CTO. In this time, I have neither organised nor carried out an official [backed by a recognised authority] mentoring scheme. It\u0026rsquo;s just been something that I do, without fuss but with purpose and pride.\nOddly, I have never been a mentee. If I had then there is a possibility that this in itself may have defined or partially influenced mentoring for me.\nThis is a list of scenarios where I have mentored others in:\n onboarding new company starters, onboarding a new colleague at a similar level as myself onboarding a graduate (their first job since graduating from university) when working on a project together  Concerning the above mentioned scenarios, I have both created and coordinated an onboarding programme. This was when I was a CTO. All this was choreographed remotely. Ironically, this is more relevant today than ever. As I write this CV-19 has started to take a grip of the UK and yesterday I heard of the sad news that 2 people had died from it in Southport where I have resided since 2008.\nThis is a bullet list of key \u0026lsquo;things\u0026rsquo; that I have discovered that have helped me through the mentoring process:\n communicate what the process of mentoring is to the mentee first, listen, then respond. Don\u0026rsquo;t attempt to expedite the process, don\u0026rsquo;t forget, it\u0026rsquo;s for them, not you! take the time to explain the rationale for a decision take the time to explain why something is not applicable in that particular instance try not to provide answers, but provide strategies (alternatives, is there an easier to do the same, what is the problem we\u0026rsquo;re trying to solve) allow for mistakes to be made and always follow them up with a post mortem. We all make mistakes, in some cases, it helps define you. Making a mistake is critical to our development so this is why the next point is important\u0026hellip; ensure you make a safe environment for your mentee to operate in make time but be clear about the amount of time you can give. You will have other responsibilities. Inadvertently, you are forcing the mentee to make decisions. This often encourages the mentee and gives them the confidence to stand on their own two feet. This too is critical for their development work on a real project, albeit, scaled back for safety and to limit the blast radius. It has to be something that matters to the business. This will help the mentee be recognized by their good work. By limiting the hypotheticals, the mentee will then get their hands on a non-fabricated, warts and all, real-life engineering problem to help in the preparation of an important [to them] event - this has meant helping produce the materials for an event as well as assessing and providing feedback develop a personal development plan - used to help keep focus as well as a comparator. This can take up a chunk of time but well worth it plus you\u0026rsquo;re holding yourself accountable to the process too!  As CTO I led both the architectural and the planned engineering effort that has been key to the strategic direction of that business. Mentoring was an important part of this process and as such, I was always in mentoring mode. To this day, no longer a CTO but still in a senior engineering position, I constantly think about, and act on, ways to help those around me to improve their engineering capabilities (think good engineering principles).\nAlthough not all of my mentoring is official, I do conduct myself in such a way that it benefits those around me. I do this by encouraging my co-workers whenever possible. Here is a list of how I have been able with success, help my co-workers:\n I demonstrate, then I include a co-worker in this process. An example of this is by whiteboarding a problem or solution. I hand over the marker and this leads to them articulating their solution in front of an audience I instigate a technical discussion or articulate an engineering problem. I solicited input from all (introverts and extroverts alike). This encourages my wo-workers to speak up and gain confidence in discussing technical issues in front of an audience I am consistent in the message of working in a safe environment, one where any question can be asked and any view given I define a piece of work\u0026rsquo;s guiding principles upfront. This helps in several ways. It defined the focus of the project, what to exclude etc. It also helps shape our collective thinking and finally, it\u0026rsquo;s a gentle way into a project instead of a rushing headlong into it without giving it any due diligence Redirect to good engineering principles whenever possible to enforce our foundation of good engineering.  I am a Principal Engineers and as such, I have a responsibility to my co-workers and the business to conduct myself in a way befitting a Principal Engineer. Quite simply put, one of the objectives is to help my co-workers in whatever way possible. This can be helping them out on a project. It can be providing feedback on a piece of work or technique. Ultimately, my goals are to be supportive, helpful, insightful, encouraging, guiding, a sounding board and inspirational. All executed respectfully. The people I have worked with and those who I currently work with are important to me. Anything I can do to help, I do. Even if it\u0026rsquo;s listening to them sound off. Returning to my goals\u0026hellip;I do see some of these being reflected at me but more importantly, I see the product of my mentoring too, which I find extremely satisfying!\nOne of the most humbling times of my life was when I mentored a colleague who, through no fault of his own, was temporarily let go from the company I was a CTO for. We as a company were struggling financially and had to slim down the workforce. It was a sh*t time. It was important to me though from a personal perspective that I didn\u0026rsquo;t just sever contact with him. The plan was always to bring him back onboard once things improved. And they did. But during the time that it wasn\u0026rsquo;t so great, I would meet-up regularly with him online - he was based in another country. We would discuss many topics; life, technology \u0026amp; side projects. Where I could, I\u0026rsquo;d provide guidance and be a sounding board for him. From time to time I would plan things for him to do. The next time we met up, I\u0026rsquo;d review what he had done and provide feedback when necessary. I would like to think that this created a bond between us. Like I say, it was all very humbling as after all, I was still in gameful employment. At some level, it must have been a bitter pill for him to swallow and he never held it against me, which is demonstrative of his good character. We no longer work together but he remains a friend and we do still often catch up online.\nOutcomes from the mentoring process can also be subtle. Just to be clear, it\u0026rsquo;s not always explosive or awe-inspiring either. It is what it is and a poor result does not equate to a lack of mentor\u0026rsquo;s ability. Generally, poor results are rare. In the one case where I observed poor results, I reported it upwards. The vertical market we were operating in didn\u0026rsquo;t float this particular mentee\u0026rsquo;s boat. It happens! Also, in my experience, it is always noticeable over time; providing you take a documented snapshot before and after. One source of personal satisfaction is seeing mentees, new and old, interacting with seasoned engineers, observing them standing on their own two feet, adding value to a conversation and project work alike. Best of all, seeing a seasoned engineer asking a mentee for their advice and input on a scenario. That my friends, is extremely satisfying!\n Written mainly for me, I do hope you\u0026rsquo;ve found something useful here and who knows, it might even help you with your mentoring journey too.\n "});index.add({'id':11,'href':'/posts/principles/','title':"Good Engineering - Principles",'content':"I have written this post as a method to document what I see as the basics, foundations if you will, for good engineering. Undoubtedly if you are a seasoned engineer, you will recognised all of these principles, less so, if you\u0026rsquo;re just starting out.\nMost Engineers are fully versed in the foundations of writing quality, efficient, succinct and testable code. As a Principal Engineer, one of my responsibilities is to ensure that these (1) foundations are recognised by the engineers and (2) are adhered to by all engineers.\nHere\u0026rsquo;s a list of concepts that for me, constitute good engineering principles:\nThese are in alphabetical order and not in order of importance\n Clean and readable code Code reviews Coding standards Composition over inheritance Defensive coding Do no more DRY KISS Occam\u0026rsquo;s razor Premature optimization Refactor Separation of Concerns SOLID Testing YAGNI  Other sections:\n My pattern discovery Being a Principal Engineer Discussion point References  Clean and readable code Clean and readable code is always better than clever code (ask any engineer who has to extend or maintain a clever piece of code!)\nI\u0026rsquo;ve seen a lot of code recently that should never have got to the shape it has. Complicated code requires time to understand, then time to add functionality. Complicated code also happens to more difficult to recall so each time you need to go near it, you have to relearn it and added to this, any changes made to improve it, most likely have not been applied in full so they\u0026rsquo;ll be a right old mixture of good, bad and the ugly thrown into the mix.\nA good measure of how bad a codebases is, and I\u0026rsquo;m going to plagiarise somebody else\u0026rsquo;s analogy here, is by stepping through an interactive debug session. If you get momentarily distracted by a fly, then immediately return to the debugging and you do not know where the feck you are in the execution of the code flow, then it\u0026rsquo;s a bad codebase!\nIt\u0026rsquo;s the responsibility of a Tech Lead or architecture to stop code bases ending up this way.\nCode reviews It should only contain helpful and constructive comments and/or implementation questions. This process is not there to caress egos (that\u0026rsquo;s for your mother to do!!). One useful by-product of code reviews is conveying of your team\u0026rsquo;s exacting coding standard and attention to deal, to new starters. So, the quicker the new starter pushes a commit, the better!\nCoding standards (provide a template of core standards then stand back and let the team thrash out the rest - wear protection!)\nAlthough important, it\u0026rsquo;s not the end of the world if some of the granular details differ between teams. The important thing here, in my opinion, is that each team know where to find their cheese. Most engineers in a team have a common set of standards they adhere too. The big things like solution structure, naming conventions, testing (AAA, GWT), pluralization, documentation structure (including README) all need to be consistent.\nComposition over inheritance (avoid class tree exploitation! - think Strategy pattern - GoF)\nThe above-bracketed statement says it all! Inheritance tends to back you into a corner especially when you consider the OCP.\nDefensive coding (guard against invalid class method parameters and accidental null assignment to class properties instead of an equality condition!)\nThis is one example of defensive coding:\nclass User(string firstnaame, string lastname, int age) { if (null == firstname) { throw new NullReferenceException(\u0026#34;Firstname cannot be null\u0026#34;) } ... The above demonstrates an example of defensive coding. The first is that we need to test for valid constructor parameter values when instantiating a class.\nThe second, is to avoid mistakes that might not be picked up by your compiler. For instance, a common mistake doing this:\nif (firstname = null) A .NET Compiler is more than happy allowing this above syntax, as, after all, it\u0026rsquo;s an assignment operator and not a equality operator as in above in the class constructor. By switching these around, you\u0026rsquo;re making a positive pattern changing and should avoid making this silly mistake again.\nDo no more (and do no less - thank you eXtreme Programming!).\nIf you code outside the scope, you\u0026rsquo;re in danger of creating code that isn\u0026rsquo;t used or needed. The worse thing about this is that others will have to maintain this code. How can this be? Well, it\u0026rsquo;s common - think HTTP chaining - for code not to be culled especially if there is a disconnect between these dependencies and there\u0026rsquo;s no IDE/compiler to shout at you.\nDRY (don\u0026rsquo;t repeat yourself)\nCode analysis tools help here, but you\u0026rsquo;re not always going to have access to these tools.\nOne way to help identify code that does the same thing is by refactoring. If you keep your code method frame small (~20 lines), and you have a good naming standard for methods (e.g. noun+verb with accurate alighment to business capability - think DDD), have unit tests with a high code coverage percentage, then this should be all you need to help you avoid writing duplicate code.\nKISS (keep it simple, silly)\nThis to a certain extent, goes hand in hand with avoiding premature optimization. We all like the big picture yes? This doesn\u0026rsquo;t mean we need to do deliver on this it right now! You just need to know the boundaries of this piece, which, if greenfield, then you won\u0026rsquo;t have any metrics to tell you the actual demand. Think Capacity planning; what this piece of work needs to do based on current expectations. For example\nDo we need multiple servers? Yes, I think Why do we need multiple servers? Mmmmm, because I read it somewhere\rDo you have the metrics that support your argument for multiple servers? Wait, what?\rNext!\r A colleague recently shared with me the architecture of their side project. They are using AWS and I have 2 certifications in AWS (Developer and Solutions Architect). I quickly went into HA/scaling/resilience/durability/DR overdrive, following it up with a verbal dump on what tech they should use. This was all wrong. They did not know their service demand. Following my initial advice, will have increased their cost; unnecessarily. I did, you\u0026rsquo;ll be glad to hear, re-affirm their decision (may have made 1 or 2 helpful suggestions) shortly after [~2 hours].\nYeah, think big but don\u0026rsquo;t deliver big without a customer base; as this, in my experience, will result in a huge waste of time, effort and money. Plus, sometimes, you don\u0026rsquo;t really know where something is going to take you, and my advice here is to roll with it. This last piece of advice is particularly pertinent if you\u0026rsquo;re starting up.\nOccam\u0026rsquo;s Razor This is a problem-solving principle.\nThe definition of this is: \u0026ldquo;Entities should not be multiplied without necessity\u0026rdquo;. It is sometimes paraphrased by a statement like \u0026ldquo;the simplest solution is most likely the right one.\nOccam\u0026rsquo;s razor says that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions. Good advice\nSuppose there are two competing theories on why something is not working. Normally, the case that requires the least amount of assumptions is correct. So, the more assumptions you have to make means it more likely to be more unlikely.\nPremature optimization Avoid premature optimization and all conversations relating to optimization until you know the facts. This will be futile until you\u0026rsquo;ve metrics to better inform you.\nI\u0026rsquo;ve hit this numerous times when planning for microservices and bounded contexts, in particular, on green-field projects. What should we include and where? Should we separate claims away from users for instance? Will the demand for Claims be greater than for users? Who knows?! You don\u0026rsquo;t until you have some metrics behind you. You can always merge or break them [microservices] up later.\nAnother area that I believe this encompasses is splitting code up across multiple files and folders. If it\u0026rsquo;s a PoC, a sample piece of code, or something that has a short shelf life, just keep it in one file. When it\u0026rsquo;s the right time - moving out of PoC/other - then you can consider optimizing it. Up until then, it\u0026rsquo;s a huge waste of time and effort.\nArchitecture is a great example of when not to prematurely optimize. Architecture normally infers cost. Generally, the more of something, the greater the cost. This could mean for a startup the difference between survival and their demise. Adopting a guiding principle of being frugal from the outset, is a prudent and wise decision. What this means is that you\u0026rsquo;re always looking for the most cost-effective way of accomplishing your goal. So, if you don\u0026rsquo;t know your demand, it means you opt for a single server instead of having a HA cluster of 3 master nodes and 5 worker nodes! Down from 8 servers to 1 which on a month by month basis during development and beta/early releases could mean the saving of thousands of pounds sterling.\nSadly, I\u0026rsquo;ve come across a few startup that have failed just because they ran out of cash early on. It\u0026rsquo;s a real shame for all involved.\nRefactor \u0026hellip;refactor refactor\nDon\u0026rsquo;t save this until the end of a piece of work \u0026hellip; you\u0026rsquo;re bound to miss something and possibly add to your team\u0026rsquo;s tech debt. Plus, if you push your commits to a PR, you\u0026rsquo;ll get your ass handed to you by your peers!\nThings to consider here are DRY and TDD. Both will nudge you towards a proper refactoring effort.\nSeparation of Concerns (think MVC, CQRS, bounded context, etc\u0026hellip;)\nIt\u0026rsquo;s all about doing the right this in the right place! I recently ran, architected and co-developed a project that involved our own hosted solution, a solution hosted on Azure and a solution hosted on the Twilio Cloud (Twilio Serverless Functions). Originally, the requirements did not include the Twilio Cloud and would have required a bucket load more effort if we\u0026rsquo;d stuck with that brief. Thankfully, I chose to take full advantage of what Twilio has to offer and used a combination of Twilio Flow and Twilio Serverless Functions. By establishing these SoCs it meant:\n a less stressful implementation a light touch to our own hosted solutions a satisfying amount of fun working with Serverless (has been my favourite and advocated approach for several years!) a time saving it revealed a range of options when dealing with specific edge and corner cases which, again, giving us a further time savings.  SOLID These are the SOLID principles:\n Single Responsibility Principle Open Closed Principle Liskov Principle Interface Segregation Principle Dependency Inversion Principle  Single Responsibility Principle A class (no method) should have one and only one reason to change, meaning that a class (or method) should have only one job.\n\u0026ldquo;When a class has more than responsibility, there are also more reasons to change that class\u0026rdquo;\nHere\u0026rsquo;s an example of a class )purposefully awful for illustrative purposes):\nclass User() { public string Username {get; set;} public string Fullname {get; set;} private readonly ILogger _logger; private IDbContext _db; public User() { _logger = new Logger() _db = new UserContext(); } public Task\u0026lt;User\u0026gt; GetProfile(string username) { ... _logger.Info($\u0026#34;Found profie for {username}\u0026#34;) return this; } } You could say that the above includes both a model responsibility and a service responsibility. These should be split into two separate .NET types, as in this example:\nclass User() { public string Username {get; set;} public string Fullname {get; set;} public User(string username, string Fullname) { ... } } class UserService() { private readonly ILogger _logger; public UserService(ILogger _logger, IDbContext db) { _logger = _logger _db = db; } public Task\u0026lt;User\u0026gt; GetProfile(string username) { ... _logger.Info($\u0026#34;Found profie for {username}\u0026#34;) return user; } } Here are the benefits of principles:\n Reduces complexity in your code Increases readability, extensibility, and maintenance of your code Reusability and bug breading Easier to test Reduces coupling by removing dependency between methods  Open Closed Principle Objects or entities should be open for extension, but closed for modification. So, what does this mean? Let\u0026rsquo;s break this down to two statements:\n Open for extension Closed for modification  Open for extension: This means that we need to design our classes in such a way that it\u0026rsquo;s new responsibilities or functionalities should be added easily when new requirements come.\nOne technique for implementing new functionality is by creating new derived classes. A derived class will inherit from base class. Another approach is to allow the \u0026lsquo;client\u0026rsquo; to access the original class with an abstract interface. I sometimes think of this simply as removing if statements by extension but I\u0026rsquo;m not convinced everybody would agree with this assessment though.\nSo, in short, if there\u0026rsquo;s an amendment or any new features required, instead of touching the existing functionality, it is better to create new derived class and leave the original class implementation. Well, that\u0026rsquo;s the advice! I worry about the class explosion and if you\u0026rsquo;re attempting to do this on top of not so perfect code!\nClosed modification: This is very easy to explain\u0026hellip;only make modifications to code if there\u0026rsquo;s a bug.\nThis sample looks at delegating method logic to derived classes.\npublic class Order { public double GetOrderDiscount(double price, ProductType productType) { double newPrice = 0; if (productType == ProductType.Food) { newPrice = price - 0.1; } else if (productType == ProductType.Hardware) { newPrice = price - 0.5; } return newPrice; } } public enum ProductType { Food, Hardward } Can rewrite, still using base implementation (think decorator pattern):\npublic class Order { public virtual double GetOrderDiscount(double price) { return price; } } public class FoodOrder : Order { public override double GetOrderDiscount(double price) { return base.GetOrderDiscount(price) - 0.1; } } public class HardwareOrder : Order { public override double GetOrderDiscount(double price) { return base.GetOrderDiscount(price) - 0.5; } } Liskov Principle Definition: \u0026ldquo;Let q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T.\u0026rdquo; \u0026hellip; clear as mud right?\nAll this is stating is that every subclass/derived class should be substitutable for their base/parent class.\nThe example below demonstrates a violation of the Liskov principle, as by replacing the parent class (SumEvenNumbersOnly-\u0026gt;Calculator), this does compromise the integrity of the derived class as the higher-order class is not replaced by the derived class. Here, both cal and eventsOnly variables will be the same:\n... var nums = new int[] {1, 2, 3, 4, 5, 6, 7}; Calculator cal = new Calculator(nums); Calculator evensOnly = new SumEvenNumbersOnly(nums); ... public class Calculator { protected readonly int[] _numbers; public Calculator(int[] numbers) { _numbers = numbers; } public int Sum() =\u0026gt; _numbers.Sum(); } public class SumEvenNumbersOnly : Calculator { public SumEvenNumbersOnly(int[] numbers) : base(numbers) { } public new int Sum() =\u0026gt; _numbers.Where(x=\u0026gt;x % 2 == 0).Sum(); } Here we have changed the assumed base class to an abstract class. Now, it can\u0026rsquo;t be instantiated and instead, must be inherited. This ensures the derived classes must implement the method detail. So, even if we replace the type declaration with the higher-order class, we should still get the intended result:\n... var nums = new int[] {1, 2, 3, 4, 5, 6, 7}; Calculator cal = new SumAllNumbersOnly(nums); Calculator evensOnly = new SumEvenNumbersOnly(nums); ... public abstract class Calculator { protected IEnumerable\u0026lt;int\u0026gt; _num; protected Calculator(IEnumerable\u0026lt;int\u0026gt; num) { _num = num; } public abstract int Sum(); } public class SumAllNumbersOnly : Calculator { public SumAllNumbersOnly(IEnumerable\u0026lt;int\u0026gt; num) : base(num) { } public override int Sum() =\u0026gt; _num.Sum(); } public class SumEvenNumbersOnly : Calculator { public SumEvenNumbersOnly(IEnumerable\u0026lt;int\u0026gt; num) : base(num) { } public override int Sum() =\u0026gt; _num.Where(x =\u0026gt; x % 2 == 0).Sum(); } Interface Segregation Principle A client should never be forced to implement an interface that it doesn\u0026rsquo;t use or clients shouldn\u0026rsquo;t be forced to depend on methods they do not use.\nTake the following interface:\npublic interface IAllTheThings { Task\u0026lt;IAsyncEnumerable\u0026lt;Claim\u0026gt;\u0026gt; GetClaims(string username); Task\u0026lt;IAsyncEnumerable\u0026lt;User\u0026gt;\u0026gt; GetUsers(string team); Task\u0026lt;User\u0026gt; AddUsers(User user); } There\u0026rsquo;s a clear distinction in responsibilities that are being suggested here by the contract name. Sufficed to say, these should be split across different interfaces:\npublic interface IUser { Task\u0026lt;IAsyncEnumerable\u0026lt;User\u0026gt;\u0026gt; GetUsers(string team); Task\u0026lt;User\u0026gt; AddUsers(User user); } public interface IClaim { Task\u0026lt;IAsyncEnumerable\u0026lt;Claim\u0026gt;\u0026gt; GetClaims(string username); } Dependency Inversion Principle There are 2 rules here:\n High-level modules should not depend on lower-level modules. Both should depend on abstractions. Abstractions should not depend upon details. Details should depend upon abstractions.  Let\u0026rsquo;s deal with the first rule first. High-level means policy, business logic and the bigger picture. Lower-level means, closer to the bare metal (think I/O, networking, Db, storage, UI, etc\u0026hellip;). Lower-level tend to change more frequently too.\nThese two examples show perfectly the before and after of the move to a \u0026lsquo;depend on abstraction\u0026rsquo;:\npublic class BusinessRule { private DbContext _context; public BusinessRule() { _context = new DbContext(); } public Rule GetRule(string ruleName) { _context.GetRuleByName(ruleName); } } public class DbContext { public DbContext() { } public Rule GetRuleByName(string name) { return new Rule(new {Name = \u0026#34;Allow All The Things\u0026#34;, Allow = false}) } } After changing to an abstraction:\npublic interface IDbContext { Rule GetRuleByName(string name); } public class BusinessRule { private IDbContext _context; public BusinessRule(IDbContext context) { _context = context; } public Rule GetRule(string ruleName) { _context.GetRuleByName(ruleName); } } public class DbContext : IDbContext { public DbContext() { } public Rule GetRuleByName(string name) { return new Rule(new {Name = \u0026#34;Allow All The Things\u0026#34;, Allow = false}) } } With the above change, the DbContext can be any class as long as it inherits from the IDbContext interface and has a method with a signature of Rule GetRuleByName(string name).\nThe above is demonstrative of the 2nd rule; do not depend on the detail. As you can see, in the example above, we\u0026rsquo;re depending on an interface method contract and the actual implementational detail is being dealt with by the Lower-level class.\nThe above example includes Dependency Injection. Although you can accomplish IoC with DI, they are not the same thing. IoC does not mention anything about the direction of the dependency.\nGeneralization restrictions The presence of interfaces to accomplish the Dependency Inversion Pattern (DIP) has other design implications in an object-oriented program:\n All member variables in a class must be interfaces or abstracts All concrete class packages must connect only through interface or abstract class packages No class should derive from a concrete class No method should override an implemented method All variable instantiation requires the implementation of a creational pattern such as the factory method or the factory pattern, or the use of a dependency-injection framework.  Testing (unit/functional, including concepts like TDD \u0026amp; BDD and frameworks)\nFor testing to be a success, the details are key. These details will come in the form of a specification or from a verbal conversation (always to be confirm in writing later). If you\u0026rsquo;re lucky, these test cases will be included as ACs (Acceptance Criteria) in the Scrum Story Description.\nTaking a test driven development approach to writing code often results in:\n a reduction in verbose code less post-deployment bug fixing succinct (do no more, no less than is required), structure and logic.  Testing is important. Obviously! I often refer to testing as \u0026lsquo;having your back\u0026rsquo;. It ensures you don\u0026rsquo;t break existing functionality when implementing new functionality or dealing with tech debt. It also protects new engineers from breaking things as well as extant engineers who may have touched this repository many times before.\nTests aren\u0026rsquo;t just for new functionality either. If you change extant functionality or class responsibilities you must modify extant tests or create new tests. Ideally, your CI build pipeline should run tests every time time a commit(s) is pushed to a PR or Draft PR. This last step is here, to again, have your back and to safeguard against erroneous code poluting your codebases and getting into production.\nIn the .NET world, there are many testing frameworks available; xUnit, NUnit, MSTest to name a few. There are also many mocking frameworks available; Moq, NSubstitute, JustMock, again, to name a few. Frameworks like these help make the testing process and overall experience less painful and cumbersome and some might even say it makes this part of development, pleasurable!\nMy .NET Core testing and mocking preferences are xUnit \u0026amp; Moq and my javascript (including node.js) testing framework preference is Jest.\nThis code sample shows how both a testing and mocking frameworks compliment each other:\nusing Moq; using Xunit; namespace BasicAAATestExample { public interface IUser { string GetFullname(); string Firstname { get; set; } string Lastname { get; set; } } public class User : IUser { public string Firstname { get; set; } public string Lastname { get; set; } public string GetFullname() { return $\u0026#34;{Firstname} {Lastname}\u0026#34;; } } public class Notify { private IUser _user; public Notify(IUser user) =\u0026gt; _user = user; public string GetMessage() =\u0026gt; $\u0026#34;{_user.GetFullname()} has been notified\u0026#34;; } public class NotifyTests { [Theory] [InlineData(\u0026#34;Garrard\u0026#34;, \u0026#34;Kitchen\u0026#34;, \u0026#34;Garrard Kitchen has been notified\u0026#34;)] [InlineData(\u0026#34;Charles\u0026#34;, \u0026#34;Kitchen\u0026#34;, \u0026#34;Charles Kitchen has been notified\u0026#34;)] public void GivenGetMessageIsCalled_WhenFirstAndLastNameExist_ThenReturnsANotificationMessage(string firstname, string lastname, string expected) { // arrange  var mockUser = new Mock\u0026lt;IUser\u0026gt;(); mockUser.Setup(x =\u0026gt; x.GetFullname()).Returns($\u0026#34;{firstname} {lastname}\u0026#34;); var sut = new Notify(mockUser.Object); // act  string message = sut.GetMessage(); // assert  Assert.Equal(expected, message); mockUser.Verify(x =\u0026gt; x.GetFullname(), Times.Once); } } } The single unit test above follows the AAA (Arrange, Act, Assert) pattern and is a common way of writing unit tests for a method under test:\n the Arrange section of a unit test method initializes objects and sets the value of the data that is passed to the method under test the Act section invokes the method under test with the arranged parameters the Assert section verifies that the action of the method under test behaves as expected.  There are a few standards I adhere to when it comes to writing tests. In the sample unit test above these standards include:\n the method name (GWT) the comment blocks of arrange, act and assert the name of the mock instantiated object (mock\u0026lt;Class\u0026gt;) the class name of the SUT - system under test - (sut).  YAGNI (you ain\u0026rsquo;t going to need it)\nDo no more, and no less than is required. You do not want to have to maintain code that is never used or produce code that others have to maintain unwittingly. It\u0026rsquo;s very difficult to future proof your code if you do not know what\u0026rsquo;s going to happen, let alone without a specification! It\u0026rsquo;s a guess at best so don\u0026rsquo;t waste your time or others. Keeps things concise, succinct and simple.\n My pattern discovery I\u0026rsquo;m a huge fan of patterns, especially cloud architectural patterns but sometimes, they add unnecessary complicity so beware!\nWhen I first started learning about patterns - some 18 years ago - I went through a few codebases I was involved with at the time to see if I\u0026rsquo;d subconsciously been repeatedly using a pattern \u0026hellip; and I had! It was the lazy loading pattern\u0026hellip;which I continue to use regularly today!\nBeing a Principal Engineer As a Principal Engineer, I consider the above as the foundation for writing quality code. The objective of this list, in conjunction with the message I propagate via this list, during discussions, evidence from my own work and by leading from the front within my role, is one of a reminder to me and my colleagues of best practice and commitment to quality and good practice. As with all foundations, it forms the base from which more advanced concepts or approaches can be learned. An important part of this practice is heuristic - enabling a person to discover or learn something by themselves. So, how do I go about doing this?\nThese are some of the activities I execute to embed good engineering principles:\n 1-2-1 Group conversations Advocate online learning platforms such as Pluralsight or Udemy. For the more keen Engineer, I also recommend certification. YouTube is another favourite of mine. With YouTube, you can tag recordings, therefore building up a catalogue of materials that you can make public Workshops Brown bags Capture How To Do\u0026rsquo;s in wikis or similar Coding advice/tips (e.g. when to use Task instead of an Async method) Take the time to explain questions about implementation reasons in DVCS Pull Requests Share blog posts \u0026amp; other materials across multiple channels Compile a learning profile for an individual  The coding advice/tips above are interesting ones. As professionals, we always want to improve our ability to code, how we approach problems, etc\u0026hellip;, and in doing so we want our colleagues to benefit from our experience. I recently became reacquainted with coding katas. As a black belt in Ju-Jitsu I am well versed in what a kata is. Katas can also be used to remind, stretch and improve our core coding capability. The last time I used a kata in programming was 10+ years ago. This was when I was first introduced to TDD. A favourite development book of mine is \u0026lsquo;The Art of Unit Testing\u0026rsquo; by Roy Osherove. It is the first edition. For many years I had it as a click-thru purchase option on a previous blog site of mine. I\u0026rsquo;ve not really participated in many katas since. I have written a few though and now having been reintroduced to them and reminded of their potential, as a Principal Engineer I can see it as an invaluable tool. One thought I\u0026rsquo;ve had is to use it as a framework to assess an Engineer\u0026rsquo;s current capability and then use during pair programming to help share coding techniques, approaches and standards.\nPair programming is a wonderful technique for propagating developer skills (think how to use cloud services), approaches to coding (think TDD and problem solving), embed team coding standards and code review in realtime. Pair Programming is an Extreme Programming technique. It is not a mentoring or coaching technique but some do use it for this. Quite often, I find I only participate in pair programming is one of two use cases. (1) if the subject I\u0026rsquo;m investigating is new (important to have shared knowledge) and (2) when I\u0026rsquo;m helping an Engineer to overcome an esoteric issue. You know what they say? \u0026hellip;a problem shared is a problem halved! However, now, I\u0026rsquo;ll be including Pair Programming in conjunction with katas as part of my techniques to stretch the Engineer\u0026rsquo;s muscle memory (including mine!).\nI love hacking away at code as much as the next Engineer. Hacking code is a great way to experiment and to get out of the starting gate. However, when it comes to pair programming I do like to give it the necessary due diligence. By this I am referring to allowing for a few moments up front to deliberate and agree on what\u0026rsquo;s required. This checklist guides me and ensures up front I set off in the right frame of mind:\n the objective of the pair programming exercise (think the desired outcome - you might even want to frame this with a Story Description; AS A, I WANT, SO THAT) what libraries (3rd party) will we need (think cloud provider SDKs and vendor client APIs) how are we going to test the code we write (think unit tests, integration, functional [e2e] as well as your approach e.g. TDD).  After the session has finished I like to perform one final task. This is to document findings/learnings and areas that require further investigation. This is normally helped by capturing notes as we go along.\nAs a side note to TDD, with modern compilers (think Roslyn in the .NET world) and even linting to a certain extent, you know if something will fail - if a reference type (.NET) does not exist yet - as your IDE will be screaming at you, so I don\u0026rsquo;t run tests that are missing these reference types (think classes and interfaces in the .NET world).\nDiscussion point I\u0026rsquo;m sure I\u0026rsquo;m not alone here when I say, having the time available for 2 Engineers to code together for skills transfer etc is a challenging one. An agile sprint doesn\u0026rsquo;t facilitate this. This is something that I often refer to as having the \u0026lsquo;space to learn\u0026rsquo;. The pressures of a sprint often, sadly, works against this. This is doubly as difficult, if your sprint is made up of technical debt, BAU, Ad-hoc etc\u0026hellip; Timeboxing \u0026lsquo;effort\u0026rsquo; into percentages doesn\u0026rsquo;t always present an obvious education path for your Engineers either. Having a day (developer day or similar) dedicated to learning also never really quite works out the way it\u0026rsquo;s meant too, plus, \u0026lsquo;a day\u0026rsquo;?! In my experience, this, and trying to cram genius into a time box also never quite works either. After all, you can\u0026rsquo;t schedule genius, in the same way, you can\u0026rsquo;t guarantee that the best Engineers are in your locality, or that the best time for Engineers to work is between 9-5.\nWhat is the answer? A mixture of all the above, at hock and at scheduled times, to ensure quality and advancement of skills.\nWhen I do speak out regarding the above, I inevitably also lead this conversation into Engineering not having the kit [hardware \u0026amp; software] they need. Engineers require the software and hardware they deem as necessary to be effective in their role. I once gave an analogy of, not giving Engineers the right kit is like giving a roller brush and a Pogo stick to Michelangelo to paint the Sistine Chapel ceiling. He\u0026rsquo;ll manage it \u0026hellip; eventually, but the attention to detail and accuracy will be woefully inadequate.\n Written mainly for me, I do hope you\u0026rsquo;ve found something useful here, and who knows, it may even help you with your engineering journey too.\n References   The pragmatic programmer\n  YAGNI\n  XP\n  Dependency inversion principle\n  OOP design patterns\n  Inversion of Control Containers and the Dependency Injection pattern\n  Write your tests\n  "});index.add({'id':12,'href':'/posts/','title':"Blog",'content':"This section contains articles form my old blogging site. Not all have been ported so will look sparse\n"});})();