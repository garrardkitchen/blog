"use strict";(function(){const t={cache:!0};t.doc={id:"id",field:["title","content"],store:["title","href"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/posts/compare-resources-from-terraform-plan/",title:"Compare Resources From Terraform Plan",content:`Here&rsquo;s a link to a repo I created today, that will be used to host examples of all challenges I encounter that relate to Terraform.
In this particular sample, I need to list out all those resources that will be created using Terraform where we do not have a state file. The product of this will be added to sheets, by Resource Group, in an Excel spreadsheet. Each sheet is an Azure Resource Group. The rationale for this is that at some point an apply was executed, and due to the state not being managed, we do not know if the current config matches with what has been deployed.
The Excel spreadsheet gives us something organised and visual to use to compare. This will also be used to confirm the correct naming convensions have been used.
`}),e.add({id:1,href:"/posts/error-NETSDK1045/",title:"Error NETSDK1045",content:"Error NETSDK1045 The current .NET SDK does not support targeting .NET 6.0 A colleague had Visual Studio shout this (see üëá) at him when he loaded up a FunctionsApp project. He received this error twice as the second project was the unit tests for the FunctionsApp.\nError NETSDK1045 The current .NET SDK does not support targeting .NET 6.0. Either target .NET 5.0 or lower, or use a version of the .NET SDK that supports .NET 6.0. TestNasLinuxFuncAppWebTests C:\\Program Files\\dotnet\\sdk\\5.0.409\\Sdks\\Microsoft.NET.Sdk\\targets\\Microsoft.NET.TargetFrameworkInference.targets 141\rI interpretted this as the newer targets were not supported by the existing SDK. Yeah, genius right üòÅ.\nHowever, even after installing .NET 6.0 and the Azure Functions Core Tools using it didn&rsquo;t fix the issue:\nwinget install -e --id Microsoft.DotNet.SDK.6 winget install -e --id Microsoft.AzureFunctionsCoreTools So, like most at this stage, I consulted with a trusted colleauge (Google) and found this in the MS Documentation - https://learn.microsoft.com/en-us/dotnet/core/tools/sdk-errors/netsdk1045. He did reboot his vm, yet the error remained.\nI did also find a SO suggesting upgrading to VS2022 would fix this issue.\nI couldn&rsquo;t find the reciprocal recommendation in MS documentation but he did upgrade to vs2022 regardless. This did the trick. ü•≥\nConclusion\nIf you get Error NETSDK1045 in Visual Studio then upgrade to the latest verison of VisualStudio.\n"}),e.add({id:2,href:"/posts/how-to-write-clipboard-to-file/",title:"How to Write Clipboard to File",content:`Have you ever wondered how to persist your clipboard to disk? Wonder no more&hellip;
To get started, copy a random sentence into your clipboard like so:
Set-Clipboard -Value &#34;This is being copied into clipboard&#34; Let&rsquo;s confirm the contents of the clipboard, like so:
Get-Clipboard output:
This is being copied into clipboard We&rsquo;ll now persist the same cliboard to a file, like so:
Get-Clipboard | Out-File -FilePath clipboard.txt To confirm the contents of this file, we type:
Get-Content .\\clipboard.txt output:
This is being copied into clipboard Let&rsquo;s append something else into the clipboard like so:
Set-Clipboard &#34;appended!&#34; -Append And finally, we confirm that this text has indeed been copied into the clipboard like so:
Get-Clipboard output:
This is being copied into clipboard appended! There you have it, a short post on how to obtain and set the clipboard using Powershell commands.
Module: Microsoft.PowerShell.Management
`}),e.add({id:3,href:"/posts/how-to-conditionally-include-a-nuget-package/",title:"How to Conditionally Include a Nuget Package",content:`The context I have implemented a solution that as it&rsquo;s primary objective iterates through a sequence of HTTP Requests and page interactions. With sequencing through these steps regularly it gives the support team the maximum amount of time to react to an outage. This outage can be isolated to this particular service, or can originate from any of it&rsquo;s downstream dependencies.
To help with the automation of these steps, I&rsquo;m using Selenium and in particular, it&rsquo;s headless browser capability in conjunction with ChromeDriver. With this combination, I can navigate within a Chrome browser, provide alphanumerical inputs and execute mouse clicks, all by using imperative code.
I am using serverless technologies to orchestrate it&rsquo;s delivery, and for this solution&rsquo;s resiliency. The particulars include an Azure FucntionsApp (Linux Container) that hosts the C# .NET 6.0 FunctionsApp, and the ChromeDriver which happens to be an external program (ergo, not managed code). I use the DevOps approach to build the FunctionsApp and associated unit tests, run the unit tests, produce code coverage stats (with min threshold), push the Docker image to ACR, deploy to a slot, assess error rates by calling az monitor log-analytics query &hellip; then performing a swap with the production slot post a manual verification step. These steps are incorporated into separate pipelines within Azure DevOps. There are multiple pipelines, one for infrastructure and one for the application. For the IaC I use the bicep DSL. I&rsquo;m using common pipeline patterns such as variable and jobs templates. I&rsquo;m also interacting with the az pipelines (azure-devops cli extension) to create/update variable-groups and variables.
The issue During the execution of this FunctionsApp, I have observed warnings reporting a ChromeDriver mismatch with the Chrome browser, in the logs. This needed to be addressed.
My Dockerfile installs the latest stable version of Chrome - which happens to be version (major - semantic versioning) 105. However, my local development environment is constrained to using Chrome 104. To avoid any potential incompatibility issues, as well as to quieten these annoying warnings, I somehow need to implement a solution that requires zero maintenance.
The solution Thanks to Mrs Google, and DDoSing this search engine with of terms such as nuget, conditions, linux, I eventually cobbled together a way to conditionally include package references. IDK there is a choose element within a csproj xml schema. I do now!
I felt that the real challenge here was to include a package based on the intended operating system of the runtime. In dotnet vernacular, this is know as a Runtime Identifier. Examples of such are, linux-64, win-64, win-32. You provide this Runtime Identifier by using the -r (runtime) switch. A full example can be found later in this post.
Additionally, I felt it was important that the DX would not suffer consequently. So, I needed someway to set a default. This default will come into play when a developer runs this FunctionsApp via:
the dotnet run cli command or from within an IDE (Rider, VS) or code editor such as VSCode. In this example xml snippet, you can see how I&rsquo;ve implemented a default and how I used the Condition attribute to include packages based on the Runtime Identifier:
&lt;Choose&gt; &lt;When Condition=&#34;$(RuntimeIdentifier) != &#39;&#39;&#34;&gt; &lt;ItemGroup&gt; &lt;PackageReference Condition=&#34;$(RuntimeIdentifier.StartsWith(&#39;win&#39;))&#34; Include=&#34;Selenium.WebDriver.ChromeDriver&#34; Version=&#34;104.0.5112.7900&#34; /&gt; &lt;PackageReference Condition=&#34;$(RuntimeIdentifier.StartsWith(&#39;linux&#39;))&#34; Include=&#34;Selenium.WebDriver.ChromeDriver&#34; Version=&#34;105.0.5195.1900&#34; /&gt; &lt;/ItemGroup&gt; &lt;/When&gt; &lt;Otherwise&gt; &lt;ItemGroup&gt; &lt;PackageReference Include=&#34;Selenium.WebDriver.ChromeDriver&#34; Version=&#34;104.0.5112.7900&#34; /&gt; &lt;/ItemGroup&gt; &lt;/Otherwise&gt; &lt;/Choose&gt; I specify the runtime identifier of linux-64 in the Dockerfile when I publish the .NET Core FunctionsApp. This can be seen in the command snippet below. This will ensure that the version (major) 105 is included:
dotnet publish -p:PublishChromeDriver=true .\\my-functionsapp.csproj -c release --self-contained -r linux-x64 --output ./publish And if you set the verbosity of the output by adding this switch -v d (detailed) you will have visual confirmation that the appropriate version has been included:
And there you have it, an example of how to include a different version of a nuget package.
`}),e.add({id:4,href:"/posts/dotnet-stack-and-heap/",title:".NET Stack, Heap and Boxing",content:`This week I have been investigating how to reduce memory allocation in a few HTTP APIs. I won&rsquo;t go into any explicit work-related examples here but I will touch on facets relating to this effort.
Let&rsquo;s start off by looking at Reference Types and Value types and how they get allocated into the Heap. I will also touch on concepts such as boxing and GC pressure.
Let me start off with some facts:
A Reference types always get allocated on the Heap A Value type mostly get put on the stack, but do get placed on the Heap sometimes. If Value type is a class field (hoisted top-level class field), this gets allocated on the Heap along with it&rsquo;s parent. If Value type is boxed, this too gets allocated on the Heap A ref Struct will always be added to the Stack as the compiler will not allow it (field cannot be of byref-like type) to be added as a class field, but Value type in the Struct will be allocated if boxed Yikes, confusing yes?
Examples I&rsquo;m going to break off into some examples next to help explain why and when Value Types get allocated.
Example 1: A Class Field
In this class, we can see that _attempts is a Value Type:
public class NewOrder { private int _attempts = 0; public void PlaceOrder() {} } Due to it being a class field, it is allocated on Heap with it&rsquo;s parent NewOrder. This is seen here:
The same will happen if you hoist a Struct to the root of the class. Take this code for instance, the Item Struct is placed on the Heap along with it&rsquo;s parent (see screengrab below):
public class Order { private Item _item = new Item(); public Order() {} } public struct Item { public int Id { get; set; } public string Sku { get; set; } } Example 2: Boxing
Class Properties Value Types don&rsquo;t get placed on the Heap. However, if they get boxed (eg via string interpolation, method group, lambda expression), then they do.
This example shows the result on the ItemCount property after it gets boxed via string interpolation:
Concepts Boxing and unboxing
Boxing is process of converting a Value Type to the type object (aka implicit conversion). It creates a new allocation in the Heap, copies in the Values type value and returns a reference.
See the last int32 instance in this screengrab:
Unboxing, on the hand, is the reverse and is the process of converting a type object to a Value Type (aka explicit conversion).
In this example, you see that the unboxing doesn&rsquo;t get allocated onto the Heap:
GC Pressure You may be asking the question, &ldquo;why is any of this important?&rdquo;
One reason it will become relevant is if you are observing GC pressure.
GC pressure means that the GC is feeling the strain and increasingly becoming overwhelmed deallocating memory. This could be the result of an incorrect GC configuration. If you&rsquo;re not careful, your production docker container workload(s) may not have adeqaute available private memory. If this is the case then GC will be working twice as hard to avoid an OOM exception. Lack of memory will kill your app. Lack of CPU however will simply throttle your app and will not result in your app being kill. There is one example when this isn&rsquo;t exactly true. Let&rsquo;s say you have a pod running in your kubernetes cluster, and your configuration includes both Liveness and a Readiness probes. If your CPU is maxed out and your HTTP Listener can&rsquo;t receive and respond to HTTP requests during the time the probes properties allow (combination of periodSeconds, failureThreshold and timeoutSeconds), then neither probe will have the availability to inform AKS that it&rsquo;s still alive but just busy so don&rsquo;t shut me down. So, the inevitable will happen. Yes, AKS will kill your pod and restart another; providing you&rsquo;ve a ReplicaSet configured.
Reviewing your code and identifying changes that can reduce allocation, will help. It&rsquo;s not the only approach. More often than not though, especially with a focus on using less space (Big O Notation), will result in larger method frames, plus verbose code, plus determinate collection sizes, etc&hellip; .
Also worth noting here too is that whenever GC executes, your running application will stop and will resume once GC completes.
To reiterate an earlier point, there are many approaches to reducing GC pressure. These are well documented and are easily found on the internet. I&rsquo;ve included several below for completeness:
Avoid memory leaks (big topic!) Use (when appropriate) in place of a Class Using a StringBuilder correctly Use .NET 6 &amp; c# 10. String interpolaton uses the new DefaultStringInterpolatedHandler, avoiding unnecessary boxing on Value Type values. Avoid finalizers Setting the initial seize of a dynamic collection ArrayPool for short-lived arrays (large) As a side note here due to the inclusion of the .NET 6 and string interpolation point above, I&rsquo;ve now been using .NET 6 for a few months. This includes both exploratory and new projects. I was especially keen to start using .NET 6 from the benefits from process isolation (out-of-process) - fewer conflicts, DI and full control of the process that we&rsquo;re all used to with paradigms outside of the serverless model. I have grown to like the minimum API. Like most, I initially felt uneasy with the lack of c# verbosity but now welcome it. I do, and I am sure I am not alone here, have been using both BenchmarkDotNET and SharpLab to compare performance and language decompilation. I didn&rsquo;t bother much with .NET 5. This was due to .NET 5 never having the LTS label.
References Boxing/unboxing `}),e.add({id:5,href:"/posts/the-grass-is-rarely-greener/",title:"The Grass Is Rarely Greener",content:`I recently had a conversation that stirred up some surpressed memories. The conversation was related to moving to a different cloud vendor. That journey didn&rsquo;t work out so well for me and the company I was working for at the time. Sadly, that company stopped trading and I don&rsquo;t think it&rsquo;s that much of a leap to link that journey to the demise of that company. Hopefully, now the title of this post is starting to make more sense?
Several years back, the company that I was working for at the time shifted to AWS, away from Azure. It was heavily suggested - I was present during that conversation - that it would be in our interest if we &ldquo;jumped ship&rdquo; to their platform. &ldquo;Their&rdquo; being &ldquo;AWS&rdquo;. Long story short, sadly it was not. In fact, it turned out to be a huge waste of time. IMO, they should never have suggested this. There were other suggestions made that they really should not have made; sadly we followed these too. In retrospect, it was totally unjustifiable not to mention unfair for them to suggest any of this for a company of that size [small], especially knowing how long all of this would take to do. I even gained 2 AWS certifications to help best advise the company on how to implement/integrate with the AWS Platform and backing services.
If you are thinking, &ldquo;You are grown-ups with years of experience under your belt yeah, so you didn&rsquo;t have to go with what they said did you?&rdquo;. Indeed, but it was AWS and these people we were talking to were big hitters and as such, the &ldquo;carrot&rdquo; was big. Who wouldn&rsquo;t right?!
I will return to this in a later post but for now I will call out, IMO, several important and possibly overlooked considerations when contemplating a move to a different Cloud Provider.
Considerations when moving to another Cloud Provider Here are a few nuggets that will hopefully resonate:
1 - I don&rsquo;t know of any business that will allow their &ldquo;technical people&rdquo; to have their way and purely focus on tech debt or similar, without developing features (or improvements) over a prolonged amount of time. If part funded with VC money, I&rsquo;m sure they won&rsquo;t be too happy either - further delay on their return, and for what? With all the best intentions in the world, migrations are time-consuming (not to mention a scheduling nightmare), and this time is exponential when dealing with new or unfamiliar tech. Tech will also present problems, regardless of where it is.
2 - Cloud vendors offer essentially the same backing services - orchestration, serverless compute, VM compute, db, caching, temporal decoupling thru queues, and the list goes on&hellip; . They may make it easier or more attractive for particular language techs and ecosystems, but essentially, it&rsquo;s the same offerings plus patterns (EIP but in the Cloud) but elsewhere. Not to mention they all have their own version of the 5 pillars of the &ldquo;Well-Architecture&rdquo; framework.
3 - As an extension of (2) above, there&rsquo;s training to consider. Also, new problems to solve, and migration paths to figure out. Not to mention RBAC and ownership. And don&rsquo;t forget DevOps, possibly made slightly easier if you&rsquo;ve opted to a non-native IaC approach like TF (Terraform). And don&rsquo;t forget automation, runbooks, alerting, 3 pillars of observation (incl. distributed tracing). And don&rsquo;t forget&hellip; .
4 - If there&rsquo;s something that your current cloud vendor doesn&rsquo;t offer and the need is absolutely justifiable, you can also look for a SaaS alternative. When you boil it down, all you are really interested in are specific use-cases/features, integration options (SDKs) and it being a &ldquo;managed service&rdquo; as to avoid requiring additional Engineers to manage it and perform associated activities to ensure it&rsquo;s HA, patched and secure. Even if it&rsquo;s a service only offered by a different cloud vendor, there are light-touch integration options - for example, AWS Lambda and AWS Kinesis Data Streaming. This is an approach we&rsquo;ve had to take.
5 - It&rsquo;s always going to take longer than you think. Fact. We tend to think &ldquo;happy path&rdquo;.
6 - Now, this next one is a biggy &hellip; if you are moving to new tech, whether it be a new cloud vendor but more appropriately shifting to a new server-side language (eg from c# to go), you are going to lose between 20-40% of those Engineers who&rsquo;s core language is c#. I&rsquo;ve not observed the same levels of attrition on a shift to a different client-side tech. More often, you&rsquo;ll have several client-side techs in play. Maybe some core concepts of newly opted JS framework have changed, but essentially the language tech hasn&rsquo;t (JS/TS). Well, that&rsquo;s if you&rsquo;re not shifting to Blazor (WebAssembly). One major deficit of when your Engineers leave is that so does their domain knowledge üò±. Again, speaking from a position of experience, it&rsquo;s sad to see people go. Morale takes a hit. And when Engineers leave, their roles need replacing too. Recruitment ensues, which requires planning plus patience, often the cause of disruptions and then it takes other Engineers&rsquo; time for onboarding and orientation.
7 - If cost is a consideration or motivation for the move then what are you doing re: the Cost Optimisation tenet of your provider&rsquo;s flavour of a &ldquo;Well-Architected&rdquo; Framework? More important than this but less obvious, what is the Cloud Service Provider (CSP) or organisation that is the conduit between you and the Cloud Vendor doing to help? In my experience, they only get involved if you need help with an issue, or if there&rsquo;s a zero-day exploit you need to be made away of or other type of issue involving one of your backing services. Anything difficult, then they tend to hand off to the Cloud Vendor themselves, but hey, at least they&rsquo;re on the same support call with you! üëÄ. They generally don&rsquo;t get in your face either if you&rsquo;re spending a futune! You may get the occasional email from them letting you know that you are very important to them. IMO, they need to be continually proving their worth to you [your organisation] and if they&rsquo;re not, engage a different CSP or employ certified professional(s) who can evaluate your architecture, and your DevOps processes as well as costs. Essentially, you need certified/experienced Engineers to help to make real tangible contributions and improvements.
8 - If technical issues are plaguing you, then likely they&rsquo;ll manifest themselves also on a different cloud provider. So, if you&rsquo;ve invested heavily with a particular tech, or approach, and this is too costly, too risky to the business due to instability, first understand why first before jumping ship. You never know, you might be using it inefficiently or incorrectly. Take AKS for instance. It&rsquo;s hugely involved. If you get any part of that wrong, it&rsquo;s a risk to your business.
There will be more I&rsquo;m sure, but for the sake of getting closure on this topic and to put these emotions back into that box, I will leave it there.
Recommendations In summary, my advice is this; evaluate your current platform to identify (in no particular order):
Cost savings (eg there could be better bin packing or scaling options or up to 70% reduction on &ldquo;compute&rdquo; by using Reserved Instances) Configuration changes to improve stability, minimize failure on upgrading your dependencies, and self-healing Improvements on patterns and practices used by your Engineers (incl. low code options, delegate failure edge/corner cases to your architecture, consistency of solution - don&rsquo;t solve the same problem multiple ways!, only measure what you can action) Automation (eg runbook when metric exceeded, cron jobs) What is your prominent tech? For example, if your ecosystem is predominately .NET then Azure is a justifiable Cloud Platform. You simply won&rsquo;t get the same depth/scope of features/integrations if you go with a different Cloud Provider. Think about it&hellip;would you offer up all your crown jewels to a competing provider? I rest my case! I do hope this post has been informative. If I help one person avoid this more often than not wasted effort, then this post will have been worth it. Changes of this magnitude are laden with risk and one thing companies are adverse to is risk.
`}),e.add({id:6,href:"/posts/nodejs-install-e401/",title:"Nodejs Install E401",content:"Today I created a simple nodeJS Azure Functions Applicaiton to start building out a PoC and when I tried to install it&rsquo;s dependencies like so:\nnpm install I got this little cherub back instead:\nnpm ERR! code E401\nnpm ERR! Incorrect or missing password.\nnpm ERR! If you were trying to login, change your password, create an\nnpm ERR! authentication token or enable two-factor authentication then\nnpm ERR! that means you likely typed your password in incorrectly.\nnpm ERR! Please try again, or recover your password at:\nnpm ERR! https://www.npmjs.com/forgot\nnpm ERR!\nnpm ERR! If you were doing some other operation then your saved credentials are\nnpm ERR! probably out of date. To correct this please try logging in again with: npm ERR! npm login\nnpm ERR! A complete log of this run can be found in:\nnpm ERR! C:\\Users\\garrard.kitchen\\AppData\\Local\\npm-cache_logs\\2022-01-22T16_53_11_848Z-debug-0.log\nOk then, I&rsquo;ve obviously set my default registry to something other than npmjs!\nThis will not have been an issue or noticeable if I had have been connected to my works VPN as I know for a fact, the upstream sources of our npm Azure DevOps npm feed is in fact https://registry.npmjs.org.\nAnyhow, to work through this slight annoyance, I typed:\nnodejs install --registry https://registry.npmjs.org ... npm install --registry https://registry.npmjs.org added 78 packages, and audited 79 packages in 12s 30 packages are looking for funding run `npm fund` for details found 0 vulnerabilities One npm run prestart and one npm run start later, I was seeing this:\n[17:09:00] Starting compilation in watch mode... Azure Functions Core Tools Core Tools Version: 4.0.3971 Commit hash: d0775d487c93ebd49e9c1166d5c3c01f3c76eaaf (64-bit) Function Runtime Version: 4.0.1.16815 [17:09:01] Found 0 errors. Watching for file changes. Functions: svrless: [GET,POST] http://localhost:7071/api/svrless For detailed output, run func with --verbose flag. info: Microsoft.AspNetCore.Hosting.Diagnostics[1] Request starting HTTP/2 POST http://127.0.0.1:54476/AzureFunctionsRpcMessages.FunctionRpc/EventStream application/grpc - info: Microsoft.AspNetCore.Routing.EndpointMiddleware[0] Executing endpoint &#39;gRPC - /AzureFunctionsRpcMessages.FunctionRpc/EventStream&#39; [2022-01-22T17:09:02.473Z] Worker process started and initialized. All good, but now I need to change my defaults &hellip;\n"}),e.add({id:7,href:"/posts/k8s-pdb/",title:"Kubernetes Pod Disruption Budget and the Helm hasKey Function",content:`Pod Disruption Budget When working with Kubernetes, one crucial component of configuration is known as a PDB (Pod Disruption Budget). A PDB will ensure your workload remains running when you work through a Voluntary Disruption.
What on earth is a Voluntary Disruption? A Voluntary Disruption is when you trigger an action that causes the disruption. For example, if you wish to upgrade a Minor AKS version or any action that recycles a Node Pool. Click here ‚û° Disruptions to read up on what Disruptions are.
This is what a PDB manifest looks like this. This example tells Kubernetes to make sure there&rsquo;s always a minimum of 2 Pods running during a disruption:
apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: my-awesome-microservice-pdb spec: minAvailable: 2 selector: matchLabels: app: my-awesome-microservice-api We use Helm Charts so part the declaration that wraps the minAvailable property looks like this:
{{- if hasKey .Values &#34;pdb&#34; }} {{- if hasKey .Values.pdb &#34;minAvailable&#34; }} minAvailable: {{ .Values.pdb.minAvailable }} {{- end }} {{- else }} minAvailable: {{ max (sub .Values.replicaCount 1) 1 }} {{- end }} What on earth is going on here then?!
There are a few rules that I need to accommodate for within our workload PDBs. These are:
Apply a specific value that may be contained within a values .yaml file Provide a default value if one is not supplied Ensure there&rsquo;s at least 1 pod running throughout the disruption so a workload doesn&rsquo;t go offline during this period. üò± How to use the value provided by the developer I&rsquo;ve designed our CICD pipeline so we get base configurations (one .NET Framework IIS workloads, one for .NET Core Web workloads, one for &hellip; etc.) from one git repo, and get all application(s) configuration properties from the application git repo itself (üìÇ /.k8s/). It is here from within the application&rsquo;s repo we set the properties for a service(s) within a values-&lt;env&gt;.yaml file. If there&rsquo;s more than one application found in an application&rsquo;s git repo, the name is reflected in the name of values .yaml file to provide uniqueness - eg values-&lt;consumer&gt;-&lt;env&gt;.yaml.
It is in this values .yaml file we set - if at all - a value to the pdb.minAvailable nested property.
Here, in this control flow, we are checking that both pdb and minAvailable properties exist. If they do, we apply the minAvailable value. We use the hasKey function to good effect to check for the existence of a property in another property:
{{- if hasKey .Values &#34;pdb&#34; }} {{- if hasKey .Values.pdb &#34;minAvailable&#34; }} minAvailable: {{ .Values.pdb.minAvailable }} {{- end }} {{- else }} How to provide a default value If the application configuration does not contain a minAvailable property, we take the value found in the replicaCount value and use this. However, we do not insist on the same value, but instead 1 less - (sub .Values.replicaCount 1).
... {{- else }} minAvailable: {{ max (sub .Values.replicaCount 1) 1 }} {{- end }} How to ensure there&rsquo;s at least one pod running We must ensure at least one instance of a workload is running and if we simply reduced the replicaCount by one and left it at that, we could end up with a budget of zero. I don&rsquo;t want this to happen. What I do here is use the max function to good effect to safeguard against this ever being a zero - and if replicaCount: 1, then the expression would read max (0) 1, meaning 1 would be the value used.
... {{- else }} minAvailable: {{ max (sub .Values.replicaCount 1) 1 }} {{- end }} References Helm Functions Flow control Disruptions Best practices - Voluntary Disruption `}),e.add({id:8,href:"/posts/github-self-hosted-runner/",title:"Adding more Github Self-Hosted Runners",content:`Adding more GitHub Self-Hosted Runners To help build out our numbers of GitHub Self-Hosted Runner, we took a shortcut and had cloned an existing Linux VM.
Unfortunately, the by-product of doing this resulted in (a) the clonee (source) Linux VM had their Self-Hosted hijacked by the new VM and (b) we had a Runner registered in GitHub that didn&rsquo;t actually have a running runner - Offline ü§™.
Madness!
Ok, so what to do?&hellip;
These are the steps I worked thru to rectify this:
Firstly, remove Runner off of the new VM Next, rerun the config.sh step on new VM Finally, restart to Runner service on Clonee VM First, let&rsquo;s deal with the New VM First of all, we need to remove the correct Runner service so I ran:
sudo systemctl | grep runner ... sudo systemctl | grep runner actions.runner.&lt;org&gt;.&lt;hostname&gt;.service loaded active I then copied the above service (actions.runner.&lt;org&gt;.&lt;hostname&gt;.service) name into clipboard and past in below (&lt;service-name&gt;)
systemctl stop &lt;service-name&gt; systemctl disable &lt;service-name&gt; Next, I returned to the GitHub portal and navigated to the Runners page and selected the new Runner. What I&rsquo;m aiming to do here is to get a token that I can use to remove the Runner from the new VM:
I pressed the Remove button
then copied and executed this on the new VM:
./config.sh remove --token &lt;redacted&gt; I re-ran the config.sh by:
./config.sh --url https://github.com/&lt;org&gt; --token &lt;redacted&gt; sudo ./svc.sh install sudo ./svc.sh start At this point I saw the Self-Hosted Runner return to the GitHub Runners page - Showing as Idle and not as Offline.
Bring the Clonee back online At this point, I could now longer see the Clonee&rsquo;s hostname in the list of Runners.
I returned to the Clonee VM and ran:
cd actions-runners sudo ./svc.sh start This restarts the Runner as a service. It then became visible in the GitHub Runners page.
`}),e.add({id:9,href:"/posts/hybrid-origins-http-traffic/",title:"Hybrid Origins Http Traffic",content:`We&rsquo;re migrating our on-premise workloads to Azure. This has presented several challenges. One of which is what I am covering specifically here in this post and that is &hellip;
How to reduce code change effort?
This isn&rsquo;t about updating runtimes, this is about having workloads spread across different platforms that need to talk to each other (with some HTTP chaining üëÄ). It is not uncommon for one HTTP API to need to talk to another HTTP API. If you move one HTTP service to run in different zone, you will need update references in those ALL dependant services so they point to that new DNS Hostname. This isn&rsquo;t a big issue if you&rsquo;ve only few HTTP APIs. However, if you&rsquo;ve 100+ HTTP APIs that you&rsquo;re migrating, including many with several HTTP dependencies, then this quickly becomes daunting and a potential PR approval and deployment (multiple environments) scheduling nightmare. A simple domain name search in your organisation&rsquo;s Github account will illuminate my point.
Ok, so the nightmare scenario has been painted. What can you do.
With the HttpClients we&rsquo;re using - .NET Framework and .NET Core, as a default have AllowAutoRedirect as true. So why is this important. Well, if you want to simply have a redirect rule return a temporary redirect - 302 - then your HttpClient will automatically react to this and reissue the same HTTP Request. However, it does not use the original Authorization header. Yikes.
We use CloudFlare. They are the best at what they do. We&rsquo;re looking; as company we have an excellent relationship with CF and with that we get Stirling advice.
We originally used their Page Rule redirect to map to a different host. This is where we observed that the authz header being stripped on the AutoRedirect.
Ok, so what can be do? Well, we can make lots of additional code changes to plug this use-case in all HTTP Handler related code and be hated by all engineers for the rest of my days or look for a cross-cutting solution. Back to CF we go&hellip; . Two of my guiding principles for the migration effort is (1) to reduce cognitive overload and (2) effort required (this includes unnecessary code changes). Ultimately, simplify all facets of the migration. I don&rsquo;t want my engineers feeling the same levels of pain that I am.
I can confirm we have a solution and it is in play. Boo-yah!
There are few things you have to do. Here&rsquo;s a short list that I will go into more detail on shortly:
CF Page rules in domain you&rsquo;re mapping from CF CNAME set as DNS proxy DNS forwarders if you&rsquo;ve dev/test environment on-premise If you&rsquo;re using Windows VMs with IIS to server up HTTP APIs, you need to a new hostname to your bindings list nginx origin configs (to receive and forward requests from new hostname so you can have weighted traffic load-balanced across both new and old environments during the rollout of to the new target of a particular HTTP API) Yes, there&rsquo;s plenty there, most of which can be automated - think Terraform.
I&rsquo;m going to focus on changes to CF here as this is where the real magic happens!
`}),e.add({id:10,href:"/posts/k8s-selectors-and-labels/",title:"K8s Selectors and Labels",content:`Right, what&rsquo;s the deal with all the labels and metadata in a Deployment manifest?!!!!
Take this example:
apiVersion: apps/v1 kind: Deployment metadata: namespace: default name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx foo: baa template: metadata: labels: app: nginx foo: baa spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 Here, we see metadata twice, and also there&rsquo;s mention of matchLabels in selector??? What does it all mean???
Ok, let me explain üëÄ &hellip;
The first metadata reference A deployment manifest kind is a manifest that describes the desired state of your application(s). I say applications here as a POD can contain more than one container (application). The desired part of this is found in a ReplicaSet kind manifest. For example, you&rsquo;d use a ReplicaSet if you require to have 2 replicas (instances) of your POD running. A Deployment manifest is a short-hand way of stipulating this, ergo, saves you having to create 2 separate manifests. Makes sense? Good.
Behind the scenes, it is the Deployment Controller that monitors your deployment&rsquo;s desired state and if it differs, it will return to it&rsquo;s desired state.So why is there two mentions of metadata? Ok, The first reference identifies this Deployment object itself:
apiVersion: apps/v1 kind: Deployment metadata: namespace: default name: nginx-deployment labels: app: nginx So for example, if you want to delete this object, you&rsquo;d issue either of these kubectl commands:
kubectl delete deployments nginx-deployment -n default OR kubectl delete deployments -l app=nginx-deployment -n default The latter delete example above uses an equality-based label selector condition. There&rsquo;s also a set-based label selector condition. You can read about these here ‚û° labels and selectors
The second metadata reference The second metadata reference is inside the template that is being used to describe the POD to be created AND is separate from the Deployment itself.
... template: metadata: labels: app: nginx foo: baa spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 To demonstrate this, let&rsquo;s run the following command:
‚ùØ kubectl.exe get pods -l app=nginx -n default --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-6494589cc9-242v8 1/1 Running 0 3s app=nginx,foo=baa,pod-template-hash=6494589cc9 nginx-deployment-6494589cc9-2fdf5 1/1 Running 0 3s app=nginx,foo=baa,pod-template-hash=6494589cc9 nginx-deployment-6494589cc9-n8vxb 0/1 ErrImagePull 0 3s app=nginx,foo=baa,pod-template-hash=6494589cc9 If you look at the LABELS column you can see labels that are not found in the Deployment metadata - eg foo=baa.
You can also use set-based selector; here&rsquo;s an example that produces the same outcome:
‚ùØ kubectl.exe get pods -l &#34;app in (nginx)&#34; -n default --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-6494589cc9-6sn7v 1/1 Running 0 6m10s app=nginx,foo=baa,pod-template-hash=6494589cc9 nginx-deployment-6494589cc9-htxpx 0/1 ImagePullBackOff 0 6m10s app=nginx,foo=baa,pod-template-hash=6494589cc9 nginx-deployment-6494589cc9-qh8nc 0/1 ErrImagePull 0 6m10s app=nginx,foo=baa,pod-template-hash=6494589cc9 Binding deployment to pod So, how do we couple the Deployment with the Pod? Well, this is where the selector comes into play. The selector instructs Kubernetes to match on the app label for those that have a value of nginx and that the foo label that has the value of baa.
spec: selector: matchLabels: app: nginx foo: baa I hope this has made sense and has cleared up any confusion you may have had.
`}),e.add({id:11,href:"/posts/azure-defender-for-cloud/",title:"Azure Defender for Cloud",content:"Defender for Cloud Containers Setting up Defender for Cloud Containers to work with your CICD pipeline is quick and uncomplicated. I do not walk through these set up steps in this post. For that, you can follow those few steps here in this Microsoft post instead ‚û° Setup. The goal of this post is to highlight a few areas of interest and to share my opinions on this feature. I have understandably obfuscated sensitive information.\nTL;DR:\nPros:\nRapid inclusion in your GH Actions CICD pipeline Uncomplicated Adherence to industry preferred practice Cons:\nExpected ALL findings from GHA run to be visible in Defender blade Doesn&rsquo;t support windows containers (.NET Framework workloads) üò± substantiated here ‚û° Availability Set up It takes very little time to configure image scanning and to secure your container images. In my current role as Head of Cloud Platform at Carfinance 247 I am spear heading the migration effort to move our entire workload real estate from on-premise to Azure. As part of this mission, we&rsquo;re using GH Actions for our CICD pipelines. Azure Defender of Cloud Containers compliments GH Actions and I personally have found it a very painless exercise 1. It takes little more time than it does to actually read their instructions to configure, run and see the scan summary and remediation advice.\nWith regards to configuring Azure Defender for Cloud, all it takes a few mouse clicks and you&rsquo;re done. During this process, you will be required to copy 2 values it makes available to you that will need to be added as GitHub secrets.\nThe final step is to insert 2 GH Actions into your GHA Workflow. You may need to seperate your build and push steps as per their instructions. Below is a snippet from one of our deploy GH Actions Workflows that shows how we&rsquo;ve incorporated these GH Actions:\n... - name: BUILD IMAGE run: | cd ${{ env.ROOT_DIR }} docker build -t ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} -f ${{ env.ROOT_DIR }}/${{ env.APP_DOCKERFILE }} . errorCode=$? if [ $errorCode -ne 0 ]; then echo &#34;Could not build to ACR, error occured with docker build&#34; exit 1 fi - name: SCAN FOR VULNERABILITIES uses: Azure/container-scan@v0 id: container-scan continue-on-error: true with: image-name: ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} - name: PUSH TO ACR run: | docker push ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} errorCode=$? if [ $errorCode -ne 0 ]; then echo &#34;Could not build to ACR, error occured with docker build&#34; exit 1 fi - name: POST LOGS TO APPINSIGHTS uses: Azure/publish-security-assessments@v0 with: scan-results-path: ${{ steps.container-scan.outputs.scan-report-path }} connection-string: ${{ secrets.AZ_APPINSIGHTS_CONNECTION_STRING }} subscription-token: ${{ secrets.AZ_SUBSCRIPTION_TOKEN }} ... üëÜ We are not using (AZ_APPINSIGHTS_CONNECTION_STRING, AZ_SUBSCRIPTION_TOKEN) secrets names\nResults There are 2 places where you can view the Commons Vulnerabilities and Exposures. These locations are wihtin GH Actions and the Defender for Cloud Blade in the Azure portal.\nGH Action Job Azure blade Summary In summary:\nI found it quick to get up and running and see the CVEs and remediation advice 1 - Like with most processes, IMO, if there&rsquo;s no predetermined action to react to findings - e.g. automated Runbook - then it&rsquo;s a pointless exercise and your organisation will remain exposed to such vulnerabilities IMO, this has be part of a wider initiative. For example, the inclusion of code quality analysis is a must to avoid vulnerabilities, bugs and poor coding practices/implementations making it into the codebase in the first place. This is how we roll. GitHub makes this easy! I find it surprising how many CVEs, irrespective of criticality, are present in established docker images. You don&rsquo;t have to look far to discover them! References Setup defender for container registries\nDefender for cloud\n"}),e.add({id:12,href:"/posts/npm-issues-e401-cert_not_yet_valid/",title:"Npm E401 and CERT_NOT_YET_VALID",content:`Today a PR Merge resulted in a GHA failure. Sadly, this is not the only CICD pipeline to fail this year! This particular pipeline builds a NodeJS Image, pushes the image to ACR and deploys the service to a production Docker Swarm (on merge to main).
This was the error:
[3/7] RUN npm install:
#7 1.469 npm ERR! code E401
#7 1.470 npm ERR! Unable to authenticate, need: Bearer authorization_uri=https://login.windows.net/736f9f**-09-49-86**-b******31f407, Basic realm=&ldquo;https://pkgsprodsu3weu.app.pkgs.visualstudio.com/", TFS-Federated
#7 1.475
#7 1.475 npm ERR! A complete log of this run can be found in:
#7 1.475 npm ERR! /root/.npm/_logs/2021-12-27T09_38_24_060Z-debug.log
Mmmm, E401? ü§î
I&rsquo;ve not seen this error before but as it was auth related, assumed the PAT token had expired 1. Google&hellip;
I did not have the actual .npmrc file that was in a GH Secret so I used my own local .npmrc file to confirm I can pull from our organisation&rsquo;s npm feed.
cd &lt;project-root&gt; rm node-modules npm cache clean --force npm install It worked. So, I had my first fallback option - regenerate PAT, register centrally for a reminder of when the PAT is to expire and update GH secret. However, I was not yet done. I had not yet reproduced verbatim the pipeline, egro a docker build.
So, I ran this:
docker build -t &lt;image:tag&gt; . Oh no! The error has returned!
This is the Dockerfile:
FROM node:14.4.0 WORKDIR / COPY . . RUN npm install RUN echo &#39;module.exports = &#39; | cat - node_modules/@&lt;redacted&gt;/&lt;redacted&gt;/dist/libs/&lt;redacted&gt;-lib/index.js &gt; temp &amp;&amp; mv temp helpers/index.js RUN cd helpers &amp;&amp; ls -la RUN head -10 helpers/index.js ENV PORT=80 EXPOSE 80 CMD [&#34;npm&#34;, &#34;run&#34;, &#34;&lt;redacted&gt;&#34;] Apart from it not being the LTS, there was nothing obviously wrong with it plus it had been building ok leading up to this.
This a the credentials part of the .npmrc:
registry=https://pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry always-auth=true //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry/:username=&lt;any-value-not-empty&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry/:_password=&lt;Base64-encoded-PAT&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry/:email=&lt;email-is-not-used&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry/:always-auth=true According to this Microsoft documentation post ‚û° npm scopes, the token structure was incomplete. I corrected the structure:
registry=https://pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry always-auth=true //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry/:username=&lt;any-value-not-empty&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry/:_password=&lt;Base64-encoded-PAT&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry/:email=&lt;email-is-not-used&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/registry/:always-auth=true //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/:username=&lt;any-value-not-empty&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/:_password=&lt;Base64-encoded-PAT&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/:email=&lt;email-is-not-used&gt; //pkgs.dev.azure.com/&lt;org-name&gt;/_packaging/&lt;feed-name&gt;/npm/:always-auth=true I re-ran the docker build -t &lt;image:tag&gt; . This time I got a different error but the original E401 error had gone away!
The new error:
#7 43.61 npm ERR! code CERT_NOT_YET_VALID
#7 43.61 npm ERR! errno CERT_NOT_YET_VALID
#7 43.61 npm ERR! request to https://xuavsblobprodsu6weus12.blob.core.windows.net/b-7a3f75bdbbf3432bbe2621e93c98932a/86CDB768B8C395B149741963D831555B55E3458************.blob?sv=2019-07-07&amp;sr=b&amp;si=1&amp;sig=LUCuO42mrmOAx5Nzds9RWS0v2qL%2FwbB86c%3D&amp;spr=https&amp;se=2022-01-12T11%3A42%3A24Z&amp;rscl=x-e2eid-59793e65--a727df6e-***********-session-59793e65-*******-a727df6e-8c628cc8&amp;rscd=attachment%3B%20filename%3D%22string-width-4.2.3.tgz%22 failed, reason: certificate is not yet valid
Mmmm, CERT_NOT_YET_VALID. I&rsquo;d not seen this error before. Google&hellip;
After a short period of online research üëÄ I found a suggestion about being explicit in setting the default npm registry. This meant I had to insert RUN npm config set registry http://registry.npmjs.org before the RUN npm install command. The resulting Dockerfile then looked like this:
FROM node:14.4.0 WORKDIR / COPY . . RUN npm config set registry http://registry.npmjs.org RUN npm install RUN echo &#39;module.exports = &#39; | cat - node_modules/@&lt;redacted&gt;/&lt;redacted&gt;/dist/libs/&lt;redacted&gt;-lib/index.js &gt; temp &amp;&amp; mv temp helpers/index.js RUN cd helpers &amp;&amp; ls -la RUN head -10 helpers/index.js ENV PORT=80 EXPOSE 80 CMD [&#34;npm&#34;, &#34;run&#34;, &#34;prod&#34;] I re-ran the docker build -t &lt;image:tag&gt; . and success! ü•≥
I updated the appropriate GH Secret with the modified .npmrc file and asked the author of the PR that had reported this issue originally to make the 1 line change to the Dockerfile. He made the change, prompted for another PR review and merged to main on approval. The GHA ran successfully! The usual monitoring post deploy and feature/fix was confirmed and I set about updating internal documentation providing instructions on what to do to remediate for others and notifying all via slack that of this.
References Npm scopes 1 - secrets that expire need to be registered centrally on a system that can notify you in advance, giving you ample time to remediate. For our Azure AAD SP (Service Principals) client secrets, we run an Automation Runbook each day that traverses the Azure Resource Graph and alerts me via email of those SP that will be expiring within 30 days.
`}),e.add({id:13,href:"/posts/github-actions-workflow-env-vars/",title:"Github Actions Workflow Env Vars",content:"In my current role as Head of Cloud Platform, I am leading the technical effort of migrating our entire on-premise real-estate to Azure. Part of this mission, is to upgrade the runtimes of our applications, regardless of their current placement; IIS Web apps, Windows Services and Docker Swarm containers. I say &ldquo;part of this mission&rdquo; as another aspect of this migration is to create a new foundation for our platform - AKS. I hope to cover more on this in later posts.\nGithub workflows We are using Self-Hosted Runners to build and deploy our applications to AKS. We have a Hub&amp;Spoke network architecture and our AKS clusters are private. We have other backing services that are deliberately behind Azure Private Endpoints. Our architecture enables us to deploy securely from our company network to our spoke VNETs that exist across our Azure Subscriptions.\nWe&rsquo;re targeting 2 guest operating systems with our containerization orchestration solution - AKS. These are Linux (.NET Core workloads) and Windows (.NET Framework workloads). We are having to upgrade our .NET Framework runtimes to 4.8 as this is the minimum requirement to running containers in Kubernetes in Azure.\nThere are subtle GitHub Actions Workflows expression differences when working with Powershell and bash. Here in this post I concentrate on how you create and set env vars.\nIs the snippet below, which is from our deploy GHA workflow, I use a workflow_dispatch to deploy either a feature branch or our main branch. Feature branches are deployed to our Development Cluster and non-feature branches to our Production Cluster.\n- name: SETUP MAIN BRANCH if: ${{ github.ref == &#39;refs/heads/main&#39; || github.ref == &#39;refs/heads/master&#39; }} run: | echo &#34;ENV=prod&#34; &gt;&gt; $GITHUB_ENV echo &#34;TAG=1.0.${{github.run_number}}&#34; &gt;&gt; $GITHUB_ENV echo &#34;PWD=$(pwd)&#34; &gt;&gt; $GITHUB_ENV - name: SETUP FEATURE BRANCH if: ${{ github.ref != &#39;refs/heads/main&#39; &amp;&amp; github.ref != &#39;refs/heads/master&#39; }} run: | echo &#34;ENV=dev&#34; &gt;&gt; $GITHUB_ENV echo &#34;TAG=${{ github.ref_name }}&#34; &gt;&gt; $GITHUB_ENV echo &#34;PWD=$(pwd)&#34; &gt;&gt; $GITHUB_ENV In the example above, we generate env vars that are used later in this workflow. The above is running on one of our Linux Self-Hosted Runners so using bash script.\nThe above example will not update env vars when run on a Windows Self-Hosted Runner (PowerShell).\nThe equivalent when targeting windows is:\n- name: SETUP MAIN BRANCH if: ${{ github.ref == &#39;refs/heads/main&#39; || github.ref == &#39;refs/heads/master&#39; }} run: | echo &#34;ENV=prod&#34; &gt;&gt; $env:GITHUB_ENV echo &#34;TAG=1.0.${{github.run_number}}&#34; &gt;&gt; $env:GITHUB_ENV echo &#34;PWD=$(Get-Location)&#34; &gt;&gt; $env:GITHUB_ENV - name: SETUP FEATURE BRANCH if: ${{ github.ref != &#39;refs/heads/main&#39; &amp;&amp; github.ref != &#39;refs/heads/master&#39; }} run: | echo &#34;ENV=dev&#34; &gt;&gt; $env:GITHUB_ENV echo &#34;TAG=${{ github.ref_name }}&#34; &gt;&gt; $env:GITHUB_ENV echo &#34;PWD=$(Get-Location)&#34; &gt;&gt; $env:GITHUB_ENV Notationally, the only difference here is &gt;&gt; $GITHUB_ENV and &gt;&gt; $env:GITHUB_ENV. Powershell requires the pre-suffix of env: (as in Get-ChildItem env:). The consumer syntax of this env var remains the same between shells - ${{ env.TAG }} - so it&rsquo;s only the creation of this env var that needs to change between shells.\nFor context, I&rsquo;ve pasted below an example of where the TAG env var is being consumed:\n- name: BUILD AND PUSH run: | try { cd ${{ env.ROOT_DIR }} docker build -t ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} -f ${{ env.ROOT_DIR }}/${{ env.APP_DOCKERFILE }} . docker push ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} } catch { Write-Output &#34;Could not push to ACR, error occured with docker build&#34; Exit 1 } CICD Here&rsquo;s a note on being sympathetic to our development teams&rsquo; nuances&hellip;\nWe are using the approach of regenerating feature images and deploying from one workflow_dispatcher instead of triggering a deployment from a merge to a dedicated development branch. Our (the virtual team I&rsquo;m managing that is the Platform Team - senior staff) combined experience lead us to determine that a dedicated development branch will get out of sync with our main branch. This is especially problematic if your branching strategy predicates promoting to production via a merge to main from a dedicated development branch. To compound this point, we often have multiple developers concurrently working on the same repo so this in itself presents inherent complexities so arriving at a CICD pipelines wasn&rsquo;t clear cut as teams have subtle nuances around how they build &amp; deploy features/fixes.\r"}),e.add({id:14,href:"/posts/runtimes/",title:"Runtimes",content:`In my current role as Head of Cloud Platform, I am leading the technical effort of migrating our entire on-premise real-estate to Azure. Part of this mission, is to upgrade the runtimes of our applications, regardless of their current placement; IIS Web apps, Windows Services and Docker Swarm containers. I say &ldquo;part of this mission&rdquo; as another aspect of this migration is to create a new foundation for our platform - AKS. I hope to cover more on this in later posts.
With being deliberately ambiguous on actual numbers here, we have a fair amount of workloads that are running on runtimes that have surpassed their end-of-life support status üò±. I am sure we&rsquo;re not the only organization that finds itself in this predicament. One of the first things our CTO wanted when he came onboard was for us to sort out our runtimes.
What this means is that we have .NET Framework and .NET Core workloads that need to be migrated to a runtime that is not out of support (now or anytime soon). At the same time, the target runtime has to have hit the mature (at least on it&rsquo;s 1st minor release) status. .NET 6.0 is now GA but it&rsquo;s paint is still a little wet. Understandably there is some concern, borderline trepidation towards targeting this new version. There are impactful benefits (improved performance, greater stability, improved GC, cost benefits resulting from being more perfomant, less vulnerabilities, improved security, etc&hellip;) and our current runtime target for our .NET Core workloads is 3.1. However, end-of-life support for this particular version is the end of this year - 03.Dec.2022. So, at some point, this upgrade needs to happen. With planning and prioritization, this can happen when appropriate.
One strategy of delivering confidence with a runtime is to update a few apps, consecutively; providing you&rsquo;ve availability to accommodate such an approach. Timeboxed: (1) A simple (CRUD-esque HTTP API that sits on top of a Db) workload then (2) one that involves an data-in-motion aspect (message queue, consumer paradigm, non-http api).
Another approach to reducing concern, mitigating risk, etc&hellip; is to meet with the SME (Subject matter expert) - a Microsoft representative - and listen to what they have to say. I have orchestrated such a meeting - we are being helped/guided by Microsoft FastTrak. The advice received was to upgrade. We were also reminded of the migration paths to take when moving to a target version. Here are some demonstrative links that will help you through this process:
Migration guidance found here ‚û° migrate from asp.net core 3.1 to 6.0.
An example of breaking changes between versions can be found here ‚û° breaking changes in .NET Core 3.1
Most of our workloads are simple HTTP API CRUDs that sit aloft databases. In short, we&rsquo;re not doing anything too complicated and so this makes most of our pain points the result of external NuGet package dependencies.
IMO, it is important to establish, if at all possible, a feedback loop back to Microsoft. If you are fortunate to have an established relationship with Microsoft and have access to certain areas/teams - eg FastTrak, or even a Support plan, if/when issues do arise (eg increased exception frequency, uncharacteristic high latency, poor performance, frequent CG, memory leaks, socket exhaustion, etc&hellip;), you can feed these back and receive remediation advice until a fix (if not a result of poor implementation) becomes available. We had been plagued with issues (socket exhaustion, CG, memory leaks) with those workloads on ASP.NET Core 2.* but now see less issues with those ASP.NET Core workloads that are now on 3.1.
To complement the above I would also recommend agreeing on, and capturing, metrics that are demonstrative of your gains from the runtime upgrade. This is where metrics come into their own. Generally, metrics tend to have a greater retention period than logs, and therefore can paint an informed picture over a wider span of time than logging can. Generally speaking, lack of memory is going to be more catastrophic than thottling through lack of CPU. When your RSS or working set bytes are depleted, more often than not your application will terminate due to OOM (out of memory) exception - state and workflow blown away unless you&rsquo;ve employed patterns to mitigate against these edge/corner cases - most don&rsquo;t. In particular, I am referring to container_memory_rss and container_memory_working_set_bytes due to working with kubernetes.
The above is not a definitive list of must do&rsquo;s and in part, is overly simplistic. IMO, it is important to stay current with runtimes to benefit from the improvements they deliver on. For me though, their stability and the vulnerabilities they address is the most important. It is scary how many organizations run on runtimes that are out of support. Applications will still run but ask yourself this, how much is this truly costing your company, how much is it costing our environment and finally, how much could your company suffer because they are exposed to vulnerabilities?
Sounds overly dramatic doesn&rsquo;t?! I stop there as I don&rsquo;t wish to perpetuate the negativity of going down this particular hole. But before I do; more food for thought. I would like to throw into the mix application dependencies and in particular those package dependencies some of us rely on - NuGet &amp; npm. Good DevOps practices mitigate again licensing or vulnerability scanning but IMO the same level of focus ought to be concentrated into this area as well. This was painfully brought to light recently regarding OSS and the log4j vulnerability - CVE-2021-44228 (common vulnerabilities and exposure). And finally, docker image scanning. There are plenty of solutions out there - Snyk for example - that can help with this. Good docker practices go a long way also toward protecting us against vulnerabilities that arise from images and not forgetting code scanning products generally that identify smelly code, bugs and other vulnerabilities. I have introduced solutions that deal with all of this.
`}),e.add({id:15,href:"/posts/permission-denied-while-trying-to-connect-to-the-docker-daemon-socket/",title:"Permission Denied While Trying to Connect to the Docker Daemon Socket",content:`Out of the blue today, my first day back after Christmas break, I got this when running a GH Actions Workflow on one of our Self-Hosted Linux Runners üò±:
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get &ldquo;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json": dial unix /var/run/docker.sock: connect: permission deniedWe have several GitHub Self-Hosted Runners running on Linux and Windows O/S that produce, amongst other artefacts, Linux and Windows images. These images are pushed to ACR. We&rsquo;re in the process of migrating our on-premise real-estate - IIS Web apps, Windows Services, Docker swarm containers - to AKS as well as migrating our SQL Server AG to Azure. We&rsquo;re using Self-Hosted Runners as we have spare compute capacity and some of our applications have a dependency on a legacy NuGet server which requires our CI pipelines to run in our network. We are in the process of also migrating these legacy packages to our Azure DevOps NuGet Feed as part of our modernization initiative. This modernization initiative encompasses upgrading our runtimes to .NET Framework 4.8 and .NET 6.0.It had been running fine prior to my break so what gives? I started to investigate&hellip;
I logged in to the Linux VM where this particular Self-Hosted Runner is hosted with the same credentials as used when I installed the Self-Hosted Runner originally. I used the following command to confirm the same outcome:
docker ps Yup, same thing.
The next configuration I wanted to check was whether this user is a member of the docker group so I used this command:
sudo groups &lt;user&gt; Mmmmm, that&rsquo;s odd. This user wasn&rsquo;t a member and therefore begs the question, how did this ever work in the first place?!!
I added this user using this command:
sudo usermod -a -G docker &lt;user&gt; I ran docker ps again but still no dice. ü§î.
I then checked the status of the docker service using this command:
sudo systemctl status docker It reported:
Active: active (running) since Thu 2021-09-16 14:13:04 UTC; 3 months 20 days agoOk, what next? ü§î
I decided to restart the self-hosted service so I entered these commands:
cd actions-runner sudo ./svc.sh start This is when I saw these failures:
Dec 19 22:01:15 redacted runsvc.sh[291703]: 2021-12-19 22:01:15Z: Runner connect error: The HTTP request timed out after 00:01:00.. Retrying unt‚Ä¶econnected.
Dec 19 22:02:35 redacted runsvc.sh[291703]: 2021-12-19 22:02:35Z: Runner reconnected.
Jan 06 14:42:21 redacted runsvc.sh[291703]: 2022-01-06 14:42:21Z: Running job: deploy
Jan 06 14:42:41 redacted runsvc.sh[291703]: 2022-01-06 14:42:41Z: Job deploy completed with result: Failed
Jan 06 14:46:19 redacted runsvc.sh[291703]: 2022-01-06 14:46:19Z: Running job: deploy
&hellip;
I restarted the Self-Hosted Runner using these commands:
sudo ./svc.sh stop sudo ./svc.sh start Then I logged out and back in again to confirm docker access docker ps and finished off by re-running the failed GH Action Workflow. ü•≥ Equilibrium is once again restored. As per protocol, I shared issue and resolution with our IT Team in case this crops up again when I&rsquo;m not online to help.
`}),e.add({id:16,href:"/posts/my-first-outing-with-dapr/",title:"My First Outing With Dapr",content:`TL;DR: Not as forgiving as I&rsquo;d have liked &hellip;
I was a speaker at a meet-up in Manchester in late 2020. I spoke about Dapr, Keda and the NestJS Framework. My talk topic was on &ldquo;Writing less code - let your architecture and abstractions help with your *-cases&rdquo;. The * in the title is a wildcard for use/edge/corner.
My code examples can be found here (includes both docker compose &amp; Kubernetes manifests) - https://github.com/garrardkitchen/meetup-nov20
Challenge #1 This took a little longer than I&rsquo;d have liked!
I was using the internal DNS to resolve the port of my redis service. My Redis single instance was deployed via a deployment manifest, along with a LoadBalancer Service - purely to give me remote access.
I&rsquo;d first create a secret, by typing:
$ kubectl create secret generic db-passwords --from-literal=redis-password=&#39;&lt;password&gt;&#39; This is the deployment manifest:
apiVersion: v1 kind: Service metadata: name: redis-svc namespace: meetup-dapr-demo labels: run: redis-svc spec: type: LoadBalancer ports: - port: 6379 targetPort: 6379 protocol: TCP selector: run: redis --- apiVersion: apps/v1 # for k8s versions before 1.9.0 use apps/v1beta2 and before 1.8.0 use extensions/v1beta1 kind: Deployment metadata: name: redis namespace: meetup-dapr-demo spec: selector: matchLabels: run: redis replicas: 1 template: metadata: labels: run: redis spec: containers: - name: cache image: redis args: [&#34;redis-server&#34;, &#34;--requirepass&#34;, $(PASSWORD) ] env: - name: PASSWORD valueFrom: secretKeyRef: name: db-passwords key: redis-password resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 This deployed correctly.
I then deployed my Dapr state store component:
apiVersion: dapr.io/v1alpha1 kind: Component metadata: name: mystore namespace: meetup-dapr-demo spec: type: state.redis metadata: - name: redisHost value: redis.meetup-dapr-demo.svc.cluster.local:6379 - name: redisPassword value: &#34;********&#34; However, I could not for the life of me give my application access to the state store!
$ dapr logs -a http-api -k -n meetup-dapr-demo ... time=&#34;2020-11-06T09:47:02.218770653Z&#34; level=error msg=&#34;process component mystore error, redis store: error connecting to redis at redis.meetup-dapr-demo.svc.cluster.local:6379: dial tcp: lookup redis.meetup-dapr-demo.svc.cluster.local on 10.0.0.10:53: no such host&#34; app_id=http-api instance=http-api-6bc44f8957-q2lvn scope=dapr.runtime type=log ver=0.11.3 Having trying every permutation known to non-gender-specific-person-entity I remembered I was kaing it available behind a service. So, I&rsquo;d been using redis.meetup-dapr-demo.svc.cluster.local:6379 when I should have used redis-svc.meetup-dapr-demo.svc.cluster.local:6379.
Once I&rsquo;d corrected my mistake, it connected without error.
apiVersion: dapr.io/v1alpha1 kind: Component metadata: name: mystore namespace: meetup-dapr-demo spec: type: state.redis metadata: - name: redisHost value: redis-svc.meetup-dapr-demo.svc.cluster.local:6379 - name: redisPassword value: &#34;********&#34; Challenge #2 secrets!
You&rsquo;re application is going to report something similar to this - NOAUTH Authentication required - if you&rsquo;re Dapr is deployed to a different namespace to that of your application:
time=&#34;2020-11-06T11:19:06.985273661Z&#34; level=error msg=&#34;process component mystore error, redis store: error connecting to redis at redis-svc.meetup-dapr-demo.svc.cluster.local:6379: NOAUTH Authentication required.&#34; app_id=http-api instance=http-api-7d49cf59d5-9blwf scope=dapr.runtime type=log ver=0.11.3 To circumvent this, you must create a role and binding this to the default ServiceAccount. This role secret-reader allows a get of the secrets resource within the meetup-depr-demo namespace. An example manifest is here:
apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: secret-reader namespace: meetup-dapr-demo rules: - apiGroups: [&#34;&#34;] resources: [&#34;secrets&#34;] verbs: [&#34;get&#34;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: dapr-secret-reader namespace: meetup-dapr-demo subjects: - kind: ServiceAccount name: default roleRef: kind: Role name: secret-reader apiGroup: rbac.authorization.k8s.io Once deployed, you&rsquo;ll see something similar to this in your dapr logs:
time=&#34;2020-11-06T11:23:20.529658232Z&#34; level=info msg=&#34;component loaded. name: mystore, type: state.redis&#34; app_id=http-api instance=http-api-7d49cf59d5-kszdz scope=dapr.runtime type=log ver=0.11.3 This post was created some time ago. Now, we&rsquo;re using the Secrets Store CSI Driver to map Azure KeyVault secrets to containers running in our AKS clusters.
Ref: https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver
`}),e.add({id:17,href:"/posts/how-to-use-kubernetes-configmap/",title:"How to Use Kubernetes Configmap",content:`There&rsquo;s a ton of material out there on how to use a ConfigMap. In this post I will provide a recap on the basics then I drill into how to protect your secrets!
There are a few ways to create a configMap. Here, I cover just two of these ways;--from-env-file and &ndash;from-literal. I won&rsquo;t cover options like from volume.
How to create a ConfigMap from a literal To create a configMap from literals and from the command line, you would type this:
$ kubectl create configmap config-demo-lit --from-literal=user.name=garrardkitchen --from-literal=user.type=admin To confirm the values, you would type this:
kubectl get cm config-demo-lit -o yaml apiVersion: v1 data: user.name: garrardkitchen user.type: admin kind: ConfigMap metadata: creationTimestamp: &#34;2020-11-02T16:06:30Z&#34; name: config-demo-lit namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-lit uid: **** How to create a ConfigMap from an .env file From the command line To create a configMap from the command line, you would type this:
$ kubectl create configmap demo-config --from-env-file=config/.env.prod To confirm the values, you would type this:
$ kubectl cm config-demo-1 -o yaml apiVersion: v1 data: foo: baa name: garrard kind: ConfigMap metadata: creationTimestamp: &#34;2020-11-02T15:44:52Z&#34; name: config-demo-1 namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-1 uid: **** üëÜ cm is shorthand for configmap
From a Kubernetes Manifest file To create a configMap from a manifest, you would create a yml|yaml file using the kind: ConfigMap like this:
apiVersion: v1 kind: ConfigMap metadata: name: config-demo-2 namespace: dapr-demo data: foo: baa name: garrard To confirm the values, you would type this:
$ kubectl cm config-demo-2 -o yaml apiVersion: v1 data: foo: baa name: garrard kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {&#34;apiVersion&#34;:&#34;v1&#34;,&#34;data&#34;:{&#34;foo&#34;:&#34;baa&#34;,&#34;name&#34;:&#34;garrard&#34;},&#34;kind&#34;:&#34;ConfigMap&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;config-demo-2&#34;,&#34;namespace&#34;:&#34;dapr-demo&#34;}} creationTimestamp: &#34;2020-11-02T15:49:48Z&#34; name: config-demo-2 namespace: dapr-demo resourceVersion: ***** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-2 uid: **** How to use this in a pod Here, I&rsquo;m setting up environment variables from different ConfigMaps. config-demo-2 is set up from manifest file and config-demo-lit is set up from literals.
This is an example pod manifest called pod-demo.yml
apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-cache image: k8s.gcr.io/busybox command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;env&#34;] env: - name: NAME valueFrom: configMapKeyRef: name: config-demo-2 key: name - name: ROLE valueFrom: configMapKeyRef: name: config-demo-lit key: user.type restartPolicy: Never All that this üëÜ does, is output to STDOUT, a list of environment variables.
To apply this manifest, type:
kubectl.exe apply -f .\\pod-demo.yml To confirm the 2 environment variables have been set, type:
$ kubectl logs test-pod ... HOSTNAME=test-pod NAME=garrard ROLE=admin ... How to use Secrets TBC
How to stop people from finding out your secrets. At the end of the day, the secrets are only Base64 encoded. Anyone with the appropriate level of permissions will be able to see your secrets. One way to stop users from seeing your secrets is by only allow particular groups of people access.
To create a secret, type:
$ kubectl create secret generic db-passwords --from-literal=mongodb-password=&#39;mypassword&#39; To see what secrets we have, type:
$ kubectl get secrets NAME TYPE DATA AGE dapr-operator-token-mgdqs kubernetes.io/service-account-token 3 2d13h dapr-sidecar-injector-cert Opaque 2 2d13h dapr-trust-bundle Opaque 3 2d13h dashboard-reader-token-j9rcg kubernetes.io/service-account-token 3 2d13h db-passwords Opaque 1 5s default-token-xl2rz kubernetes.io/service-account-token 3 2d14h sh.helm.release.v1.dapr.v1 helm.sh/release.v1 1 2d13h To see the actual password, type:
$ kubectl get secrets db-passwords -o yaml apiVersion: v1 data: mongodb-password: bXlwYXNzd29yZA== kind: Secret metadata: creationTimestamp: &#34;2020-11-03T10:00:14Z&#34; name: db-passwords namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/secrets/db-passwords uid: **** This is just a Base64 encoded string of mypassword. This is not secure enough. We need another way of to protect our sensitive information/passwords.So, what do we do?
Here&rsquo;s a link to how Kubernetes deals with secrets
To use this secret with a deployment, save this to aks-deploy-mongodb-demo.yml:
Please note, this is not a production configuration
apiVersion: v1 kind: Service metadata: name: mongodb-svc namespace: dapr-demo labels: run: mongodb-svc spec: type: LoadBalancer ports: - port: 27017 targetPort: 27017 protocol: TCP selector: run: mongodb --- apiVersion: apps/v1 # for k8s versions before 1.9.0 use apps/v1beta2 and before 1.8.0 use extensions/v1beta1 kind: Deployment metadata: name: mongodb namespace: dapr-demo spec: selector: matchLabels: run: mongodb replicas: 1 template: metadata: labels: run: mongodb spec: containers: - name: mongodb image: mongo resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 27017 env: - name: MONGO_USERNAME valueFrom: configMapKeyRef: name: config-demo-lit key: user.name - name: MONGO_INITDB_ROOT_USERNAME valueFrom: configMapKeyRef: name: config-demo-lit key: user.name - name: MONGO_INITDB_ROOT_PASSWORD valueFrom: secretKeyRef: name: db-passwords key: mongodb-password - name: MONGO_DBNAME value: &#34;orders&#34; To deploy the above üëÜ, type this:
$ kubectl apply -f .\\aks-deploy-mongodb-demo.yml `}),e.add({id:18,href:"/posts/nodejs-container-restart-policy/",title:"Nodejs Container Restart Policy",content:"If by accident to deploy a solution using the Node.js Cluster API and do not fork exited processes then the following docker-compose restart_policy will not help you:\ndeploy: restart_policy: condition: on-failure If you&rsquo;re using the Cluster API to schedule tasks across your processes, and all forked processes die, then the docker engine will just assume you&rsquo;ve gracefully shutdown.\nTake this code for example, you will see that it doesn&rsquo;t fork another process and therefore, at some point it will no longer process any anything:\nimport { Injectable } from &#39;@nestjs/common&#39;; import * as cluster from &#39;cluster&#39;; import * as os from &#39;os&#39; @Injectable() export class ClusterService { static clusterize(numCPUs: number, callback: () =&gt; void): void { if (cluster.isMaster ) { const procs = (numCPUs &gt; os.cpus().length) ? os.cpus().length : numCPUs console.log(`GOING TO USE ${procs} PROCESSES`) console.log(`MASTER SERVER (${process.pid}) IS RUNNING `); console.log(`MASTER SERVER (${process.pid}) IS RUNNING `); console.log(`SCHED_NONE: ${cluster.SCHED_NONE}`) console.log(`SCHED_RR: ${cluster.SCHED_RR}`) console.log(`CLUSTER SCHEDULING POLICY: ${cluster.schedulingPolicy}`) for (let i = 0; i &lt; procs; i++) { const worker = cluster.fork(); console.log(`CREATING PROCESS ${worker.process.pid}`); } cluster.on(&#39;exit&#39;, (worker, code, signal) =&gt; { console.log(`worker ${worker.process.pid} died ${signal || code}`); }); cluster.on(&#39;disconnect&#39;, (worker) =&gt; { console.log(`worker ${worker.process.pid} disconnected`); }) } else { callback() } } } To mitigate this, you simply fork another process within the exit event handler like this:\ncluster.on(&#39;exit&#39;, (worker, code, signal) =&gt; { console.log(`worker ${worker.process.pid} died ${signal || code}, restarting...`); const newWorker = cluster.fork(); console.log(`CREATING PROCESS ${newWorker.process.pid}`); }); To avoid the container not restarting due to lack of process availability to deal with demand in the above scenario, you can&rsquo;t use the on-failure condition in the restart_policy. You must use the &lsquo;any&rsquo; condition. This section incidentally replaces the old restart sub-option.\ndeploy: replicas: 1 resources: limits: cpus: &#34;2&#34; memory: 512M update_config: order: start-first parallelism: 1 restart_policy: condition: any delay: 5s window: 120s placement: constraints: - node.role == worker Caution: You can&rsquo;t use max_attempts: 3 in combination with condition: any\nAdditionally, I found one further interesting facts when looking into this issue.\nIf you&rsquo;re using Docker Stack Deploy (think stack in Portainer) using a docker-compose file to deploy to your swarm and you&rsquo;re using restart: always, then beware, the restart is not supported.\nref: üëÜ compose-file\n"}),e.add({id:19,href:"/posts/apache-ignite-running-agent-with-dotnetcore-server-node/",title:"How to run the Apache Ignite Agent with an Ignite.NET Core Server Node",content:`I&rsquo;ve recently been researching into Apache Ignite. Apache Ignite is an in-memory, memory-centric, distributed database, caching and processing platform for transactional, analytical, and streaming workloads.
So why the post? Well, with using .NET Core, I have run into one or two challenges that I have had to work through. One of which involves the Agent. I feel it is important to share with you how I get beyond this issue. It may save you a lot of time if you&rsquo;re an Apache Ignite noob like me.
You use the Agent when you want to execute queries, SQL DML &amp; DDL amongst other actions, from within the Web Console app. The Agent acts as a proxy. The Agent must connect to both the Web Console and your Server node or Thick Client node.
As I say above, this Agent acts as a proxy between the Web Console (UI to configure clusters, execute SQL DML &amp; DDL, and query KV stores including visuals on caches etc‚Ä¶) and a Server node (aka, data node) to execute SQL &amp; KV stores.
With all previous efforts, I was not able connect the Agent to the data node when the data node was created using a .NET runtime. When running the same configuration but using Java instead, it would connect without issue. Based on being able to connect to data node when created via java, I was confident that I would find a way to get this to work.
After many hours of twawling the internet for answers and failed attempts, I figured out what the issue was. The Apache.Ignite nuget package does not include the ignite-rest-http module (contains many Jars), and this is what the Agent needs to communicate with the data node. So, what you need to do is download the entire Ignite binary package, and extract the ignite-rest-http folder. Then you use the following code to add a list of comma-separated file names of the HTTP Jar files to the IgniteConfiguration.JvmClasspath property:
var cfg = new IgniteConfiguration { JvmClasspath = Directory.GetFiles(pathToIgniteRestHttpJars) .Aggregate((x, y) =&gt; x + &#34;;&#34; + y) }; using (var ignite = Ignition.Start(cfg)) { ... From what I could find, there&rsquo;s no plans on including the ignite-rest-http module in the Apache.Ignite nuget package.
I will share a GitHub repo to ease you into this shortly.
`}),e.add({id:20,href:"/posts/terraform-get-values/",title:"Terraform Get Values",content:`With not wanting to have hard coded values pushed to a project&rsquo;s code repository, and an antiquated way to derive Azure Service Principal credentials, I set about exploring ways on accomplishing this with this in mind using Terraform.
Attempt 1 Here ins my first attempt, I load all permutations into map variables. I use an environment variable as an indexer to the appropriate map value:
provider &#34;azurerm&#34; { version = &#34;=2.17&#34; ... subscription_id = var.azure_subscription_id[var.environment] client_id = var.azure_client_id[var.environment] ... } variable &#34;azure_subscription_id&#34; { type = map default = { &#34;dev&#34; = &#34;********-****-****-****-************&#34; &#34;prod&#34;= &#34;********-****-****-****-************&#34; } } variable &#34;azure_client_id&#34; { type = map default = { &#34;dev&#34; = &#34;********-****-****-****-************&#34; &#34;prod&#34;= &#34;********-****-****-****-************&#34; } } ... Attempt 2 This second and more efficient approach, I used jsondecode function to load the entire credentials JSON to access the subscriptionId property:
provider &#34;azurerm&#34; { version = &#34;=2.17&#34; ... subscription_id = var.environment == &#34;dev&#34; ? jsondecode(var.azure_sp_dev).subscriptionId : jsondecode(var.azure_sp_prod).subscriptionId client_id = var.environment == &#34;dev&#34; ? jsondecode(var.azure_sp_dev).clientId : jsondecode(var.azure_sp_prod).clientId ... } variable &#34;azure_sp_dev&#34; { type = string default = &lt;&lt;EOT { &#34;clientId&#34;: &#34;********-****-****-****-************&#34;, &#34;clientSecret&#34;: &#34;********-****-****-****-************&#34;, &#34;subscriptionId&#34;: &#34;********-****-****-****-************&#34;, &#34;tenantId&#34;: &#34;********-****-****-****-************&#34;, &#34;activeDirectoryEndpointUrl&#34;: &#34;https://login.microsoftonline.com&#34;, &#34;resourceManagerEndpointUrl&#34;: &#34;https://management.azure.com/&#34;, &#34;activeDirectoryGraphResourceId&#34;: &#34;https://graph.windows.net/&#34;, &#34;sqlManagementEndpointUrl&#34;: &#34;https://management.core.windows.net:8443/&#34;, &#34;galleryEndpointUrl&#34;: &#34;https://gallery.azure.com/&#34;, &#34;managementEndpointUrl&#34;: &#34;https://management.core.windows.net/&#34; } EOT } variable &#34;azure_sp_prod&#34; { type = string ... Obviously, none of the above deals with not pushing these credentials into the code repository.
So, in my opinion, there are 2 options available here. The first option is to use a GitHub Secret and to inject this secret into a script file. It could even be passed as a parameter to Terraform (e.g. terrafor apply -var credentials={...} ). Or, the second option is to obtain this key using the GitHub Azure/get-keyvault-secrets@v1.0 Action. This method will then allow you to obtain the Service Principal credentials from an Azure KeyVault. This latter approach means that we never need to expose these secrets outside of Azure, which we would have to do if we cut &amp; paste them into a GitHub Secret.
`}),e.add({id:21,href:"/posts/digital-certificates/",title:"Digital Certificates",content:`I&rsquo;ve been wanting to put some notes down on digital certificates, signing and JWT for some time now. I find there are plenty of confusing terms involved in this area, plus a few nuances that have added to my personal confusion. I feel it now important to document these before I forget and move on [to another project].
So, what&rsquo;s triggered this post? Well, one of many tasks I&rsquo;m involved in [juggling] evolves SSO (single sign on). Albeit, mainly focused on the architecture on this task, I have compiled a few PoCs where I&rsquo;m using digital certificates for authentication. In particular, SSOing into Twilio Flex and using an identity field returned from their I.AM service, to seamlessly log into our internal CRM, securely using a digital certificate.
Terms Ok, let&rsquo;s start with a few terms. I&rsquo;ll slowly integrate these terms in the following examples.
Keys
A key is something that is used to encrypt a piece of data (think JWT payload). It can be phrase (series of characters) or a public/private key held in a digital certificate.
Hash
A hash is a piece of data, that cannot be reengineered to reveal it&rsquo;s original content, also referred to as a digest or one-way hash.
SHA (Secure Hashing Algorithm)
It is for cryptographic security. It is used to produce an irreversible and unique hash.
Encryption
The process of converting something to gobbledygook and only be able to read it when you have the key used when it was encrypted.
Digital signature
The encrypted hash, that proves the data has not been tampered with in-flighted AND verifies the identity of the entity presenting it.
Signing
The process of creating the digital signature.
Base64
The more efficient was of encoding and sending data over a network.
Cipher algorithm
A cipher algorithm is a mathematical formula designed specifically to obscure the value and content of data. Most valuable cipher algorithms use a key as part of the formula. This key is used to encrypt the data, and either that key or a complementary key is needed to decrypt the data back to a useful form.
RSA (Rivest‚ÄìShamir‚ÄìAdleman)
RSA is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and distinct from the decryption key which is kept secret (private).
Symmetric Encryption
Symmetric encryption is a type of encryption where only one key (a secret key) is used to both encrypt and decrypt electronic information.
Asymmetric Encryption
Asymmetric Encryption is a form of Encryption where keys come in pairs. What one key encrypts, only the other can decrypt.
X.509
Is a standard format for public key certificates. Each X.509 certificate includes a public key, identifying information, and a digital signature.
Of course, if this [digital signature] is new to you, the above won&rsquo;t (yet) make much sense.
I&rsquo;m going to walk you through an example, well 2 actually. One that used a phrase as a key(aka keyphrase), and the other that used a public/private key found in a digital certificate (albeit, self-signed). I am going to use a tool call openssl, not may have heard of it?
Symmetric encyrption Encryption using a keyphrase
In this first example, I&rsquo;m going to encrypt a message with a keyphrase.
Before I begin, I&rsquo;m going to write the content of my secret message to a file called msg.txt.
Next, I&rsquo;m going to encrypt this file it using a keyphrase of abc123 and output the encrypted file to msg.txt.enc:
$ openssl enc -e -aes256 -k abc123 -in ./msg.txt -out ./msg.txt.enc above you&rsquo;ll see -aes256. This is the cipher algorithm we&rsquo;re using
The encrypted file looks something like this:
Salted__BmFÔøΩj‘ø≈ÄaÔøΩÔøΩ1ÔøΩÔøΩ\\X ÔøΩÔøΩÔøΩ{&#39;VÔøΩdFqÔøΩ&amp;ÔøΩÔøΩÔøΩLÔøΩ8:ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩT Pure gobbledygook!
Now, I&rsquo;m going to decrypt this encrypted file msg.txt.enc and output the encrypted file to msg.txt.dec. It is imperative that I used the same keyphrase:
$ openssl enc -d -aes256 -k abc123 -in ./msg.txt.enc -out ./msg.txt.dec The decrypted file msg.txt.dec looks like:
my secret message If I had omitted the keyphrase, I will have been prompt for it which will have looked like this:
$ openssl enc -d -e -aes256 -in ./msg.txt.enc -out ./msg.txt.dec enter aes-256-cbc decryption password: Or, if I had used the incorrect \`keyphrase, I will have seen something like this:
bad decrypt 140120216352064:error:06065064:digital envelope routines:EVP_DecryptFinal_ex:bad decrypt:../crypto/evp/evp_enc.c:583: A good simple illustration of how to encrypt and decrypt a message using openssl enc command.
Asymmetric encryption Encryption using a public/private key
In this section I&rsquo;m going to:
Generate a RSA private key Extract the public key from the private key Generate a hash of the data I want to send, as well as signing it (using private key) Encrypt the data I want to send Decrypt the data I have received Verify the signature of the data received - ensure it data wasn&rsquo;t tampered with in-flight Let&rsquo;s first generate the message I want to securely transmit:
$ echo &#39;my secret message&#39; &gt; msg Create a RSA private key Here I&rsquo;m using the genrsa command. This command generates an RA private key:
$ openssl genrsa -out private.pem 4096 $ openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 Extract public key $ openssl rsa -in private.pem -pubout -out public.pem Generate a digital signature using dgst Here, I&rsquo;m generating a hash (digest) of the message as well as signing it with the private key
$ openssl dgst -sha256 -sign private.pem -out msg.signature msg using rsautl rsautl, unlike dgst, does not create a hash or ASN1 encoding.
As rsautl uses the RSA algorithm directly, it can only be used to sign, or verify, small pieces of data:$ openssl rsautl -sign -in msg -inkey private.pem -out msg.sig Encrypt the message The rsautl command can be used to sign, verify, encrypt and decrypt data using the RSA algorithm.
$ openssl rsautl -encrypt -inkey public.pem -pubin -in msg -out msg.enc By including the -pubin switch, you&rsquo;re telling the command that the input key file (-inkey) is an RSA public key. Withou this, it assumed you&rsquo;re using a private key
Decrypt the message $ openssl rsautl -decrypt -inkey private.pem -in msg.enc -out msg.dec Verify signature using dgst This uses the public key to decrypt the Hash of the original msg:
pseudo logic:
hash_1 = Hash ( msg ) hash_2 = Dec ( Key -&gt; Hash ) IsVarified = hash_1 == hash_2 $ openssl dgst -sha256 -verify public.pem -signature msg.signature msg Verified OK using rsautl This verifies the original message using the signature and outputs it:
$ openssl rsautl -verify -inkey private.pem -in msg.sig my secret message Digital Certificates To verify the identity of the entity presenting it
So far, we&rsquo;ve covered hashes, key pairs, digital signatures and encryption and decryption. This section is where I cover, briefly, digital certificates. I&rsquo;m using a digital certificate to replace the key pair as covered above and to give the capability of using additional information to verify that the identity of the entity presenting this message.
Let&rsquo;s start by creating a self-signed certificate. Type:
# create self-signed certificate openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:4096 -keyout myserver.pem -out myserver.crt -subj &#34;/C=UK/OU=IT/CN=myserver.com&#34; We can inspect the content of the certificate by typing:
openssl x509 -in myserver.crt -text -noout For bravity, I&rsquo;m using the same commands as used above to; extract public key, generate digital signature, encrypt/decrypt then verify the signature. I&rsquo;ve included all the statements in one block:
# extract public key from self-signed certificate openssl rsa -in myserver.pem -pubout -out server-public.pem # generate hash and sign (digital signature) $ openssl dgst -sha256 -sign myserver.pem -out msg.server-signature msg # encrypt my message openssl rsautl -encrypt -inkey server-public.pem -pubin -in msg -out msg.enc2 # decrypt my encrypted message openssl rsautl -decrypt -inkey myserver.pem -in msg.enc2 -out msg.dec2 # vertify signature openssl dgst -sha256 -verify server-public.pem -signature msg.server-signature msg This results in:
Verified OK JWT So, how does the digital signature relate to the Signature verification against a JWT Token?
In this section I will be using the jwt.io website. From this site I can choose which cipher algorithm. I will be using a RSA (widely used for secure data transmission and public-key cryptography) cipher as I&rsquo;m simulating the sending and receiving of a JWT Token over HTTP.
A JWT signature will use RSA SHA (irreversible hash) of the header and payload. This algorithm is set in the header so we have all the information we need to decrypt the encrypted data. However, we&rsquo;re not able to verify these points yet: (a) has the message been tampered with inflight and (b) the identity of the entity presenting this message.
In actual fact, you will see this if you copied the a JWT token without keys into jwt.io (selecting RSA256 algorithm). It will show Invalid Signature. So, to verify these points, you need to provide the public and private key. It will use the private key to obtain the original Hash (hash of the original data) then decrypt this. If once decrypted, this equates to the RSASHA246 HMAC, then the signature is verified.
All I&rsquo;ve done is added a tenant property to the claims (payload). I&rsquo;ve doing this prove that I&rsquo;ve changed the claim and will become apparent shortly why I&rsquo;ve done this:
{ &#34;sub&#34;: &#34;1234567890&#34;, &#34;name&#34;: &#34;John Doe&#34;, &#34;admin&#34;: true, &#34;iat&#34;: 1516239022, &#34;tenant&#34;: &#34;foobaa&#34; } Next, I copy in the public and private key into the verify signature area. This all looks like this:
I copy the encoded token (will paste it back in, in a moment) then refresh the page. The page is recent and defaults loaded (observe the changed payload):
I now copy in my encoded token:
You will see the Invalid Signature near the bottom, but, the correct payload is back! This is because the certificates key pair in this default screen are different to my digital certificate&rsquo;s key pair.
So, if I removed the entire encoded signature from the encoded token, we&rsquo;ll still see the decrypted payload:
So, how&rsquo;s it validating the signature? &hellip;
JWT Token format Let&rsquo;s remind ourselves of the structure of a JWT Token is:
{header}.{payload}.{signature}
The signature, with using the RS246 cipher, is a RSA SHA of the header (just cipher algorithm &amp; type, which is JWT) AND payload. This simply means it is using a public-key (from our digital certificate) to encrypt a one-way hash of our original message (header + payload).
How is the signature verified? Token From encoded token
Algorithm (HASH_1):
ENCRYPT ( KEY -&gt; ( HASH ( base64 (header) + &#34;.&#34; + base64 (payload) ) ) ) The Signature is RSA SHA of ( base64(header) + &ldquo;.&rdquo; + base64(payload)).
Here, in the jwt.io site, it is recalculated after each valid payload change.
Key pair Comparison using key pair
Algorithm (HASH_2):
DECRYPT ( KEY -&gt; ( HASH ( base64 (header) + &#34;.&#34; + base64 (payload) ) ) ) It base64 encodes the header + payload using the key pair, then encrypts it. If this matches the signature in the the encoded token then the signature is verified:
VERIFIED = HASH_1 == HASH_2
As soon as I paste in my public and private keys, it correctly verifies the digital signature:
References openssl genrsa openssl rsautl openssl examples the difference rsautl -sign AND dgst -sign openssl `}),e.add({id:22,href:"/posts/jest-fs-readFileSync/",title:"Unit testing and mocking fs.ReadFileSync",content:"I&rsquo;d just ran npm run test in a newly created package I&rsquo;d added to a monorepo (lerna) I&rsquo;d created for a project I was working on that integrates with Twilio Sync, RabbitMQ, Twilio TaskRouter and MSSQL, and I go this:\n*******************************consumers\\packages\\eda [CRMBROK-233 +0 ~2 -0 !]&gt; npm run test &gt; @cf247/eda@1.0.2 test *******************************consumers\\packages\\eda &gt; jest FAIL __tests__/eda.test.js ‚óè Test suite failed to run ENOENT: no such file or directory, open &#39;.env&#39; 2 | const fs = require(&#39;fs&#39;) 3 | const dotenv = require(&#39;dotenv&#39;) &gt; 4 | const envConfig = dotenv.parse(fs.readFileSync(`.env`)) | ^ 5 | for (const k in envConfig) { 6 | process.env[k] = envConfig[k] 7 | } at Object.&lt;anonymous&gt; (lib/setenv.js:4:35) at Object.&lt;anonymous&gt; (lib/eda.js:1:1) Test Suites: 1 failed, 1 total Tests: 0 total Snapshots: 0 total Time: 1.772 s Ran all test suites. npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! @cf247/eda@1.0.2 test: `jest` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the @cf247/eda@1.0.2 test script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm WARN Local package.json exists, but node_modules missing, did you mean to install? npm ERR! A complete log of this run can be found in: npm ERR! *******************************\\npm-cache\\_logs\\2020-05-28T08_04_32_271Z-debug.log *******************************consumers\\packages\\eda [CRMBROK-233 +0 ~3 -0 !]&gt; Not great but hey, first run and all!\nThe error message tells me everything I need to know:\nENOENT: no such file or directory, open &#39;.env&#39; 2 | const fs = require(&#39;fs&#39;) 3 | const dotenv = require(&#39;dotenv&#39;) &gt; 4 | const envConfig = dotenv.parse(fs.readFileSync(`.env`)) Which is that it can&rsquo;t find an .env file. And it wouldn&rsquo;t. Later refactoring would remove this file dependency but for now, all I want to do is to get my test working.\nThis was the unit test code:\n&#39;use strict&#39; const eda = require(&#39;..&#39;) describe(&#39;@cf247/eda&#39;, () =&gt; { it(&#39;no tests&#39;, () =&gt; { }) }) This is the code from the module it was importing via the require('..') statement:\nrequire(&#39;./setenv&#39;) const amqp = require(&#39;amqplib/callback_api&#39;); module.exports = (io, emitter) =&gt; { ... The top line is importing code from this file:\nI&rsquo;ve highlighted the problematic line of code\n1 2 3 4 5 6 const fs = require(&#39;fs&#39;) const dotenv = require(&#39;dotenv&#39;) const envConfig = dotenv.parse(fs.readFileSync(`.env`)) for (const k in envConfig) { process.env[k] = envConfig[k] } The quickest (IMO) way to deal with this and move forward is to Mock the fs class. I did this by included a jest module mock into my unit test file:\nI&rsquo;ve highlighted the mock related code\n1 2 3 4 5 6 7 8 9 10 11 12 13 &#39;use strict&#39; const fs = require(&#39;fs&#39;) const eda = require(&#39;..&#39;) jest.mock(&#39;fs&#39;, () =&gt; ({ readFileSync: jest.fn((file_name) =&gt; { return [] }) })) describe(&#39;@cf247/eda&#39;, () =&gt; { it(&#39;no tests&#39;, () =&gt; { }) }); What this does is, when the readFileSync class function is called, it always returns an empty array []. As the unit code does not have a dependency on environment variables, this mocked response will work fine.\n"}),e.add({id:23,href:"/posts/kubernetes-on-windows/",title:"Kubernetes on Windows",content:`This post is a reminder to me of what needs to be installed in order for a pod, created from a local image, that is to be served up via a kubernetes cluster, to be run from your local development environment.
What is Kubernetes and why is it so important? &ldquo;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.&rdquo;
So why is Kubernetes important? &ldquo;Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn‚Äôt it be easier if this behavior was handled by a system?
That‚Äôs how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system.&rdquo;
Kubernetes is the community&rsquo;s (has a much larger community than that of Swarm's community) choice of container orchestrators.
Some important notices Permissions
To install kubectl and minikube you must start Powershell with Administrator permissions
Shell
These settings will only viable for the current shell, if you need to run another shell, ensure the minikube docker-env commands in the Steps to take to configure your environment section are also executed in the new shell. As minikube is the tool that runs a local cluster in your development environment, we need to tell it to use it&rsquo;s built-in docker daemon and have images pulled from there, and not from a container registry.
How do I install kubectl (and what the heck is it)? kubectl is a CLI (command line interface) tool for controlling Kubernetes clusters. You can use this tool to deploy applications, inspect and manage cluster resources and view logs.
To ease the installation process, use chocolatey to install kubernetes-cli, run:
PS C:\\&gt; choco install kubernetes-cli How do I install minikube (and what the heck is it)? Minikube implements a local Kubernetes cluster and is deemed the best tool for local Kubernetes application development.
To ease the installation process, use chocolatey to install minikube, run:
PS C:\\&gt; choco install minikube Before you start, you must ensure that you have a platform virtualisation system available. Platform virtualisation software provides the mechanism to run virtual machines and containers in isolation and exposes them to one or more networks. It is within a virtual machine that your Kubernetes cluster will run. Windows 10 comes with a virtualisation hypervisor feature called hyper-v. You need to ensure it is running first. To do this, run:
PS C:\\&gt; Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All PS C:\\&gt; minikube start --driver hyperv * minikube v1.9.1 on Microsoft Windows 10 Enterprise 10.0.18363 Build 18363 * Using the hyperv driver based on user configuration * Downloading VM boot image ... &gt; minikube-v1.9.0.iso.sha256: 65 B / 65 B [--------------] 100.00% ? p/s 0s &gt; minikube-v1.9.0.iso: 174.93 MiB / 174.93 MiB [ 100.00% 1.03 MiB p/s 2m51s * Starting control plane node m01 in cluster minikube * Creating hyperv VM (CPUs=2, Memory=6000MB, Disk=20000MB) ... * Preparing Kubernetes v1.18.0 on Docker 19.03.8 ... * Enabling addons: default-storageclass, storage-provisioner * Done! kubectl is now configured to use &#34;minikube&#34; Steps to take to configure your environment To set up your minikube environment, run:
PS C:\\&gt; minikube docker-env $Env:DOCKER_TLS_VERIFY = &#34;1&#34; $Env:DOCKER_HOST = &#34;tcp://192.168.75.126:2376&#34; $Env:DOCKER_CERT_PATH = &#34;C:\\Users\\garrard.kitchen\\.minikube\\certs&#34; $Env:MINIKUBE_ACTIVE_DOCKERD = &#34;minikube&#34; # To point your shell to minikube&#39;s docker-daemon, run: # &amp; minikube -p minikube docker-env | Invoke-Expression To point your shell to minikube&rsquo;s docker-daemon, run:
PS C:\\&gt; minikube docker-env | Invoke-Expression To get access to minikube&rsquo;s dashboard, run:
PS C:\\&gt; minikube.exe dashboard * Verifying dashboard health ... * Launching proxy ... * Verifying proxy health ... * Opening http://127.0.0.1:54553/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser... Here&rsquo;s some sample nodejs (server.js) code. It starts a server on port 8080:
var http = require(&#39;http&#39;); var handleRequest = function (request, response) { console.log(&#39;Received request for URL: &#39; + request.url); response.writeHead(200); response.end(&#39;Hello World!&#39;); }; console.log(&#34;started&#34;) var www = http.createServer(handleRequest); www.listen(8080); Here&rsquo;s a Dockerfile for the above nodejs server. Please observe that it exposes port 8080. This ensures that network TCP traffic can be received by the container via port 8080.
FROM node:13.5.0 EXPOSE 8080 COPY server.js . CMD [ &#34;node&#34;, &#34;server.js&#34; ] To build a image of the above Dockerfile, run:
PS C:\\&gt; docker build -t hello-world:1 . Include a build tag
You must specify a version tag and it has to be something other than latest. Here, I have used 1. If you don&rsquo;t follow these instructions, minikube will attempt to pull the image from a docker registry (normally DockerHub).
To check that the image exists in Minikube&rsquo;s built-in Docker daemon, run:
PS C:\\&gt; minikube ssh $ docker images You should see something similar to this:
$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/&#39;) _ _ | |_ __ /&#39; _ \` _ \`\\| |/&#39; _ \`\\| || , &lt; ( ) ( )| &#39;_\`\\ /&#39;__\`\\ | ( ) ( ) || || ( ) || || |\\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)\`\\___/&#39;(_,__/&#39;\`\\____) $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world 1 55f40b7f5c32 13 days ago 660MB hello-world latest 50c4285f25a5 13 days ago 660MB nginx latest ed21b7a8aee9 2 weeks ago 127MB k8s.gcr.io/kube-proxy v1.18.0 43940c34f24f 3 weeks ago 117MB k8s.gcr.io/kube-scheduler v1.18.0 a31f78c7c8ce 3 weeks ago 95.3MB k8s.gcr.io/kube-apiserver v1.18.0 74060cea7f70 3 weeks ago 173MB k8s.gcr.io/kube-controller-manager v1.18.0 d3e55153f52f 3 weeks ago 162MB kubernetesui/dashboard v2.0.0-rc6 cdc71b5a8a0e 5 weeks ago 221MB k8s.gcr.io/pause 3.2 80d28bedfe5d 2 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 2 months ago 43.8MB kindest/kindnetd 0.5.3 aa67fec7d7ef 5 months ago 78.5MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 5 months ago 288MB kubernetesui/metrics-scraper v1.0.2 3b08661dc379 5 months ago 40.1MB gcr.io/k8s-minikube/storage-provisioner v1.8.1 4689081edb10 2 years ago 80.8MB To run this image as a pod, run:
PS C:\\&gt; kubectl run hello-world --image=hello-world:1 --port=8080 --image-pull-policy=never pod/hello-world created The --image-pull-policy=never is telling Kubectl to use the local image and not one from a container registry (Docker, ACR, ECR, GCP)
To expose this port for external access (from browser) from outside of the cluster, run:
PS C:\\&gt; kubectl expose pod hello-world --type=LoadBalancer service &#34;hello-world&#34; exposed To confirm your service is running and to get the port number of this exposed service, run:
PS C:\\&gt; kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-world LoadBalancer 10.111.126.10 &lt;pending&gt; 8080:31589/TCP 45h kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 2d16h You will see the &lt;pending&gt; state of your LoadBalancer if you do not have not a Load Balancer integrated with your cluster. For your local development environment, it is nothing to worry about.You will see that the hello-world service is accessible via port 8080. However, we still don&rsquo;t know behind what IPv4 address, this services is available. To get the IPv4 address of your cluster, you type:
PS C:\\&gt; minikube ip 192.168.75.126 Finally, to access your service, run the cURL command, using the minikube ip address and the TCP port as listed in the kubectl get services output:
PS C:\\&gt; curl &#34;http://192.168.75.126:31589&#34; -UseBasicParsing StatusCode : 200 StatusDescription : OK Content : {72, 101, 108, 108...} RawContent : HTTP/1.1 200 OK Connection: keep-alive Transfer-Encoding: chunked Date: Mon, 06 Apr 2020 13:05:42 GMT Hello World! Headers : {[Connection, keep-alive], [Transfer-Encoding, chunked], [Date, Mon, 06 Apr 2020 13:05:42 GMT]} RawContentLength : 12 You can also use minikube to obtain your service&rsquo;s url. To do this, run:
PS C:\\&gt; minikube service hello-world --url http://192.168.75.126:31589 Useful kubectl commands This first command is important. Some background first&hellip;a context is a group of access parameters. Each context contains a Kubernetes cluster, a user, and a namespace. When you are working with multiple contexts off of your development machine, you may run into compatibility issues due to your client version not being compatible with the server API version. All kubectl commands will run against the current context. To check your client version, run:
PS C:\\&gt; kubectl version --client Client Version: version.Info{Major:&#34;1&#34;, Minor:&#34;18&#34;, GitVersion:&#34;v1.18.0&#34;, GitCommit:&#34;9e991415386e4cf155a24b1da15becaa390438d8&#34;, GitTreeState:&#34;clean&#34;, BuildDate:&#34;2020-03-25T14:58:59Z&#34;, GoVersion:&#34;go1.13.8&#34;, Compiler:&#34;gc&#34;, Platform:&#34;windows/amd64&#34;} To ascertain your current context, run:
PS C:\\&gt; kubectl config current-context minikube To list all of your configured contexts, run:
PS C:\\&gt; kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-desktop docker-desktop docker-desktop docker-for-desktop docker-desktop docker-desktop * minikube minikube minikube The * next to minikube indicates that minikube is your current context.
If you are configured to access a cluster hosted from a cloud provider such as Azure, then this context will also be listed.
To use a specific context, run:
PS C:\\&gt; kubectl config use-context docker-for-desktop References Install Kubernetes Install Minikube Kubectl Cheatsheet Minikube&rsquo;s built-in Docker daemon `}),e.add({id:24,href:"/posts/modern-javascript/",title:"Modern-ish Javascript",content:`This post includes a few notes on ECMA language features that I like as well as some info on memory leaking. I have no doubt that it will read disjointed; I started this eons ago and only now have I decided to publish it.
A simple reminder of what Node.js is &hellip;it is a set of APIs wrapped around the V8 Engine (written in c++) and is a high-performance JavaScript and WebAssembly engine.
ES2015 (ES6) Class I find Javascript messy at the best of times. When things are messy, personally I find it difficult to see the forest through the trees, and by this I mean, have I coded for all the *-cases (use/edge/corner)? Or worse, can I see the existing defects or bug breaders?! Then there&rsquo;s the lack of readability.
I&rsquo;ve discussed the use of classes with many Engineers and I have had a mixed reception but in the main, most said they preferred the simplicity of arrow functions. Not sure if there is a right or wrong answer to this (bit like the tabs or spaces&hellip;tabs, obvs!)&hellip; and at one time I will have agreed with the majority. Now though is a different story. Like so many others, I too have drank the cool-aid on TypeScript and now the only reason I can see myself opting for Javascript in the future is mainly for legacy reasons.
Coming from an OOP background, I naturally gravitate towards constructs like classes:
class Admin extends User { constructor (name) { super(name) this.initialize() } initialize = () =&gt; {} } Destructuring const getProfile = () =&gt; { return {firstname: &#34;garrard&#34;, lastname: &#34;kitchen&#34;, married: true, children: 2} } const {firstname, lastname, ...family} = getProfile() console.log(\`firstname: \${firstname}\`) console.log(\`lastname: \${lastname}\`) console.log(family) This would result in:
Arrow function Arrow functions are a great addition to the ES spec! Their scope is purely inside of it&rsquo;s closure and is not affected by the this context which may hoisted functions fall victum of.
initialize = () =&gt; {} ES2016 (ES7) Language Features The Decorator Awesome addition to the EMCA family!
I&rsquo;ve used this with great affect with Typescript, and mostly with NestJS solutions.
This is a contrived example on how you can use very basic decorator on a class function:
You must have configured your solution to use babel
exampleclass Content { @link(&#39;nodejs&#39;, &#34;&lt;a href=&#39;https://nodejs.org/en/&#39;&gt;Node.js&lt;/a&gt;&#34;) html() { return \`This server language is called nodejs!\` } } function link(_find, _replace) { return function(target, key, descriptor) { var old = descriptor.value() descriptor.value = () =&gt; { var n = old.replace(_find, _replace) return n } } } const m = new Content() console.log(m.html()) output:
[nodemon] restarting due to changes... [nodemon] starting \`babel-node index.js\` This server language is called &lt;a href=&#39;https://nodejs.org/en/&#39;&gt;Node.js&lt;/a&gt;! [nodemon] clean exit - waiting for changes before restart babelpackage.json:
... &#34;scripts&#34;: { &#34;start&#34;: &#34;nodemon --exec babel-node index.js&#34; }, ... &#34;devDependencies&#34;: { &#34;@babel/core&#34;: &#34;^7.12.3&#34;, &#34;@babel/node&#34;: &#34;^7.12.1&#34;, &#34;nodemon&#34;: &#34;^2.0.6&#34; }, &#34;dependencies&#34;: { &#34;@babel/plugin-proposal-decorators&#34;: &#34;^7.12.1&#34; } ... .babelrc:
{ &#34;plugins&#34;: [ [&#34;@babel/plugin-proposal-decorators&#34;, { &#34;legacy&#34;: true }] ] } ES2018 (ES9) Spread I was reminded of something useful this morning (on the morning I wrote this, originally!) from a youtube video I was watching. JS passes objects (non-primitives) by reference, ergo, memory pointers, so it is possible to effect an object outside of it&rsquo;s closure. So, imagine you return an array of objects (e.g. from a service to a controller). It is possible, to effect this array of objects from within the controller. One way I have found to avoid this is by using the spread syntax:
private readonly list: string[] getList() { return list } you can do this:
getList() { return [...list] } Obvs, the üëÜ is using an array but you can do this same with an object too {&hellip;list}
Memory Functions arguments passed by value; always See also spread üëÜ to for advance on how to avoid memory leakage.
Further to the above, JS always passes by value (not reference) ALL augments to a function. This means that, if you pass in an argument (primitive or object) into a function, the closure is honoured and therefore any changes made to this value inside the closure is not reflected outside, example:
let v: string = &#34;A&#34; getValue(v){ v = v + &#34;B&#34; } let result = getValue(v) console.log(result) // output: AB console.log(v) // output: A `}),e.add({id:25,href:"/posts/mentoring/",title:"How do I mentor?",content:`I have written this post to document my experiences of mentoring. I have mentored front-end engineers, back-end engineers and UX designers. I have had the pleasure of helping others as well as learning one of two things about myself along this journey too. If ever you get the opportunity to be a mentor, I recommend you jump at the opportunity. It is a self-rewarding experience.
So, what is mentoring?&hellip;
The definition of Mentoring is the act of advising or training (someone, especially a younger colleague).
In her book The Manager&rsquo;s Path, Camille Fournier talks about Mentoring. She writes:
&quot;The first act of people management for many engineers is often unofficial.&quot;This has always been the case for me too. I am currently employed as a Principal Engineer, before this, a CTO. In this time, I have neither organised nor carried out an official [backed by a recognised authority] mentoring scheme. It&rsquo;s just been something that I do, without fuss but with purpose and pride.
Oddly, I have never been a mentee. If I had then there is a possibility that this in itself may have defined or partially influenced mentoring for me.
This is a list of scenarios where I have mentored others in:
onboarding new company starters, onboarding a new colleague at a similar level as myself onboarding a graduate (their first job since graduating from university) when working on a project together Concerning the above mentioned scenarios, I have both created and coordinated an onboarding programme. This was when I was a CTO. All this was choreographed remotely. Ironically, this is more relevant today than ever. As I write this CV-19 has started to take a grip of the UK and yesterday I heard of the sad news that 2 people had died from it in Southport where I have resided since 2008.
This is a bullet list of key &rsquo;things&rsquo; that I have discovered that have helped me through the mentoring process:
communicate what the process of mentoring is to the mentee first, listen, then respond. Don&rsquo;t attempt to expedite the process, don&rsquo;t forget, it&rsquo;s for them, not you! take the time to explain the rationale for a decision take the time to explain why something is not applicable in that particular instance try not to provide answers, but provide strategies (alternatives, is there an easier to do the same, what is the problem we&rsquo;re trying to solve) allow for mistakes to be made and always follow them up with a post mortem. We all make mistakes, in some cases, it helps define you. Making a mistake is critical to our development so this is why the next point is important&hellip; ensure you make a safe environment for your mentee to operate in make time but be clear about the amount of time you can give. You will have other responsibilities. Inadvertently, you are forcing the mentee to make decisions. This often encourages the mentee and gives them the confidence to stand on their own two feet. This too is critical for their development work on a real project, albeit, scaled back for safety and to limit the blast radius. It has to be something that matters to the business. This will help the mentee be recognized by their good work. By limiting the hypotheticals, the mentee will then get their hands on a non-fabricated, warts and all, real-life engineering problem to help in the preparation of an important [to them] event - this has meant helping produce the materials for an event as well as assessing and providing feedback develop a personal development plan - used to help keep focus as well as a comparator. This can take up a chunk of time but well worth it plus you&rsquo;re holding yourself accountable to the process too! As CTO I led both the architectural and the planned engineering effort that has been key to the strategic direction of that business. Mentoring was an important part of this process and as such, I was always in mentoring mode. To this day, no longer a CTO but still in a senior engineering position, I constantly think about, and act on, ways to help those around me to improve their engineering capabilities (think good engineering principles).
Although not all of my mentoring is official, I do conduct myself in such a way that it benefits those around me. I do this by encouraging my co-workers whenever possible. Here is a list of how I have been able with success, help my co-workers:
I demonstrate, then I include a co-worker in this process. An example of this is by whiteboarding a problem or solution. I hand over the marker and this leads to them articulating their solution in front of an audience I instigate a technical discussion or articulate an engineering problem. I solicited input from all (introverts and extroverts alike). This encourages my wo-workers to speak up and gain confidence in discussing technical issues in front of an audience I am consistent in the message of working in a safe environment, one where any question can be asked and any view given I define a piece of work&rsquo;s guiding principles upfront. This helps in several ways. It defined the focus of the project, what to exclude etc. It also helps shape our collective thinking and finally, it&rsquo;s a gentle way into a project instead of a rushing headlong into it without giving it any due diligence Redirect to good engineering principles whenever possible to enforce our foundation of good engineering. I am a Principal Engineers and as such, I have a responsibility to my co-workers and the business to conduct myself in a way befitting a Principal Engineer. Quite simply put, one of the objectives is to help my co-workers in whatever way possible. This can be helping them out on a project. It can be providing feedback on a piece of work or technique. Ultimately, my goals are to be supportive, helpful, insightful, encouraging, guiding, a sounding board and inspirational. All executed respectfully. The people I have worked with and those who I currently work with are important to me. Anything I can do to help, I do. Even if it&rsquo;s listening to them sound off. Returning to my goals&hellip;I do see some of these being reflected at me but more importantly, I see the product of my mentoring too, which I find extremely satisfying!
One of the most humbling times of my life was when I mentored a colleague who, through no fault of his own, was temporarily let go from the company I was a CTO for. We as a company were struggling financially and had to slim down the workforce. It was a sh*t time. It was important to me though from a personal perspective that I didn&rsquo;t just sever contact with him. The plan was always to bring him back onboard once things improved. And they did. But during the time that it wasn&rsquo;t so great, I would meet-up regularly with him online - he was based in another country. We would discuss many topics; life, technology &amp; side projects. Where I could, I&rsquo;d provide guidance and be a sounding board for him. From time to time I would plan things for him to do. The next time we met up, I&rsquo;d review what he had done and provide feedback when necessary. I would like to think that this created a bond between us. Like I say, it was all very humbling as after all, I was still in gameful employment. At some level, it must have been a bitter pill for him to swallow and he never held it against me, which is demonstrative of his good character. We no longer work together but he remains a friend and we do still often catch up online.
Outcomes from the mentoring process can also be subtle. Just to be clear, it&rsquo;s not always explosive or awe-inspiring either. It is what it is and a poor result does not equate to a lack of mentor&rsquo;s ability. Generally, poor results are rare. In the one case where I observed poor results, I reported it upwards. The vertical market we were operating in didn&rsquo;t float this particular mentee&rsquo;s boat. It happens! Also, in my experience, it is always noticeable over time; providing you take a documented snapshot before and after. One source of personal satisfaction is seeing mentees, new and old, interacting with seasoned engineers, observing them standing on their own two feet, adding value to a conversation and project work alike. Best of all, seeing a seasoned engineer asking a mentee for their advice and input on a scenario. That my friends, is extremely satisfying!
Written mainly for me, I do hope you&rsquo;ve found something useful here and who knows, it might even help you with your mentoring journey too.
`}),e.add({id:26,href:"/posts/principles/",title:"Good Engineering - Principles",content:`I have written this post as a method to document what I see as the basics, foundations if you will, for good engineering. Undoubtedly if you are a seasoned engineer, you will recognised all of these principles, less so, if you&rsquo;re just starting out.
Most Engineers are fully versed in the foundations of writing quality, efficient, succinct and testable code. As a Principal Engineer, one of my responsibilities is to ensure that these (1) foundations are recognised by the engineers and (2) are adhered to by all engineers.
Here&rsquo;s a list of concepts that for me, constitute good engineering principles:
These are in alphabetical order and not in order of importance
Clean and readable code Code reviews Coding standards Composition over inheritance Defensive coding Do no more DRY KISS Occam&rsquo;s razor Premature optimization Refactor Separation of Concerns SOLID Testing YAGNI Other sections:
My pattern discovery Being a Principal Engineer Discussion point References Clean and readable code Clean and readable code is always better than clever code (ask any engineer who has to extend or maintain a clever piece of code!)
I&rsquo;ve seen a lot of code recently that should never have got to the shape it has. Complicated code requires time to understand, then time to add functionality. Complicated code also happens to more difficult to recall so each time you need to go near it, you have to relearn it and added to this, any changes made to improve it, most likely have not been applied in full so they&rsquo;ll be a right old mixture of good, bad and the ugly thrown into the mix.
A good measure of how bad a codebases is, and I&rsquo;m going to plagiarise somebody else&rsquo;s analogy here, is by stepping through an interactive debug session. If you get momentarily distracted by a fly, then immediately return to the debugging and you do not know where the feck you are in the execution of the code flow, then it&rsquo;s a bad codebase!
It&rsquo;s the responsibility of a Tech Lead or architecture to stop code bases ending up this way.
Code reviews It should only contain helpful and constructive comments and/or implementation questions. This process is not there to caress egos (that&rsquo;s for your mother to do!!). One useful by-product of code reviews is conveying of your team&rsquo;s exacting coding standard and attention to deal, to new starters. So, the quicker the new starter pushes a commit, the better!
Coding standards (provide a template of core standards then stand back and let the team thrash out the rest - wear protection!)
Although important, it&rsquo;s not the end of the world if some of the granular details differ between teams. The important thing here, in my opinion, is that each team know where to find their cheese. Most engineers in a team have a common set of standards they adhere too. The big things like solution structure, naming conventions, testing (AAA, GWT), pluralization, documentation structure (including README) all need to be consistent.
Composition over inheritance (avoid class tree exploitation! - think Strategy pattern - GoF)
The above-bracketed statement says it all! Inheritance tends to back you into a corner especially when you consider the OCP.
Defensive coding (guard against invalid class method parameters and accidental null assignment to class properties instead of an equality condition!)
This is one example of defensive coding:
class User(string firstnaame, string lastname, int age) { if (null == firstname) { throw new NullReferenceException(&#34;Firstname cannot be null&#34;) } ... The above demonstrates an example of defensive coding. The first is that we need to test for valid constructor parameter values when instantiating a class.
The second, is to avoid mistakes that might not be picked up by your compiler. For instance, a common mistake doing this:
if (firstname = null) A .NET Compiler is more than happy allowing this above syntax, as, after all, it&rsquo;s an assignment operator and not a equality operator as in above in the class constructor. By switching these around, you&rsquo;re making a positive pattern changing and should avoid making this silly mistake again.
Do no more (and do no less - thank you eXtreme Programming!).
If you code outside the scope, you&rsquo;re in danger of creating code that isn&rsquo;t used or needed. The worse thing about this is that others will have to maintain this code. How can this be? Well, it&rsquo;s common - think HTTP chaining - for code not to be culled especially if there is a disconnect between these dependencies and there&rsquo;s no IDE/compiler to shout at you.
DRY (don&rsquo;t repeat yourself)
Code analysis tools help here, but you&rsquo;re not always going to have access to these tools.
One way to help identify code that does the same thing is by refactoring. If you keep your code method frame small (~20 lines), and you have a good naming standard for methods (e.g. noun+verb with accurate alighment to business capability - think DDD), have unit tests with a high code coverage percentage, then this should be all you need to help you avoid writing duplicate code.
KISS (keep it simple, silly)
This to a certain extent, goes hand in hand with avoiding premature optimization. We all like the big picture yes? This doesn&rsquo;t mean we need to do deliver on this it right now! You just need to know the boundaries of this piece, which, if greenfield, then you won&rsquo;t have any metrics to tell you the actual demand. Think Capacity planning; what this piece of work needs to do based on current expectations. For example
Do we need multiple servers? Yes, I think Why do we need multiple servers? Mmmmm, because I read it somewhereDo you have the metrics that support your argument for multiple servers? Wait, what?Next!A colleague recently shared with me the architecture of their side project. They are using AWS and I have 2 certifications in AWS (Developer and Solutions Architect). I quickly went into HA/scaling/resilience/durability/DR overdrive, following it up with a verbal dump on what tech they should use. This was all wrong. They did not know their service demand. Following my initial advice, will have increased their cost; unnecessarily. I did, you&rsquo;ll be glad to hear, re-affirm their decision (may have made 1 or 2 helpful suggestions) shortly after [~2 hours].
Yeah, think big but don&rsquo;t deliver big without a customer base; as this, in my experience, will result in a huge waste of time, effort and money. Plus, sometimes, you don&rsquo;t really know where something is going to take you, and my advice here is to roll with it. This last piece of advice is particularly pertinent if you&rsquo;re starting up.
Occam&rsquo;s Razor This is a problem-solving principle.
The definition of this is: &ldquo;Entities should not be multiplied without necessity&rdquo;. It is sometimes paraphrased by a statement like &ldquo;the simplest solution is most likely the right one.
Occam&rsquo;s razor says that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions. Good advice
Suppose there are two competing theories on why something is not working. Normally, the case that requires the least amount of assumptions is correct. So, the more assumptions you have to make means it more likely to be more unlikely.
Premature optimization Avoid premature optimization and all conversations relating to optimization until you know the facts. This will be futile until you&rsquo;ve metrics to better inform you.
I&rsquo;ve hit this numerous times when planning for microservices and bounded contexts, in particular, on green-field projects. What should we include and where? Should we separate claims away from users for instance? Will the demand for Claims be greater than for users? Who knows?! You don&rsquo;t until you have some metrics behind you. You can always merge or break them [microservices] up later.
Another area that I believe this encompasses is splitting code up across multiple files and folders. If it&rsquo;s a PoC, a sample piece of code, or something that has a short shelf life, just keep it in one file. When it&rsquo;s the right time - moving out of PoC/other - then you can consider optimizing it. Up until then, it&rsquo;s a huge waste of time and effort.
Architecture is a great example of when not to prematurely optimize. Architecture normally infers cost. Generally, the more of something, the greater the cost. This could mean for a startup the difference between survival and their demise. Adopting a guiding principle of being frugal from the outset, is a prudent and wise decision. What this means is that you&rsquo;re always looking for the most cost-effective way of accomplishing your goal. So, if you don&rsquo;t know your demand, it means you opt for a single server instead of having a HA cluster of 3 master nodes and 5 worker nodes! Down from 8 servers to 1 which on a month by month basis during development and beta/early releases could mean the saving of thousands of pounds sterling.
Sadly, I&rsquo;ve come across a few startup that have failed just because they ran out of cash early on. It&rsquo;s a real shame for all involved.
Refactor &hellip;refactor refactor
Don&rsquo;t save this until the end of a piece of work &hellip; you&rsquo;re bound to miss something and possibly add to your team&rsquo;s tech debt. Plus, if you push your commits to a PR, you&rsquo;ll get your ass handed to you by your peers!
Things to consider here are DRY and TDD. Both will nudge you towards a proper refactoring effort.
Separation of Concerns (think MVC, CQRS, bounded context, etc&hellip;)
It&rsquo;s all about doing the right this in the right place! I recently ran, architected and co-developed a project that involved our own hosted solution, a solution hosted on Azure and a solution hosted on the Twilio Cloud (Twilio Serverless Functions). Originally, the requirements did not include the Twilio Cloud and would have required a bucket load more effort if we&rsquo;d stuck with that brief. Thankfully, I chose to take full advantage of what Twilio has to offer and used a combination of Twilio Flow and Twilio Serverless Functions. By establishing these SoCs it meant:
a less stressful implementation a light touch to our own hosted solutions a satisfying amount of fun working with Serverless (has been my favourite and advocated approach for several years!) a time saving it revealed a range of options when dealing with specific edge and corner cases which, again, giving us a further time savings. SOLID These are the SOLID principles:
Single Responsibility Principle Open Closed Principle Liskov Principle Interface Segregation Principle Dependency Inversion Principle Single Responsibility Principle A class (no method) should have one and only one reason to change, meaning that a class (or method) should have only one job.
&ldquo;When a class has more than responsibility, there are also more reasons to change that class&rdquo;
Here&rsquo;s an example of a class )purposefully awful for illustrative purposes):
class User() { public string Username {get; set;} public string Fullname {get; set;} private readonly ILogger _logger; private IDbContext _db; public User() { _logger = new Logger() _db = new UserContext(); } public Task&lt;User&gt; GetProfile(string username) { ... _logger.Info($&#34;Found profie for {username}&#34;) return this; } } You could say that the above includes both a model responsibility and a service responsibility. These should be split into two separate .NET types, as in this example:
class User() { public string Username {get; set;} public string Fullname {get; set;} public User(string username, string Fullname) { ... } } class UserService() { private readonly ILogger _logger; public UserService(ILogger _logger, IDbContext db) { _logger = _logger _db = db; } public Task&lt;User&gt; GetProfile(string username) { ... _logger.Info($&#34;Found profie for {username}&#34;) return user; } } Here are the benefits of principles:
Reduces complexity in your code Increases readability, extensibility, and maintenance of your code Reusability and bug breading Easier to test Reduces coupling by removing dependency between methods Open Closed Principle Objects or entities should be open for extension, but closed for modification. So, what does this mean? Let&rsquo;s break this down to two statements:
Open for extension Closed for modification Open for extension: This means that we need to design our classes in such a way that it&rsquo;s new responsibilities or functionalities should be added easily when new requirements come.
One technique for implementing new functionality is by creating new derived classes. A derived class will inherit from base class. Another approach is to allow the &lsquo;client&rsquo; to access the original class with an abstract interface. I sometimes think of this simply as removing if statements by extension but I&rsquo;m not convinced everybody would agree with this assessment though.
So, in short, if there&rsquo;s an amendment or any new features required, instead of touching the existing functionality, it is better to create new derived class and leave the original class implementation. Well, that&rsquo;s the advice! I worry about the class explosion and if you&rsquo;re attempting to do this on top of not so perfect code!
Closed modification: This is very easy to explain&hellip;only make modifications to code if there&rsquo;s a bug.
This sample looks at delegating method logic to derived classes.
public class Order { public double GetOrderDiscount(double price, ProductType productType) { double newPrice = 0; if (productType == ProductType.Food) { newPrice = price - 0.1; } else if (productType == ProductType.Hardware) { newPrice = price - 0.5; } return newPrice; } } public enum ProductType { Food, Hardward } Can rewrite, still using base implementation (think decorator pattern):
public class Order { public virtual double GetOrderDiscount(double price) { return price; } } public class FoodOrder : Order { public override double GetOrderDiscount(double price) { return base.GetOrderDiscount(price) - 0.1; } } public class HardwareOrder : Order { public override double GetOrderDiscount(double price) { return base.GetOrderDiscount(price) - 0.5; } } Liskov Principle Definition: &ldquo;Let q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T.&rdquo; &hellip; clear as mud right?
All this is stating is that every subclass/derived class should be substitutable for their base/parent class.
The example below demonstrates a violation of the Liskov principle, as by replacing the parent class (SumEvenNumbersOnly-&gt;Calculator), this does compromise the integrity of the derived class as the higher-order class is not replaced by the derived class. Here, both cal and eventsOnly variables will be the same:
... var nums = new int[] {1, 2, 3, 4, 5, 6, 7}; Calculator cal = new Calculator(nums); Calculator evensOnly = new SumEvenNumbersOnly(nums); ... public class Calculator { protected readonly int[] _numbers; public Calculator(int[] numbers) { _numbers = numbers; } public int Sum() =&gt; _numbers.Sum(); } public class SumEvenNumbersOnly : Calculator { public SumEvenNumbersOnly(int[] numbers) : base(numbers) { } public new int Sum() =&gt; _numbers.Where(x=&gt;x % 2 == 0).Sum(); } Here we have changed the assumed base class to an abstract class. Now, it can&rsquo;t be instantiated and instead, must be inherited. This ensures the derived classes must implement the method detail. So, even if we replace the type declaration with the higher-order class, we should still get the intended result:
... var nums = new int[] {1, 2, 3, 4, 5, 6, 7}; Calculator cal = new SumAllNumbersOnly(nums); Calculator evensOnly = new SumEvenNumbersOnly(nums); ... public abstract class Calculator { protected IEnumerable&lt;int&gt; _num; protected Calculator(IEnumerable&lt;int&gt; num) { _num = num; } public abstract int Sum(); } public class SumAllNumbersOnly : Calculator { public SumAllNumbersOnly(IEnumerable&lt;int&gt; num) : base(num) { } public override int Sum() =&gt; _num.Sum(); } public class SumEvenNumbersOnly : Calculator { public SumEvenNumbersOnly(IEnumerable&lt;int&gt; num) : base(num) { } public override int Sum() =&gt; _num.Where(x =&gt; x % 2 == 0).Sum(); } Interface Segregation Principle A client should never be forced to implement an interface that it doesn&rsquo;t use or clients shouldn&rsquo;t be forced to depend on methods they do not use.
Take the following interface:
public interface IAllTheThings { Task&lt;IAsyncEnumerable&lt;Claim&gt;&gt; GetClaims(string username); Task&lt;IAsyncEnumerable&lt;User&gt;&gt; GetUsers(string team); Task&lt;User&gt; AddUsers(User user); } There&rsquo;s a clear distinction in responsibilities that are being suggested here by the contract name. Sufficed to say, these should be split across different interfaces:
public interface IUser { Task&lt;IAsyncEnumerable&lt;User&gt;&gt; GetUsers(string team); Task&lt;User&gt; AddUsers(User user); } public interface IClaim { Task&lt;IAsyncEnumerable&lt;Claim&gt;&gt; GetClaims(string username); } Dependency Inversion Principle There are 2 rules here:
High-level modules should not depend on lower-level modules. Both should depend on abstractions. Abstractions should not depend upon details. Details should depend upon abstractions. Let&rsquo;s deal with the first rule first. High-level means policy, business logic and the bigger picture. Lower-level means, closer to the bare metal (think I/O, networking, Db, storage, UI, etc&hellip;). Lower-level tend to change more frequently too.
These two examples show perfectly the before and after of the move to a &lsquo;depend on abstraction&rsquo;:
public class BusinessRule { private DbContext _context; public BusinessRule() { _context = new DbContext(); } public Rule GetRule(string ruleName) { _context.GetRuleByName(ruleName); } } public class DbContext { public DbContext() { } public Rule GetRuleByName(string name) { return new Rule(new {Name = &#34;Allow All The Things&#34;, Allow = false}) } } After changing to an abstraction:
public interface IDbContext { Rule GetRuleByName(string name); } public class BusinessRule { private IDbContext _context; public BusinessRule(IDbContext context) { _context = context; } public Rule GetRule(string ruleName) { _context.GetRuleByName(ruleName); } } public class DbContext : IDbContext { public DbContext() { } public Rule GetRuleByName(string name) { return new Rule(new {Name = &#34;Allow All The Things&#34;, Allow = false}) } } With the above change, the DbContext can be any class as long as it inherits from the IDbContext interface and has a method with a signature of Rule GetRuleByName(string name).
The above is demonstrative of the 2nd rule; do not depend on the detail. As you can see, in the example above, we&rsquo;re depending on an interface method contract and the actual implementational detail is being dealt with by the Lower-level class.
The above example includes Dependency Injection. Although you can accomplish IoC with DI, they are not the same thing. IoC does not mention anything about the direction of the dependency.
Generalization restrictions The presence of interfaces to accomplish the Dependency Inversion Pattern (DIP) has other design implications in an object-oriented program:
All member variables in a class must be interfaces or abstracts All concrete class packages must connect only through interface or abstract class packages No class should derive from a concrete class No method should override an implemented method All variable instantiation requires the implementation of a creational pattern such as the factory method or the factory pattern, or the use of a dependency-injection framework. Testing (unit/functional, including concepts like TDD &amp; BDD and frameworks)
For testing to be a success, the details are key. These details will come in the form of a specification or from a verbal conversation (always to be confirm in writing later). If you&rsquo;re lucky, these test cases will be included as ACs (Acceptance Criteria) in the Scrum Story Description.
Taking a test driven development approach to writing code often results in:
a reduction in verbose code less post-deployment bug fixing succinct (do no more, no less than is required), structure and logic. Testing is important. Obviously! I often refer to testing as &lsquo;having your back&rsquo;. It ensures you don&rsquo;t break existing functionality when implementing new functionality or dealing with tech debt. It also protects new engineers from breaking things as well as extant engineers who may have touched this repository many times before.
Tests aren&rsquo;t just for new functionality either. If you change extant functionality or class responsibilities you must modify extant tests or create new tests. Ideally, your CI build pipeline should run tests every time time a commit(s) is pushed to a PR or Draft PR. This last step is here, to again, have your back and to safeguard against erroneous code poluting your codebases and getting into production.
In the .NET world, there are many testing frameworks available; xUnit, NUnit, MSTest to name a few. There are also many mocking frameworks available; Moq, NSubstitute, JustMock, again, to name a few. Frameworks like these help make the testing process and overall experience less painful and cumbersome and some might even say it makes this part of development, pleasurable!
My .NET Core testing and mocking preferences are xUnit &amp; Moq and my javascript (including node.js) testing framework preference is Jest.
This code sample shows how both a testing and mocking frameworks compliment each other:
using Moq; using Xunit; namespace BasicAAATestExample { public interface IUser { string GetFullname(); string Firstname { get; set; } string Lastname { get; set; } } public class User : IUser { public string Firstname { get; set; } public string Lastname { get; set; } public string GetFullname() { return $&#34;{Firstname} {Lastname}&#34;; } } public class Notify { private IUser _user; public Notify(IUser user) =&gt; _user = user; public string GetMessage() =&gt; $&#34;{_user.GetFullname()} has been notified&#34;; } public class NotifyTests { [Theory] [InlineData(&#34;Garrard&#34;, &#34;Kitchen&#34;, &#34;Garrard Kitchen has been notified&#34;)] [InlineData(&#34;Charles&#34;, &#34;Kitchen&#34;, &#34;Charles Kitchen has been notified&#34;)] public void GivenGetMessageIsCalled_WhenFirstAndLastNameExist_ThenReturnsANotificationMessage(string firstname, string lastname, string expected) { // arrange var mockUser = new Mock&lt;IUser&gt;(); mockUser.Setup(x =&gt; x.GetFullname()).Returns($&#34;{firstname} {lastname}&#34;); var sut = new Notify(mockUser.Object); // act string message = sut.GetMessage(); // assert Assert.Equal(expected, message); mockUser.Verify(x =&gt; x.GetFullname(), Times.Once); } } } The single unit test above follows the AAA (Arrange, Act, Assert) pattern and is a common way of writing unit tests for a method under test:
the Arrange section of a unit test method initializes objects and sets the value of the data that is passed to the method under test the Act section invokes the method under test with the arranged parameters the Assert section verifies that the action of the method under test behaves as expected. There are a few standards I adhere to when it comes to writing tests. In the sample unit test above these standards include:
the method name (GWT) the comment blocks of arrange, act and assert the name of the mock instantiated object (mock&lt;Class&gt;) the class name of the SUT - system under test - (sut). YAGNI (you ain&rsquo;t going to need it)
Do no more, and no less than is required. You do not want to have to maintain code that is never used or produce code that others have to maintain unwittingly. It&rsquo;s very difficult to future proof your code if you do not know what&rsquo;s going to happen, let alone without a specification! It&rsquo;s a guess at best so don&rsquo;t waste your time or others. Keeps things concise, succinct and simple.
My pattern discovery I&rsquo;m a huge fan of patterns, especially cloud architectural patterns but sometimes, they add unnecessary complicity so beware!
When I first started learning about patterns - some 18 years ago - I went through a few codebases I was involved with at the time to see if I&rsquo;d subconsciously been repeatedly using a pattern &hellip; and I had! It was the lazy loading pattern&hellip;which I continue to use regularly today!
Being a Principal Engineer As a Principal Engineer, I consider the above as the foundation for writing quality code. The objective of this list, in conjunction with the message I propagate via this list, during discussions, evidence from my own work and by leading from the front within my role, is one of a reminder to me and my colleagues of best practice and commitment to quality and good practice. As with all foundations, it forms the base from which more advanced concepts or approaches can be learned. An important part of this practice is heuristic - enabling a person to discover or learn something by themselves. So, how do I go about doing this?
These are some of the activities I execute to embed good engineering principles:
1-2-1 Group conversations Advocate online learning platforms such as Pluralsight or Udemy. For the more keen Engineer, I also recommend certification. YouTube is another favourite of mine. With YouTube, you can tag recordings, therefore building up a catalogue of materials that you can make public Workshops Brown bags Capture How To Do&rsquo;s in wikis or similar Coding advice/tips (e.g. when to use Task instead of an Async method) Take the time to explain questions about implementation reasons in DVCS Pull Requests Share blog posts &amp; other materials across multiple channels Compile a learning profile for an individual The coding advice/tips above are interesting ones. As professionals, we always want to improve our ability to code, how we approach problems, etc&hellip;, and in doing so we want our colleagues to benefit from our experience. I recently became reacquainted with coding katas. As a black belt in Ju-Jitsu I am well versed in what a kata is. Katas can also be used to remind, stretch and improve our core coding capability. The last time I used a kata in programming was 10+ years ago. This was when I was first introduced to TDD. A favourite development book of mine is &lsquo;The Art of Unit Testing&rsquo; by Roy Osherove. It is the first edition. For many years I had it as a click-thru purchase option on a previous blog site of mine. I&rsquo;ve not really participated in many katas since. I have written a few though and now having been reintroduced to them and reminded of their potential, as a Principal Engineer I can see it as an invaluable tool. One thought I&rsquo;ve had is to use it as a framework to assess an Engineer&rsquo;s current capability and then use during pair programming to help share coding techniques, approaches and standards.
Pair programming is a wonderful technique for propagating developer skills (think how to use cloud services), approaches to coding (think TDD and problem solving), embed team coding standards and code review in realtime. Pair Programming is an Extreme Programming technique. It is not a mentoring or coaching technique but some do use it for this. Quite often, I find I only participate in pair programming is one of two use cases. (1) if the subject I&rsquo;m investigating is new (important to have shared knowledge) and (2) when I&rsquo;m helping an Engineer to overcome an esoteric issue. You know what they say? &hellip;a problem shared is a problem halved! However, now, I&rsquo;ll be including Pair Programming in conjunction with katas as part of my techniques to stretch the Engineer&rsquo;s muscle memory (including mine!).
I love hacking away at code as much as the next Engineer. Hacking code is a great way to experiment and to get out of the starting gate. However, when it comes to pair programming I do like to give it the necessary due diligence. By this I am referring to allowing for a few moments up front to deliberate and agree on what&rsquo;s required. This checklist guides me and ensures up front I set off in the right frame of mind:
the objective of the pair programming exercise (think the desired outcome - you might even want to frame this with a Story Description; AS A, I WANT, SO THAT) what libraries (3rd party) will we need (think cloud provider SDKs and vendor client APIs) how are we going to test the code we write (think unit tests, integration, functional [e2e] as well as your approach e.g. TDD). After the session has finished I like to perform one final task. This is to document findings/learnings and areas that require further investigation. This is normally helped by capturing notes as we go along.
As a side note to TDD, with modern compilers (think Roslyn in the .NET world) and even linting to a certain extent, you know if something will fail - if a reference type (.NET) does not exist yet - as your IDE will be screaming at you, so I don&rsquo;t run tests that are missing these reference types (think classes and interfaces in the .NET world).
Discussion point I&rsquo;m sure I&rsquo;m not alone here when I say, having the time available for 2 Engineers to code together for skills transfer etc is a challenging one. An agile sprint doesn&rsquo;t facilitate this. This is something that I often refer to as having the &lsquo;space to learn&rsquo;. The pressures of a sprint often, sadly, works against this. This is doubly as difficult, if your sprint is made up of technical debt, BAU, Ad-hoc etc&hellip; Timeboxing &rsquo;effort&rsquo; into percentages doesn&rsquo;t always present an obvious education path for your Engineers either. Having a day (developer day or similar) dedicated to learning also never really quite works out the way it&rsquo;s meant too, plus, &lsquo;a day&rsquo;?! In my experience, this, and trying to cram genius into a time box also never quite works either. After all, you can&rsquo;t schedule genius, in the same way, you can&rsquo;t guarantee that the best Engineers are in your locality, or that the best time for Engineers to work is between 9-5.
What is the answer? A mixture of all the above, at hock and at scheduled times, to ensure quality and advancement of skills.
When I do speak out regarding the above, I inevitably also lead this conversation into Engineering not having the kit [hardware &amp; software] they need. Engineers require the software and hardware they deem as necessary to be effective in their role. I once gave an analogy of, not giving Engineers the right kit is like giving a roller brush and a Pogo stick to Michelangelo to paint the Sistine Chapel ceiling. He&rsquo;ll manage it &hellip; eventually, but the attention to detail and accuracy will be woefully inadequate.
Written mainly for me, I do hope you&rsquo;ve found something useful here, and who knows, it may even help you with your engineering journey too.
References The pragmatic programmer
YAGNI
XP
Dependency inversion principle
OOP design patterns
Inversion of Control Containers and the Dependency Injection pattern
Write your tests
`}),e.add({id:27,href:"/posts/",title:"Blog",content:`This section contains articles form my old blogging site. Not all have been ported so will look sparse
`})})()