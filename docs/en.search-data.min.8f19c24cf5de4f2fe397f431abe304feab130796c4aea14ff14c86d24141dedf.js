'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/posts/jest-fs-readFileSync/','title':"Unit testing and mocking fs.ReadFileSync",'content':"I\u0026rsquo;d just ran npm run test in a newly created package I\u0026rsquo;d added to a monorepo (lerna) I\u0026rsquo;d created for a project I was working on that integrates with Twilio Sync, RabbitMQ, Twilio TaskRouter and MSSQL, and I go this:\n*******************************consumers\\packages\\eda [CRMBROK-233 +0 ~2 -0 !]\u0026gt; npm run test \u0026gt; @cf247/eda@1.0.2 test *******************************consumers\\packages\\eda \u0026gt; jest FAIL __tests__/eda.test.js ● Test suite failed to run ENOENT: no such file or directory, open \u0026#39;.env\u0026#39; 2 | const fs = require(\u0026#39;fs\u0026#39;) 3 | const dotenv = require(\u0026#39;dotenv\u0026#39;) \u0026gt; 4 | const envConfig = dotenv.parse(fs.readFileSync(`.env`)) | ^ 5 | for (const k in envConfig) { 6 | process.env[k] = envConfig[k] 7 | } at Object.\u0026lt;anonymous\u0026gt; (lib/setenv.js:4:35) at Object.\u0026lt;anonymous\u0026gt; (lib/eda.js:1:1) Test Suites: 1 failed, 1 total Tests: 0 total Snapshots: 0 total Time: 1.772 s Ran all test suites. npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! @cf247/eda@1.0.2 test: `jest` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the @cf247/eda@1.0.2 test script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm WARN Local package.json exists, but node_modules missing, did you mean to install? npm ERR! A complete log of this run can be found in: npm ERR! *******************************\\npm-cache\\_logs\\2020-05-28T08_04_32_271Z-debug.log *******************************consumers\\packages\\eda [CRMBROK-233 +0 ~3 -0 !]\u0026gt; Not great but hey, first run and all!\nThe error message tells me everything I need to know:\nENOENT: no such file or directory, open \u0026#39;.env\u0026#39; 2 | const fs = require(\u0026#39;fs\u0026#39;) 3 | const dotenv = require(\u0026#39;dotenv\u0026#39;) \u0026gt; 4 | const envConfig = dotenv.parse(fs.readFileSync(`.env`)) Which is that it can\u0026rsquo;t find an .env file. And it wouldn\u0026rsquo;t. Later refactoring would remove this file dependency but for now, all I want to do is to get my test working.\nThis was the unit test code:\n\u0026#39;use strict\u0026#39; const eda = require(\u0026#39;..\u0026#39;) describe(\u0026#39;@cf247/eda\u0026#39;, () =\u0026gt; { it(\u0026#39;no tests\u0026#39;, () =\u0026gt; { }) }) This is the code from the module it was importing via the require('..') statement:\nrequire(\u0026#39;./setenv\u0026#39;) const amqp = require(\u0026#39;amqplib/callback_api\u0026#39;); module.exports = (io, emitter) =\u0026gt; { ... The top line is importing code from this file:\nI\u0026rsquo;ve highlighted the problematic line of code\n1 2 3 4 5 6  const fs = require(\u0026#39;fs\u0026#39;) const dotenv = require(\u0026#39;dotenv\u0026#39;) const envConfig = dotenv.parse(fs.readFileSync(`.env`)) for (const k in envConfig) { process.env[k] = envConfig[k] }    The quickest (IMO) way to deal with this and move forward is to Mock the fs class. I did this by included a jest module mock into my unit test file:\nI\u0026rsquo;ve highlighted the mock related code\n1 2 3 4 5 6 7 8 9 10 11 12 13  \u0026#39;use strict\u0026#39; const fs = require(\u0026#39;fs\u0026#39;) const eda = require(\u0026#39;..\u0026#39;) jest.mock(\u0026#39;fs\u0026#39;, () =\u0026gt; ({ readFileSync: jest.fn((file_name) =\u0026gt; { return [] }) }))  describe(\u0026#39;@cf247/eda\u0026#39;, () =\u0026gt; { it(\u0026#39;no tests\u0026#39;, () =\u0026gt; { }) });    What this does is, when the readFileSync class function is called, it always returns an empty array []. As the unit code does not have a dependency on environment variables, this mocked response will work fine.\n"});index.add({'id':1,'href':'/posts/reality-of-engineering/','title':"Reality of Engineering",'content':"I gave myself some downtime this weekend. The weather was rubbish so unfortunately I couldn\u0026rsquo;t execute some \u0026ldquo;list\u0026rdquo; tasks. Instead, I watched some movies, helped my boys find stuff, build and generally make a mess - ah, the good stuff yeah?! This gave me the necessary space to think about some of the areas of engineering that would have benefited me from knowing \u0026ldquo;at the time\u0026rdquo;. Below is a list of guiding principles, advice and general experience in engineering resulting from this \u0026ldquo;moment of clarify\u0026rdquo;. I never know whether to call it engineering or development. Apologies if I switch between the two terms in this post. I essentially mean the same thing. One thing to note, it\u0026rsquo;s a list with some content. It not exhaustive and they are just my opinions. As always, this is a reminder to myself but hoping this will help others too!\n No code is perfect Put the keyboard down, and walk away\u0026hellip;.but only after you\u0026rsquo;ve finished writing the unit tests, yes. Code and requirements change frequently. So, don\u0026rsquo;t over think your code. Just make sure you adhere to good engineering principles and that your code does exactly what it is required to do, no more. Please don\u0026rsquo;t add code that isn\u0026rsquo;t needed. This will become somebody elses nighmare to support and to refactor later!\nNow, write tests and move on\u0026hellip;after capturing tech debt of course!\nTech debt Technical Debt is created all of the time. It is what happens when you develop. It shouldn\u0026rsquo;t been seen as a bad thing, providing it is managed correctly and as well as openly. This is a key point, it must be captured. So, what do you need to capture? Well, this is will give you a good start:\n what needs to be done to deal with it (e.g. re-platform to the cloud) why the decision was made to leave it as tech debt (e.g. temp solution to keep up momentum) what benefit it will give the organisation when dealt with (e.g. better resilience and CX - Custer eXperience).  Simples.\nNo man is an island Keeping on the theme of Tech Debt; things like this need to be discussed openly. More to the point, the inclusion of tech debt needs to be agreed AND it has to be captured. This approach should be driven by good engineering leadership.\nIdeas; thick and fast To work rapidly towards a solution, ideas need to be thick and fast. These will be dismissed and will help shape the eventual solution. This goes for both architecture and code solutionizing. The quicker the ideas are out there, the quicker these ideas can be critiqued. With these ideas being shared, the discussion that will enevitably ensue will always bring the team members together. Collaboration is your friend. The serendipity of this action will give more team members shared knowledge of this problem domain and will, in part, help in their personal development and education.\nDon\u0026rsquo;t be precious about your codebases This time next month, 6 months, a year, 2 years, etc.. it won\u0026rsquo;t look the same and most probably will not exists.\nWrite tests and move on.\nTDD This approach is always spoken of, especially in interviews or technical tests, but sadly not always adhered too in practice. In my experience, it always, ALWAYS results in succinct, readable and less verbose code. Oh yeah, with tests. Bingo!\nKick towards the end goal Quick releases will measure your success rate and offer the greatest amount of flexibility to react and adapt. So what do I mean to \u0026ldquo;react and adapt\u0026rdquo;. When you are developing a new features, you never, EVER release the all-encompassing, panacea. You devise a MVP (minimum viable product, or feature), to purely test your original hypothesis. This way, if no one needs it OR can\u0026rsquo;t use it, you can rapidly either (a) chalk it up to experience and move on OR (b) improve the UsereXperience. Ah yes, slow development and big bangs don\u0026rsquo;t help. Whether it\u0026rsquo;s a rewrite or a new feature, you won\u0026rsquo;t know whether you\u0026rsquo;re on the right track until it\u0026rsquo;s out there and, here\u0026rsquo;s the biggy, and being measured! Yeah, I went there! I mean, how else are you going to know whether your feature is working or being used right? This includes re-engineering technical problems too.\nIf developing features, have the PO provide test-cases There\u0026rsquo;s 2 types of development work. These are (a) Technical work which address engineering improvements and (b) Features. As engineers, we know how to test our sh*t, but not necessarily how best to test a new feature. This requires excellent product domain knowledge. So, an important part of the effort is to have your PO (Product Owner) provide the test-cases. This needs to be part of the Sprint.\nLet your architecture deal with resilience A definition of Resilience:\nResilience is the ability for your solution to continue to operate without issue, when faced with a transient failure.\r You\u0026rsquo;ll think you\u0026rsquo;re doing a good job by including a resilience framework to exponentially backoff or circuit break failed downstream dependencies. Unless this is done properly and is known by all dependents, you\u0026rsquo;ve creating a huge problem for others! This is one of the reasons I hate HTTP chaining. You are inadvertantly creating something that may cause DDoSing of your own infrastructure\u0026hellip;even with a jitter! There are better approaches to mitigate this - EDA.\nMore on this later.\nOffload cross-cutting concerns This is in keeping with the DRY principle. Heres a list of typical cross-cutting concerns you can achieve through the use of an API Gateway:\n authentication \u0026amp; authorization service discovery integration response caching retry policies, circuit breaker, and QoS (quality of service) Rate limiting and throttling Load balancing Logging, tracing, correlation Headers, query strings, and claim transformation IP whitelisting  What this gives you is a smaller attack surface areas as well as facade-esque capability to re-utilisize your services. I am not saying that this, or aggregation, are the best solutions, more that they are just solutions that may work for your particular problem domain. This by itself is NOT the entire solution but will help your architecture a better solution.\nMore recently, a large number of the above cross-cutting concerns can be accomplished using technologies like service meshes (e.g. istio). I digress \u0026hellip;\nTake authorization for instance. Implementing this repeatedly across all your services is simply a waste of time and effort. The best place to target this, is at the API Gateway. Done once, which means less code, less maintenance and one place to have your X.509 certificate. Imagine what a pain it will be having to redeploy a new X.509 certificate?! Mind you, not so much of a pain if you have your CI/CD pipelines configured!\nEDA between domains If you\u0026rsquo;ve ever done DDD (domain driven development) or even attempted to group your services whtin bounded contexts, you will still have found you need data from another domain. Think Orders and Logistics. These typically will sit behind different domains. There will be a time that these will need to be request information from one another. What about reporting, especially, executive level summary reporting? Each domain or even bounded context may/will have their own data repositories will will lead to some complicated and drawn out conversations when solutionizing! Let\u0026rsquo;s bounce back to requesting data. Typically, you\u0026rsquo;d use a RESTful approach to this or even gRPC - both using HTTP/2. Either way, hopefully you will have configured internal/private HTTP APIs, as need to make this public even if they\u0026rsquo;re only required by the backend services right? The thing about HTTP traffic is that, it offers a point of failure, notice I\u0026rsquo;m saying single point of failure\u0026hellip;.we wouldn\u0026rsquo;t do anything this silly would we?! For HTTP calls, we\u0026rsquo;d, again, typically have it wrapped up using some nice resilience framework yes? So, if the first attempt fails, we\u0026rsquo;d try again immediately then start to expotentially back off right? And for good measure, we\u0026rsquo;d have a jitter too yes? Well, if you have multiple clients doing this and the is several levels deep, then you could have a real problem on your hands if you have a failure that is is nested pretty deep. All, because a web form or similar is requiring data, causing a ripple affect across all your HTTP services!\nA better way to manage this is to have one BFF, that has access to all required to services that one web form request AND does NOT require need to violate cross-boundary calls. EDA (Event Driven Development) offers a solution to this.\nWhat the illustration below articulates is a way to have immediate across to data that is mutated in other domains, negating the need to having to request it each and everytime a web request is made:\nThe data repositories above, as used by the consumers, are all denormalized, configured specifically for reads instead of writes.\nOnly take the microservices route if this addresses the issue \u0026hellip; you\u0026rsquo;re trying to solve!\nFor everything we do, we need to justify it to someone above, whether they be senior management, stakeholders or even the board. If we don\u0026rsquo;t, it simiply won\u0026rsquo;t get done. There\u0026rsquo;s always some other priority that is more important right?! So, the best way to justify a piece of work, albeit dealing with Tech Debt, is you have it to back it by meaningful metrics. You can\u0026rsquo;t, and you shouldn\u0026rsquo;t, deem something worth doing because of your gut\u0026hellip;if we build it, they will come kind of thing which is a no-no.\nThis will help enormously by forcing you to identify the ACTUAL problem you\u0026rsquo;re trying to solve. Anyone who\u0026rsquo;s trying to move away from Microservices will atest to the fact that they are not always the way to go. Even if there\u0026rsquo;s a lot of evidence present to suggest this. Monolithics, I\u0026rsquo;ve been reading a lot about this reversal recently, in one form or another (single process, modular and distributed) might give you, dependant on your scenario, equal or even more that one the complexities and maintenance overhead that Microservices brings to the game! But, different horses for courses. Just don\u0026rsquo;t always be swayed by the new shiny. Serverless obviously is the exception to the rule ;O).\nAll I\u0026rsquo;m going to say in the section is, always address the ACTUAL problem, and then let the best solution lead the technology choice, not the other way around.\nCross-boundary requests metrics OMG, how easy is this??!! Next!\nThere\u0026rsquo;s no excuse for not knowing how perfomant your solution is, and where fault responsibilities are.\nIf you are calling across bounded context, notice I didn\u0026rsquo;t say domains here, or when calling out to external services, you MUST capture latencies and errors rates. I\u0026rsquo;m not focusing here on the other 2 pillars of observabililty (tracing \u0026amp; logging), although, all 3 pillars are super important.\nIf you want a positive Customer experience, you must be the first to know about an failure. Alerting will help here. Alerting will tell you when a latency is creeping up or worse, when errors have started to happen. Either way, you need a mechanism to give you the maximum about of time to address an issue if it arises. Most monitoring solutions come with alerts that allow you to notify people/teams. There are even feature that automatically assign the investigation to teams or specific indivuals. Slightly off topic here, but only just, error management is hugely important to your organisation. If used correctly, it can give you instant information, including all logging up to the actual error, even cross boundary in the error happened down stream in another internal service. It pays for itself and takes the guessing out of most of the debugging effort. This used to capture releases if powerful and again can help you pinpoint rapidly reasons for issues.\nDocument as you go, must not be an afterthought As we rapidly move from one task to the next, our memory of a prior piece of work will deminish. Along with this will be the bigger picture. I find documenting as you go is super useful. Any not jsut for you, for your organisation. I dreed to think how much time per year is spent on hunting down domain knowledge and reading through line at line of code. Not to mention, inadvertantly implementing something that does adhere to the big picture simply because you were not aware.\nWhat helps for me is, capturing thoughts and possble solutions. This leads to better articulation with your colleagues. It also helps explaain Tech Debt. How often have you had a \u0026ldquo;Why on earth did they do this?!\u0026rdquo; moment? One of my sayings is always respect the legacy as, more oftern than that, it was the right thing to do at the time\u0026hellip;you weren\u0026rsquo;t there mannnnn! Everyine can be be scinic after the fact! So, to stop you coming off as an idiot with your colleagues, document you though process, final solution, your rationale and any Tech Debt (see section above).\nNo initial design will ever play out I\u0026rsquo;ve been doing this for years and never has anything ever played out exactly as first envisaged. If anyone tells you different then they have never developed or architected anything, ever, EVER! It\u0026rsquo;s that simple. This leads on nicely with the next section\u0026hellip;\nIt\u0026rsquo;s 100% ok to change your mind It is 100% ok to change your mind. If you\u0026rsquo;re working for an organisation which makes this awkward or embarrasing then leave. This does not include banter. Banter is good. We must do our utmost to make the environment we work in, a safe one. One where it is absolutely o\u0026rsquo;kay to make mistakes in and one where you can change your made. These, simply put, are good engineering practices and good leadership. Don\u0026rsquo;t forget, if you\u0026rsquo;re changing your mind, it is for the good of the organisation and not for shits and giggles. If you make a mistake, and it\u0026rsquo;s a whopper, then this is the fault the leadership team and not you. Mistake don\u0026rsquo;t include deliberate and melcious actions. You do not stand by and let you colleagues make mistakes either. Mind you, if you did, people will have already the cut of your jib. Engineers, we\u0026rsquo;re generally the good guys and girls who are morally uncorruptable and know right from wrong AND will move heaven and earth to help our colleagues. Not sure you can say the same with other types of occupations. Respek!\nMonorepos for shared code - can still scale by deploying to separate docker containers I read a blog post recently where it said monorepos can\u0026rsquo;t scale\u0026hellip; \u0026ldquo;Calling Bullsh*t!\u0026rdquo;\nI\u0026rsquo;ve recently ventured into monorepos and the services I\u0026rsquo;ve developed, that relay on shared code, can absolutely be scaled. It just so happens there\u0026rsquo;s no need for them too, but there is a requirement for them to be HA (highly available) which isn\u0026rsquo;t the same thing but still requires at least one of an instance type to be running and to be able to recover gracefully if it does \u0026ldquo;sh*t the bed\u0026rdquo;. So for our requirement, a monorepo works just fine.\nI cover this in another post I\u0026rsquo;m in the process of writing and I will include the link here when published; promise!\nCode to interfaces for HTTP API calls One pattern that was shown to me not so long ago involved using a proxy as the gatekeeper to a HTTP API service. This client can be either server-side or front-end facing. The responsibility of this pattern is to guard against any breaking changing introduced unwittingly.\nSo, why is this important? Imagine you have an HTTP API. It has several controllers (endpoints), that are called from other HTTP APIs, services or the browser. Unless, your documentation is 100% up-to-date and you and, more importantly, your colleagues are ruthless in the pursuit of removing unaccessible code, you will end up with code that just isn\u0026rsquo;t used but will inevitably need to be maintained. Also, how often have you experienced your code, in production, just stop working? Only to find out that an endpoint signature has changed?!!!! WFT!!\nThis is where this pattern comes into its own. By shimming a service client in between the service and the client, you mitigate against any breaking changes.\nKeep code simple, doesn\u0026rsquo;t need to be clever. OMG! How much time is wasted reading OR debugging complex code?! Especially asynchronous code (thinking of .NET Core here). God forbid your attention being distracted for a nano-second and having to start over again! I am not saying there must not be any complex code, ever. Some software requires complex code but surely not a web application right?\nControllers (thin MVC) should be thin/light - biz logic in models I\u0026rsquo;ve seen my fair share of controllers that contains line after line after line of business logic. Business logic has no place in a controller.\nAnother, slightest of going off topic here, is UX principles. I\u0026rsquo;ve always advocated that, if a client (think brower) moves away from page having issues a request but yet in receipt of that request\u0026rsquo;s response, a pre-nav call should issue a cancel. You can do this easily with ASP.NET Core. This Cancellation token can be passed onto down and intervine in that executing request. It\u0026rsquo;s good practice but not often seen.\nAntoher advocated UX principle is to storie all form values in the browser\u0026rsquo;s local storage to safe guard against catastropic failure, which may stop the user from submitting their input to the host or service endpoint. Client side code can then remind the user of uncommit changes at a later date.\nOpen Source - the not so \u0026ldquo;promised land\u0026rdquo; This deserves its own blog post. For bravity, I\u0026rsquo;ll summary the main points. I apologise in advance if I leave out any specifics.\nI good portion of my development has been cloud based. It\u0026rsquo;s where I feel the most relaxed and confident. It\u0026rsquo;s also the source of my enjoyment. It often makes light of any nightmarish architectural constraints that are brough about by on-premise solutions.\nSeveral years ago the Teactive Manifesto came about. A bunch of us subscribed to thing philosphy. I think 12 Factor app popped up around the same time. I could be wrong though. Anyways, not important. One of the tenets of the Reactive Manifesto was message driven. Hugely relavant for cloud based development. It\u0026rsquo;s like the 101 of Cloud development; decoupling dependencies.\nAt this time, we had developed a HA solution. The problem with it though, was it wasn\u0026rsquo;t scalable. Services, HTTP APIs and the web application itself would also be used in a active/active or active/passive configuration. We had high hopes for our solution. I was tasked to create a scalable solution, one that could take full advantage of cloud infrastrutre. It eventually could, but the journey to get there was a bit of a rought one to say the least.\nAs intimated at the beginning of this section, I\u0026rsquo;m not going to be to forthcoming with the details. This is definitely something for a fuller post.\nOur installation procedure was all Powershell driven. We hadn\u0026rsquo;t entertained including other products to support our product. We just didn\u0026rsquo;t have the experience, the budget or confidence of doing this. After all, there were just 3 of us. So, I set about investigating a way to effectively create our own software cluster capability that could scale. This includes the automatic registering services that would come online to manage load. A well known Scala clsutering tech had been converted in part to .NET Framework. This seemed ideal. It was open source and lots positive references started to surface. One of the benefits was location tranparency. Location transparency is a term that used to identify network resoures by name and not by their actual location. So, from a coding perspective, you can reference something from configuration, that could be running on another server, remote from where the client is interfacing with it. I think this is cool.\nThe problem with it was that the clustering aspect of it took some considerable time to be ironed out. It also didn\u0026rsquo;t play nicely with IIS. There was also limited online documentation around the topic of clustering through this OSS. The author had a living to make and so all training was chargeable. We didn\u0026rsquo;t have a budget. Myself and my lead developer worked through the available free online training. This was extensive but it did take you down a path that would later prove difficult to back out of. In short, you change your development paradigm to making everything message driven, even to the point of how your classes methods interacted with one other. This was my 2nd mistake. The 1st mistake was to use something that wasn\u0026rsquo;t mature but more importantly, not fit for purpose for our requirements.\nAs a consequence, we changed a large portion of our codebases on the promise that clustering will just work. It didn\u0026rsquo;t and it wasn\u0026rsquo;t until later that we discovered it really didn\u0026rsquo;t like IIS. I was told that it wasn\u0026rsquo;t its fault, more of the fault of IIS. My 3rd mistake was believing that by myself I could get it to work properly. I couldn\u0026rsquo;t and sadly this took a long time to realize. This is one of those rare occasions when bloody-mindedness worked against me!\nDuring this time, I reimagined our CI/CD pipeline and started using Octopus Deploy to package up and deploy our HA Web product. This made it extremely easy to include and deploy supporting solutions that complimented our own. This presented us with the opportunity of using a replacement to this clustering library. This was NATS. I had used NATS several time before and felt comfortable and confident with it. NATS is awesome. The elation of finally moving away from this horror was indescribable.\nEventually, our CI/CD pipeline would be used to explain our processes to potential partners and those customers we supported directly. This gave them peace of mind, knowing how to develop our software, test it and deploy it. We also advocated the benefits of having multiple environments. There was not one partner/customer who did not take heed of this advice and extended their own established QA programme to include our product too.\nSo, key take aways here are:\n do not use OSS in the way that it dependency is critical to any of your components / systems only use OSS that is mature, do not entertain versions ~1.0! if you do not get rapid success through PoC, be unequivocal in your move to a different solution do not go all in on something new, introduce it slowly, with limited exposure and measure it\u0026rsquo;s performance (latency and error rates)  Splitting data out across microservice \u0026ldquo;What is meant to be kept together, should be kept together\u0026rdquo;\nI\u0026rsquo;ve had countless discussions with team members over the years on what a service (mainly SOA and Microservices) should include. More often than not, the outcome of these discussions have been, \u0026ldquo;let\u0026rsquo;s wait and see what the metrics tell us, then decide\u0026rdquo;. One of the principles of good engineering is not to prematurally optimise. This is especially poignant if you\u0026rsquo;re developing a new service and yet know if something will grow or be surplus to requirement. So why waste both your time or effort on effectively trying to answer a question that cannot be answered?\nThe more challenging conversation come about when services start to mature and you\u0026rsquo;ve a service that has a dependency on another. This is commonly referred to as a \u0026ldquo;decomposing the database\u0026rdquo;. There are several patterns you can adopt, and each have their place. If data that is required rarely changes, then simply derive the class from the data and embed this into the requesting service as a type collection (e.g. IEnumerable\u0026lt;{type}\u0026gt;). Life normally isn\u0026rsquo;t this simple and other solutions might include shared database or common sense will prevail and keep that data together in the same database. However, one of the benefits of EDA (Event Drive Development), is that you can have a local copy of this dependency data.\nObviously, this is a HUGE topic and I\u0026rsquo;ve massively stepped over the common courtesy line by writing the above throw away comments. But, to justify my rush through this section, I would say, do what\u0026rsquo;s right based on your collected metrics. There are plenty of patterns you can follow, here\u0026rsquo;s a few:\n The shared Database Database view Database Wrapping Services Database as a Service Interface Transferring Owership Data Synchronisation Synchronisation Data in Application Tracer Write Splitting Apart the Database Split Table Transactions Sagas Event Driven Development  I am of the opinion that not every bit of data has tpo be stored in a database. These next 2 example will give exampled of how I\u0026rsquo;ve employed this approach and why.\nMulti-tenant User hierarchy In my previous employment, we had a HA web application that had database resource intensive use-cases. These use-cases would run everytime a user or hierarchy was updated. The update might involve them moving or being assigned to a new position (or role) in the hierarchy, sometimes a user exist in multiple locations at a time. We called this capability the \u0026ldquo;User Hierarchy\u0026rdquo;. This was all served from MSSQL (then Azure SQL then eventually AWS Aurora). We had an HTTP API that people used (part of our SDK) to sync their product\u0026rsquo;s user hierarchy with ours. This too triggered these resource intensive use-cases. We had replaced the datbase library with ADO.NET to reduce operation latency but this, if you had multiple Tenants sync\u0026rsquo;ing or making modifications to users and or the hierarchy, then things from a performance perspective would get a little sluggish.\nIt was the regeneration of the hierarchy that was the cultpret. It would regenerate the user position using Left and Right boundary markers, to position a user\u0026rsquo;s position wihtin a team or unit (structural domain node types). They could also hold multiple roles, like Manager of a specific type of hierarchical node (e.g. unit manager, team manager). Another dimension to the complexity was our Scheduling feature. This would scheduled jobs to pull data from a Call Recorder\u0026rsquo;s database (line of sight) or API. This would use our internal entity IDs (think user, team, etc.) for mappings to their internal IDs. So, the schuduler would have to be updated each and every time too, otherwise hierarchical groupings wouldn\u0026rsquo;t line up (our Recorder queries had the capabiity to pull back a sample of recordings per hierarchical gorupings based on node types (Unit, Team). It was sophicated as well as being a well respected and used feature.\nHowever, the hierarchy was a constraint. It was a prime example of technical debt. It needed to be dealt with so space had to be found to deal with it.\nSo, we explored ways to remove the hierarchy from the database. What we did was to move the resting place of the hierarchy to disk, and when ever it was required (think UI or our Scheduler feature), we\u0026rsquo;d lazy load it into cache from disk. We eventually improved on the soution by using gRPC and protobuf to reduce the execution time to ~1sec. We also used both Azure Blog Storage and AWS S3 Buckets.\nThis approach of not storing missing critical data led to other uses in our products too.\nAggregation reporting ETL\nClient side validation should never substitude server side validation An oldie but goldie. I don\u0026rsquo;t think I need to add more here do I?\nReferences  direct client to microservices Vs API Gateway pattern gateway offloading Monolith to Microservices  "});index.add({'id':2,'href':'/posts/kubernetes-on-windows/','title':"Kubernetes on Windows",'content':"This post is a reminder to me of what needs to be installed in order for a pod, created from a local image, that is to be served up via a kubernetes cluster, to be run from your local development environment.\nWhat is Kubernetes and why is it so important? \u0026ldquo;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\u0026rdquo;\nSo why is Kubernetes important? \u0026ldquo;Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn’t it be easier if this behavior was handled by a system?\nThat’s how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system.\u0026rdquo;\nKubernetes is the community\u0026rsquo;s (has a much larger community than that of Swarm's community) choice of container orchestrators.\nSome important notices \r.header {\rcolor: white;\rfont-weight: bold;\rpadding-left: 10px;\rpadding-top: 5px;\rpadding-bottom: 5px;\rborder-top-right-radius: 5px;\rborder-top-left-radius: 5px;\rfont-size: smaller;\r}\r.info {\rbackground-color: #336699;\r}\r.warning {\rbackground-color: orange;\r}\r.error {\rbackground-color: red;\r}\r.note-panel {\rbackground-color: #c2f5f5;\rpadding: 10px;\rborder-radius: 5px;\r}\r.panel-info {\rborder: 1px solid #336699;\rbackground-color: rgba(51, 102, 153, 0.2);\r}\r.panel-warning {\rborder: 1px solid orange;\rbackground-color: rgba(255, 166, 0, 0.2);\r}\r.panel-error {\rborder: 1px solid red;\rbackground-color: rgba(255, 0, 0, 0.2);\r}\r.panel-header {\rborder-top-left-radius: 0px !important;\rborder-top-right-radius: 0px !important;\r}\r\r\r\rPermissions\r\r\r-- \r\r-- Permissions\nTo install kubectl and minikube you must start Powershell with Administrator permissions\n\r\r.header {\rcolor: white;\rfont-weight: bold;\rpadding-left: 10px;\rpadding-top: 5px;\rpadding-bottom: 5px;\rborder-top-right-radius: 5px;\rborder-top-left-radius: 5px;\rfont-size: smaller;\r}\r.info {\rbackground-color: #336699;\r}\r.warning {\rbackground-color: orange;\r}\r.error {\rbackground-color: red;\r}\r.note-panel {\rbackground-color: #c2f5f5;\rpadding: 10px;\rborder-radius: 5px;\r}\r.panel-info {\rborder: 1px solid #336699;\rbackground-color: rgba(51, 102, 153, 0.2);\r}\r.panel-warning {\rborder: 1px solid orange;\rbackground-color: rgba(255, 166, 0, 0.2);\r}\r.panel-error {\rborder: 1px solid red;\rbackground-color: rgba(255, 0, 0, 0.2);\r}\r.panel-header {\rborder-top-left-radius: 0px !important;\rborder-top-right-radius: 0px !important;\r}\r\r\r\rShell\r\r\r\r\r\r-- Shell\nThese settings will only viable for the current shell, if you need to run another shell, ensure the minikube docker-env commands in the Steps to take to configure your environment section are also executed in the new shell. As minikube is the tool that runs a local cluster in your development environment, we need to tell it to use it\u0026rsquo;s built-in docker daemon and have images pulled from there, and not from a container registry.\n\rHow do I install kubectl (and what the heck is it)? kubectl is a CLI (command line interface) tool for controlling Kubernetes clusters. You can use this tool to deploy applications, inspect and manage cluster resources and view logs.\nTo ease the installation process, use chocolatey to install kubernetes-cli, run:\nPS C:\\\u0026gt; choco install kubernetes-cli How do I install minikube (and what the heck is it)? Minikube implements a local Kubernetes cluster and is deemed the best tool for local Kubernetes application development.\nTo ease the installation process, use chocolatey to install minikube, run:\nPS C:\\\u0026gt; choco install minikube Before you start, you must ensure that you have a platform virtualisation system available. Platform virtualisation software provides the mechanism to run virtual machines and containers in isolation and exposes them to one or more networks. It is within a virtual machine that your Kubernetes cluster will run. Windows 10 comes with a virtualisation hypervisor feature called hyper-v. You need to ensure it is running first. To do this, run:\nPS C:\\\u0026gt; Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All PS C:\\\u0026gt; minikube start --driver hyperv * minikube v1.9.1 on Microsoft Windows 10 Enterprise 10.0.18363 Build 18363 * Using the hyperv driver based on user configuration * Downloading VM boot image ... \u0026gt; minikube-v1.9.0.iso.sha256: 65 B / 65 B [--------------] 100.00% ? p/s 0s  \u0026gt; minikube-v1.9.0.iso: 174.93 MiB / 174.93 MiB [ 100.00% 1.03 MiB p/s 2m51s * Starting control plane node m01 in cluster minikube * Creating hyperv VM (CPUs=2, Memory=6000MB, Disk=20000MB) ... * Preparing Kubernetes v1.18.0 on Docker 19.03.8 ... * Enabling addons: default-storageclass, storage-provisioner * Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; Steps to take to configure your environment To set up your minikube environment, run:\nPS C:\\\u0026gt; minikube docker-env $Env:DOCKER_TLS_VERIFY = \u0026#34;1\u0026#34; $Env:DOCKER_HOST = \u0026#34;tcp://192.168.75.126:2376\u0026#34; $Env:DOCKER_CERT_PATH = \u0026#34;C:\\Users\\garrard.kitchen\\.minikube\\certs\u0026#34; $Env:MINIKUBE_ACTIVE_DOCKERD = \u0026#34;minikube\u0026#34; # To point your shell to minikube\u0026#39;s docker-daemon, run: # \u0026amp; minikube -p minikube docker-env | Invoke-Expression To point your shell to minikube\u0026rsquo;s docker-daemon, run:\nPS C:\\\u0026gt; minikube docker-env | Invoke-Expression To get access to minikube\u0026rsquo;s dashboard, run:\nPS C:\\\u0026gt; minikube.exe dashboard * Verifying dashboard health ... * Launching proxy ... * Verifying proxy health ... * Opening http://127.0.0.1:54553/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser... Here\u0026rsquo;s some sample nodejs (server.js) code. It starts a server on port 8080:\nvar http = require(\u0026#39;http\u0026#39;); var handleRequest = function (request, response) { console.log(\u0026#39;Received request for URL: \u0026#39; + request.url); response.writeHead(200); response.end(\u0026#39;Hello World!\u0026#39;); }; console.log(\u0026#34;started\u0026#34;) var www = http.createServer(handleRequest); www.listen(8080); Here\u0026rsquo;s a Dockerfile for the above nodejs server. Please observe that it exposes port 8080. This ensures that network TCP traffic can be received by the container via port 8080.\nFROMnode:13.5.0EXPOSE8080COPY server.js .CMD [ \u0026#34;node\u0026#34;, \u0026#34;server.js\u0026#34; ]To build a image of the above Dockerfile, run:\nPS C:\\\u0026gt; docker build -t hello-world:1 . \r.header {\rcolor: white;\rfont-weight: bold;\rpadding-left: 10px;\rpadding-top: 5px;\rpadding-bottom: 5px;\rborder-top-right-radius: 5px;\rborder-top-left-radius: 5px;\rfont-size: smaller;\r}\r.info {\rbackground-color: #336699;\r}\r.warning {\rbackground-color: orange;\r}\r.error {\rbackground-color: red;\r}\r.note-panel {\rbackground-color: #c2f5f5;\rpadding: 10px;\rborder-radius: 5px;\r}\r.panel-info {\rborder: 1px solid #336699;\rbackground-color: rgba(51, 102, 153, 0.2);\r}\r.panel-warning {\rborder: 1px solid orange;\rbackground-color: rgba(255, 166, 0, 0.2);\r}\r.panel-error {\rborder: 1px solid red;\rbackground-color: rgba(255, 0, 0, 0.2);\r}\r.panel-header {\rborder-top-left-radius: 0px !important;\rborder-top-right-radius: 0px !important;\r}\r\r\r\rInclude a build tag\r\r\r\r\r\r-- Include a build tag\nYou must specify a version tag and it has to be something other than latest. Here, I have used 1. If you don\u0026rsquo;t follow these instructions, minikube will attempt to pull the image from a docker registry (normally DockerHub).\n\rTo check that the image exists in Minikube\u0026rsquo;s built-in Docker daemon, run:\nPS C:\\\u0026gt; minikube ssh $ docker images You should see something similar to this:\n$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/\u0026#39;) _ _ | |_ __ /\u0026#39; _ ` _ `\\| |/\u0026#39; _ `\\| || , \u0026lt; ( ) ( )| \u0026#39;_`\\ /\u0026#39;__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/\u0026#39;(_,__/\u0026#39;`\\____) $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world 1 55f40b7f5c32 13 days ago 660MB hello-world latest 50c4285f25a5 13 days ago 660MB nginx latest ed21b7a8aee9 2 weeks ago 127MB k8s.gcr.io/kube-proxy v1.18.0 43940c34f24f 3 weeks ago 117MB k8s.gcr.io/kube-scheduler v1.18.0 a31f78c7c8ce 3 weeks ago 95.3MB k8s.gcr.io/kube-apiserver v1.18.0 74060cea7f70 3 weeks ago 173MB k8s.gcr.io/kube-controller-manager v1.18.0 d3e55153f52f 3 weeks ago 162MB kubernetesui/dashboard v2.0.0-rc6 cdc71b5a8a0e 5 weeks ago 221MB k8s.gcr.io/pause 3.2 80d28bedfe5d 2 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 2 months ago 43.8MB kindest/kindnetd 0.5.3 aa67fec7d7ef 5 months ago 78.5MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 5 months ago 288MB kubernetesui/metrics-scraper v1.0.2 3b08661dc379 5 months ago 40.1MB gcr.io/k8s-minikube/storage-provisioner v1.8.1 4689081edb10 2 years ago 80.8MB To run this image as a pod, run:\nPS C:\\\u0026gt; kubectl run hello-world --image=hello-world:1 --port=8080 --image-pull-policy=never pod/hello-world created The --image-pull-policy=never is telling Kubectl to use the local image and not one from a container registry (Docker, ACR, ECR, GCP)\nTo expose this port for external access (from browser) from outside of the cluster, run:\nPS C:\\\u0026gt; kubectl expose pod hello-world --type=LoadBalancer service \u0026#34;hello-world\u0026#34; exposed To confirm your service is running and to get the port number of this exposed service, run:\nPS C:\\\u0026gt; kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-world LoadBalancer 10.111.126.10 \u0026lt;pending\u0026gt; 8080:31589/TCP 45h kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2d16h You will see the \u0026lt;pending\u0026gt; state of your LoadBalancer if you do not have not a Load Balancer integrated with your cluster. For your local development environment, it is nothing to worry about.\r\rYou will see that the hello-world service is accessible via port 8080. However, we still don\u0026rsquo;t know behind what IPv4 address, this services is available. To get the IPv4 address of your cluster, you type:\nPS C:\\\u0026gt; minikube ip 192.168.75.126 Finally, to access your service, run the cURL command, using the minikube ip address and the TCP port as listed in the kubectl get services output:\nPS C:\\\u0026gt; curl \u0026#34;http://192.168.75.126:31589\u0026#34; -UseBasicParsing StatusCode : 200 StatusDescription : OK Content : {72, 101, 108, 108...} RawContent : HTTP/1.1 200 OK Connection: keep-alive Transfer-Encoding: chunked Date: Mon, 06 Apr 2020 13:05:42 GMT Hello World! Headers : {[Connection, keep-alive], [Transfer-Encoding, chunked], [Date, Mon, 06 Apr 2020 13:05:42 GMT]} RawContentLength : 12 You can also use minikube to obtain your service\u0026rsquo;s url. To do this, run:\nPS C:\\\u0026gt; minikube service hello-world --url http://192.168.75.126:31589 Useful kubectl commands This first command is important. Some background first\u0026hellip;a context is a group of access parameters. Each context contains a Kubernetes cluster, a user, and a namespace. When you are working with multiple contexts off of your development machine, you may run into compatibility issues due to your client version not being compatible with the server API version. All kubectl commands will run against the current context. To check your client version, run:\nPS C:\\\u0026gt; kubectl version --client Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;18\u0026#34;, GitVersion:\u0026#34;v1.18.0\u0026#34;, GitCommit:\u0026#34;9e991415386e4cf155a24b1da15becaa390438d8\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-03-25T14:58:59Z\u0026#34;, GoVersion:\u0026#34;go1.13.8\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;windows/amd64\u0026#34;} To ascertain your current context, run:\nPS C:\\\u0026gt; kubectl config current-context minikube To list all of your configured contexts, run:\nPS C:\\\u0026gt; kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-desktop docker-desktop docker-desktop docker-for-desktop docker-desktop docker-desktop * minikube minikube minikube The * next to minikube indicates that minikube is your current context.\nIf you are configured to access a cluster hosted from a cloud provider such as Azure, then this context will also be listed.\nTo use a specific context, run:\nPS C:\\\u0026gt; kubectl config use-context docker-for-desktop References  Install Kubernetes Install Minikube Kubectl Cheatsheet Minikube\u0026rsquo;s built-in Docker daemon  "});index.add({'id':3,'href':'/posts/mentoring/','title':"How do I mentor?",'content':"I have written this post to document my experiences of mentoring. I have mentored front-end engineers, back-end engineers and UX designers. I have had the pleasure of helping others as well as learning one of two things about myself along this journey too. If ever you get the opportunity to be a mentor, I recommend you jump at the opportunity. It is a self-rewarding experience.\nSo, what is mentoring?\u0026hellip;\nThe definition of Mentoring is the act of advising or training (someone, especially a younger colleague).\nIn her book The Manager\u0026rsquo;s Path, Camille Fournier talks about Mentoring. She writes:\n\u0026quot;The first act of people management for many engineers is often unofficial.\u0026quot;\r This has always been the case for me too. I am currently employed as a Principal Engineer, before this, a CTO. In this time, I have neither organised nor carried out an official [backed by a recognised authority] mentoring scheme. It\u0026rsquo;s just been something that I do, without fuss but with purpose and pride.\nOddly, I have never been a mentee. If I had then there is a possibility that this in itself may have defined or partially influenced mentoring for me.\nThis is a list of scenarios where I have mentored others in:\n onboarding new company starters, onboarding a new colleague at a similar level as myself onboarding a graduate (their first job since graduating from university) when working on a project together  Concerning the above mentioned scenarios, I have both created and coordinated an onboarding programme. This was when I was a CTO. All this was choreographed remotely. Ironically, this is more relevant today than ever. As I write this CV-19 has started to take a grip of the UK and yesterday I heard of the sad news that 2 people had died from it in Southport where I have resided since 2008.\nThis is a bullet list of key \u0026lsquo;things\u0026rsquo; that I have discovered that have helped me through the mentoring process:\n communicate what the process of mentoring is to the mentee first, listen, then respond. Don\u0026rsquo;t attempt to expedite the process, don\u0026rsquo;t forget, it\u0026rsquo;s for them, not you! take the time to explain the rationale for a decision take the time to explain why something is not applicable in that particular instance try not to provide answers, but provide strategies (alternatives, is there an easier to do the same, what is the problem we\u0026rsquo;re trying to solve) allow for mistakes to be made and always follow them up with a post mortem. We all make mistakes, in some cases, it helps define you. Making a mistake is critical to our development so this is why the next point is important\u0026hellip; ensure you make a safe environment for your mentee to operate in make time but be clear about the amount of time you can give. You will have other responsibilities. Inadvertently, you are forcing the mentee to make decisions. This often encourages the mentee and gives them the confidence to stand on their own two feet. This too is critical for their development work on a real project, albeit, scaled back for safety and to limit the blast radius. It has to be something that matters to the business. This will help the mentee be recognized by their good work. By limiting the hypotheticals, the mentee will then get their hands on a non-fabricated, warts and all, real-life engineering problem to help in the preparation of an important [to them] event - this has meant helping produce the materials for an event as well as assessing and providing feedback develop a personal development plan - used to help keep focus as well as a comparator. This can take up a chunk of time but well worth it plus you\u0026rsquo;re holding yourself accountable to the process too!  As CTO I led both the architectural and the planned engineering effort that has been key to the strategic direction of that business. Mentoring was an important part of this process and as such, I was always in mentoring mode. To this day, no longer a CTO but still in a senior engineering position, I constantly think about, and act on, ways to help those around me to improve their engineering capabilities (think good engineering principles).\nAlthough not all of my mentoring is official, I do conduct myself in such a way that it benefits those around me. I do this by encouraging my co-workers whenever possible. Here is a list of how I have been able with success, help my co-workers:\n I demonstrate, then I include a co-worker in this process. An example of this is by whiteboarding a problem or solution. I hand over the marker and this leads to them articulating their solution in front of an audience I instigate a technical discussion or articulate an engineering problem. I solicited input from all (introverts and extroverts alike). This encourages my wo-workers to speak up and gain confidence in discussing technical issues in front of an audience I am consistent in the message of working in a safe environment, one where any question can be asked and any view given I define a piece of work\u0026rsquo;s guiding principles upfront. This helps in several ways. It defined the focus of the project, what to exclude etc. It also helps shape our collective thinking and finally, it\u0026rsquo;s a gentle way into a project instead of a rushing headlong into it without giving it any due diligence Redirect to good engineering principles whenever possible to enforce our foundation of good engineering.  I am a Principal Engineers and as such, I have a responsibility to my co-workers and the business to conduct myself in a way befitting a Principal Engineer. Quite simply put, one of the objectives is to help my co-workers in whatever way possible. This can be helping them out on a project. It can be providing feedback on a piece of work or technique. Ultimately, my goals are to be supportive, helpful, insightful, encouraging, guiding, a sounding board and inspirational. All executed respectfully. The people I have worked with and those who I currently work with are important to me. Anything I can do to help, I do. Even if it\u0026rsquo;s listening to them sound off. Returning to my goals\u0026hellip;I do see some of these being reflected at me but more importantly, I see the product of my mentoring too, which I find extremely satisfying!\nOne of the most humbling times of my life was when I mentored a colleague who, through no fault of his own, was temporarily let go from the company I was a CTO for. We as a company were struggling financially and had to slim down the workforce. It was a sh*t time. It was important to me though from a personal perspective that I didn\u0026rsquo;t just sever contact with him. The plan was always to bring him back onboard once things improved. And they did. But during the time that it wasn\u0026rsquo;t so great, I would meet-up regularly with him online - he was based in another country. We would discuss many topics; life, technology \u0026amp; side projects. Where I could, I\u0026rsquo;d provide guidance and be a sounding board for him. From time to time I would plan things for him to do. The next time we met up, I\u0026rsquo;d review what he had done and provide feedback when necessary. I would like to think that this created a bond between us. Like I say, it was all very humbling as after all, I was still in gameful employment. At some level, it must have been a bitter pill for him to swallow and he never held it against me, which is demonstrative of his good character. We no longer work together but he remains a friend and we do still often catch up online.\nOutcomes from the mentoring process can also be subtle. Just to be clear, it\u0026rsquo;s not always explosive or awe-inspiring either. It is what it is and a poor result does not equate to a lack of mentor\u0026rsquo;s ability. Generally, poor results are rare. In the one case where I observed poor results, I reported it upwards. The vertical market we were operating in didn\u0026rsquo;t float this particular mentee\u0026rsquo;s boat. It happens! Also, in my experience, it is always noticeable over time; providing you take a documented snapshot before and after. One source of personal satisfaction is seeing mentees, new and old, interacting with seasoned engineers, observing them standing on their own two feet, adding value to a conversation and project work alike. Best of all, seeing a seasoned engineer asking a mentee for their advice and input on a scenario. That my friends, is extremely satisfying!\n Written mainly for me, I do hope you\u0026rsquo;ve found something useful here and who knows, it might even help you with your mentoring journey too.\n "});index.add({'id':4,'href':'/posts/principles/','title':"Good Engineering - Principles",'content':"I have written this post as a method to document what I see as the basics, foundations if you will, for good engineering. Undoubtedly if you are a seasoned engineer, you will recognised all of these principles, less so, if you\u0026rsquo;re just starting out.\nMost Engineers are fully versed in the foundations of writing quality, efficient, succinct and testable code. As a Principal Engineer, one of my responsibilities is to ensure that these (1) foundations are recognised by the engineers and (2) are adhered to by all engineers.\nHere\u0026rsquo;s a list of concepts that for me, constitute good engineering principles:\nThese are in alphabetical order and not in order of importance\n Clean and readable code Code reviews Coding standards Composition over inheritance Defensive coding Do no more DRY KISS Occam\u0026rsquo;s razor Premature optimization Refactor Separation of Concerns SOLID Testing YAGNI  Other sections:\n My pattern discovery Being a Principal Engineer Discussion point References  Clean and readable code Clean and readable code is always better than clever code (ask any engineer who has to extend or maintain a clever piece of code!)\nI\u0026rsquo;ve seen a lot of code recently that should never have got to the shape it has. Complicated code requires time to understand, then time to add functionality. Complicated code also happens to more difficult to recall so each time you need to go near it, you have to relearn it and added to this, any changes made to improve it, most likely have not been applied in full so they\u0026rsquo;ll be a right old mixture of good, bad and the ugly thrown into the mix.\nA good measure of how bad a codebases is, and I\u0026rsquo;m going to plagiarise somebody else\u0026rsquo;s analogy here, is by stepping through an interactive debug session. If you get momentarily distracted by a fly, then immediately return to the debugging and you do not know where the feck you are in the execution of the code flow, then it\u0026rsquo;s a bad codebase!\nIt\u0026rsquo;s the responsibility of a Tech Lead or architecture to stop code bases ending up this way.\nCode reviews It should only contain helpful and constructive comments and/or implementation questions. This process is not there to caress egos (that\u0026rsquo;s for your mother to do!!). One useful by-product of code reviews is conveying of your team\u0026rsquo;s exacting coding standard and attention to deal, to new starters. So, the quicker the new starter pushes a commit, the better!\nCoding standards (provide a template of core standards then stand back and let the team thrash out the rest - wear protection!)\nAlthough important, it\u0026rsquo;s not the end of the world if some of the granular details differ between teams. The important thing here, in my opinion, is that each team know where to find their cheese. Most engineers in a team have a common set of standards they adhere too. The big things like solution structure, naming conventions, testing (AAA, GWT), pluralization, documentation structure (including README) all need to be consistent.\nComposition over inheritance (avoid class tree exploitation! - think Strategy pattern - GoF)\nThe above-bracketed statement says it all! Inheritance tends to back you into a corner especially when you consider the OCP.\nDefensive coding (guard against invalid class method parameters and accidental null assignment to class properties instead of an equality condition!)\nThis is one example of defensive coding:\nclass User(string firstnaame, string lastname, int age) { if (null == firstname) { throw new NullReferenceException(\u0026#34;Firstname cannot be null\u0026#34;) } ... The above demonstrates an example of defensive coding. The first is that we need to test for valid constructor parameter values when instantiating a class.\nThe second, is to avoid mistakes that might not be picked up by your compiler. For instance, a common mistake doing this:\nif (firstname = null) A .NET Compiler is more than happy allowing this above syntax, as, after all, it\u0026rsquo;s an assignment operator and not a equality operator as in above in the class constructor. By switching these around, you\u0026rsquo;re making a positive pattern changing and should avoid making this silly mistake again.\nDo no more (and do no less - thank you eXtreme Programming!).\nIf you code outside the scope, you\u0026rsquo;re in danger of creating code that isn\u0026rsquo;t used or needed. The worse thing about this is that others will have to maintain this code. How can this be? Well, it\u0026rsquo;s common - think HTTP chaining - for code not to be culled especially if there is a disconnect between these dependencies and there\u0026rsquo;s no IDE/compiler to shout at you.\nDRY (don\u0026rsquo;t repeat yourself)\nCode analysis tools help here, but you\u0026rsquo;re not always going to have access to these tools.\nOne way to help identify code that does the same thing is by refactoring. If you keep your code method frame small (~20 lines), and you have a good naming standard for methods (e.g. noun+verb with accurate alighment to business capability - think DDD), have unit tests with a high code coverage percentage, then this should be all you need to help you avoid writing duplicate code.\nKISS (keep it simple, silly)\nThis to a certain extent, goes hand in hand with avoiding premature optimization. We all like the big picture yes? This doesn\u0026rsquo;t mean we need to do deliver on this it right now! You just need to know the boundaries of this piece, which, if greenfield, then you won\u0026rsquo;t have any metrics to tell you the actual demand. Think Capacity planning; what this piece of work needs to do based on current expectations. For example\nDo we need multiple servers? Yes, I think Why do we need multiple servers? Mmmmm, because I read it somewhere\rDo you have the metrics that support your argument for multiple servers? Wait, what?\rNext!\r A colleague recently shared with me the architecture of their side project. They are using AWS and I have 2 certifications in AWS (Developer and Solutions Architect). I quickly went into HA/scaling/resilience/durability/DR overdrive, following it up with a verbal dump on what tech they should use. This was all wrong. They did not know their service demand. Following my initial advice, will have increased their cost; unnecessarily. I did, you\u0026rsquo;ll be glad to hear, re-affirm their decision (may have made 1 or 2 helpful suggestions) shortly after [~2 hours].\nYeah, think big but don\u0026rsquo;t deliver big without a customer base; as this, in my experience, will result in a huge waste of time, effort and money. Plus, sometimes, you don\u0026rsquo;t really know where something is going to take you, and my advice here is to roll with it. This last piece of advice is particularly pertinent if you\u0026rsquo;re starting up.\nOccam\u0026rsquo;s Razor This is a problem-solving principle.\nThe definition of this is: \u0026ldquo;Entities should not be multiplied without necessity\u0026rdquo;. It is sometimes paraphrased by a statement like \u0026ldquo;the simplest solution is most likely the right one.\nOccam\u0026rsquo;s razor says that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions. Good advice\nSuppose there are two competing theories on why something is not working. Normally, the case that requires the least amount of assumptions is correct. So, the more assumptions you have to make means it more likely to be more unlikely.\nPremature optimization Avoid premature optimization and all conversations relating to optimization until you know the facts. This will be futile until you\u0026rsquo;ve metrics to better inform you.\nI\u0026rsquo;ve hit this numerous times when planning for microservices and bounded contexts, in particular, on green-field projects. What should we include and where? Should we separate claims away from users for instance? Will the demand for Claims be greater than for users? Who knows?! You don\u0026rsquo;t until you have some metrics behind you. You can always merge or break them [microservices] up later.\nAnother area that I believe this encompasses is splitting code up across multiple files and folders. If it\u0026rsquo;s a PoC, a sample piece of code, or something that has a short shelf life, just keep it in one file. When it\u0026rsquo;s the right time - moving out of PoC/other - then you can consider optimizing it. Up until then, it\u0026rsquo;s a huge waste of time and effort.\nArchitecture is a great example of when not to prematurely optimize. Architecture normally infers cost. Generally, the more of something, the greater the cost. This could mean for a startup the difference between survival and their demise. Adopting a guiding principle of being frugal from the outset, is a prudent and wise decision. What this means is that you\u0026rsquo;re always looking for the most cost-effective way of accomplishing your goal. So, if you don\u0026rsquo;t know your demand, it means you opt for a single server instead of having a HA cluster of 3 master nodes and 5 worker nodes! Down from 8 servers to 1 which on a month by month basis during development and beta/early releases could mean the saving of thousands of pounds sterling.\nSadly, I\u0026rsquo;ve come across a few startup that have failed just because they ran out of cash early on. It\u0026rsquo;s a real shame for all involved.\nRefactor \u0026hellip;refactor refactor\nDon\u0026rsquo;t save this until the end of a piece of work \u0026hellip; you\u0026rsquo;re bound to miss something and possibly add to your team\u0026rsquo;s tech debt. Plus, if you push your commits to a PR, you\u0026rsquo;ll get your ass handed to you by your peers!\nThings to consider here are DRY and TDD. Both will nudge you towards a proper refactoring effort.\nSeparation of Concerns (think MVC, CQRS, bounded context, etc\u0026hellip;)\nIt\u0026rsquo;s all about doing the right this in the right place! I recently ran, architected and co-developed a project that involved our own hosted solution, a solution hosted on Azure and a solution hosted on the Twilio Cloud (Twilio Serverless Functions). Originally, the requirements did not include the Twilio Cloud and would have required a bucket load more effort if we\u0026rsquo;d stuck with that brief. Thankfully, I chose to take full advantage of what Twilio has to offer and used a combination of Twilio Flow and Twilio Serverless Functions. By establishing these SoCs it meant:\n a less stressful implementation a light touch to our own hosted solutions a satisfying amount of fun working with Serverless (has been my favourite and advocated approach for several years!) a time saving it revealed a range of options when dealing with specific edge and corner cases which, again, giving us a further time savings.  SOLID These are the SOLID principles:\n Single Responsibility Principle Open Closed Principle Liskov Principle Interface Segregation Principle Dependency Inversion Principle  Single Responsibility Principle A class (no method) should have one and only one reason to change, meaning that a class (or method) should have only one job.\n\u0026ldquo;When a class has more than responsibility, there are also more reasons to change that class\u0026rdquo;\nHere\u0026rsquo;s an example of a class )purposefully awful for illustrative purposes):\nclass User() { public string Username {get; set;} public string Fullname {get; set;} private readonly ILogger _logger; private IDbContext _db; public User() { _logger = new Logger() _db = new UserContext(); } public Task\u0026lt;User\u0026gt; GetProfile(string username) { ... _logger.Info($\u0026#34;Found profie for {username}\u0026#34;) return this; } } You could say that the above includes both a model responsibility and a service responsibility. These should be split into two separate .NET types, as in this example:\nclass User() { public string Username {get; set;} public string Fullname {get; set;} public User(string username, string Fullname) { ... } } class UserService() { private readonly ILogger _logger; public UserService(ILogger _logger, IDbContext db) { _logger = _logger _db = db; } public Task\u0026lt;User\u0026gt; GetProfile(string username) { ... _logger.Info($\u0026#34;Found profie for {username}\u0026#34;) return user; } } Here are the benefits of principles:\n Reduces complexity in your code Increases readability, extensibility, and maintenance of your code Reusability and bug breading Easier to test Reduces coupling by removing dependency between methods  Open Closed Principle Objects or entities should be open for extension, but closed for modification. So, what does this mean? Let\u0026rsquo;s break this down to two statements:\n Open for extension Closed for modification  Open for extension: This means that we need to design our classes in such a way that it\u0026rsquo;s new responsibilities or functionalities should be added easily when new requirements come.\nOne technique for implementing new functionality is by creating new derived classes. A derived class will inherit from base class. Another approach is to allow the \u0026lsquo;client\u0026rsquo; to access the original class with an abstract interface. I sometimes think of this simply as removing if statements by extension but I\u0026rsquo;m not convinced everybody would agree with this assessment though.\nSo, in short, if there\u0026rsquo;s an amendment or any new features required, instead of touching the existing functionality, it is better to create new derived class and leave the original class implementation. Well, that\u0026rsquo;s the advice! I worry about the class explosion and if you\u0026rsquo;re attempting to do this on top of not so perfect code!\nClosed modification: This is very easy to explain\u0026hellip;only make modifications to code if there\u0026rsquo;s a bug.\nThis sample looks at delegating method logic to derived classes.\npublic class Order { public double GetOrderDiscount(double price, ProductType productType) { double newPrice = 0; if (productType == ProductType.Food) { newPrice = price - 0.1; } else if (productType == ProductType.Hardware) { newPrice = price - 0.5; } return newPrice; } } public enum ProductType { Food, Hardward } Can rewrite, still using base implementation (think decorator pattern):\npublic class Order { public virtual double GetOrderDiscount(double price) { return price; } } public class FoodOrder : Order { public override double GetOrderDiscount(double price) { return base.GetOrderDiscount(price) - 0.1; } } public class HardwareOrder : Order { public override double GetOrderDiscount(double price) { return base.GetOrderDiscount(price) - 0.5; } } Liskov Principle Definition: \u0026ldquo;Let q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T.\u0026rdquo; \u0026hellip; clear as mud right?\nAll this is stating is that every subclass/derived class should be substitutable for their base/parent class.\nThe example below demonstrates a violation of the Liskov principle, as by replacing the parent class (SumEvenNumbersOnly-\u0026gt;Calculator), this does compromise the integrity of the derived class as the higher-order class is not replaced by the derived class. Here, both cal and eventsOnly variables will be the same:\n... var nums = new int[] {1, 2, 3, 4, 5, 6, 7}; Calculator cal = new Calculator(nums); Calculator evensOnly = new SumEvenNumbersOnly(nums); ... public class Calculator { protected readonly int[] _numbers; public Calculator(int[] numbers) { _numbers = numbers; } public int Sum() =\u0026gt; _numbers.Sum(); } public class SumEvenNumbersOnly : Calculator { public SumEvenNumbersOnly(int[] numbers) : base(numbers) { } public new int Sum() =\u0026gt; _numbers.Where(x=\u0026gt;x % 2 == 0).Sum(); } Here we have changed the assumed base class to an abstract class. Now, it can\u0026rsquo;t be instantiated and instead, must be inherited. This ensures the derived classes must implement the method detail. So, even if we replace the type declaration with the higher-order class, we should still get the intended result:\n... var nums = new int[] {1, 2, 3, 4, 5, 6, 7}; Calculator cal = new SumAllNumbersOnly(nums); Calculator evensOnly = new SumEvenNumbersOnly(nums); ... public abstract class Calculator { protected IEnumerable\u0026lt;int\u0026gt; _num; protected Calculator(IEnumerable\u0026lt;int\u0026gt; num) { _num = num; } public abstract int Sum(); } public class SumAllNumbersOnly : Calculator { public SumAllNumbersOnly(IEnumerable\u0026lt;int\u0026gt; num) : base(num) { } public override int Sum() =\u0026gt; _num.Sum(); } public class SumEvenNumbersOnly : Calculator { public SumEvenNumbersOnly(IEnumerable\u0026lt;int\u0026gt; num) : base(num) { } public override int Sum() =\u0026gt; _num.Where(x =\u0026gt; x % 2 == 0).Sum(); } Interface Segregation Principle A client should never be forced to implement an interface that it doesn\u0026rsquo;t use or clients shouldn\u0026rsquo;t be forced to depend on methods they do not use.\nTake the following interface:\npublic interface IAllTheThings { Task\u0026lt;IAsyncEnumerable\u0026lt;Claim\u0026gt;\u0026gt; GetClaims(string username); Task\u0026lt;IAsyncEnumerable\u0026lt;User\u0026gt;\u0026gt; GetUsers(string team); Task\u0026lt;User\u0026gt; AddUsers(User user); } There\u0026rsquo;s a clear distinction in responsibilities that are being suggested here by the contract name. Sufficed to say, these should be split across different interfaces:\npublic interface IUser { Task\u0026lt;IAsyncEnumerable\u0026lt;User\u0026gt;\u0026gt; GetUsers(string team); Task\u0026lt;User\u0026gt; AddUsers(User user); } public interface IClaim { Task\u0026lt;IAsyncEnumerable\u0026lt;Claim\u0026gt;\u0026gt; GetClaims(string username); } Dependency Inversion Principle There are 2 rules here:\n High-level modules should not depend on lower-level modules. Both should depend on abstractions. Abstractions should not depend upon details. Details should depend upon abstractions.  Let\u0026rsquo;s deal with the first rule first. High-level means policy, business logic and the bigger picture. Lower-level means, closer to the bare metal (think I/O, networking, Db, storage, UI, etc\u0026hellip;). Lower-level tend to change more frequently too.\nThese two examples show perfectly the before and after of the move to a \u0026lsquo;depend on abstraction\u0026rsquo;:\npublic class BusinessRule { private DbContext _context; public BusinessRule() { _context = new DbContext(); } public Rule GetRule(string ruleName) { _context.GetRuleByName(ruleName); } } public class DbContext { public DbContext() { } public Rule GetRuleByName(string name) { return new Rule(new {Name = \u0026#34;Allow All The Things\u0026#34;, Allow = false}) } } After changing to an abstraction:\npublic interface IDbContext { Rule GetRuleByName(string name); } public class BusinessRule { private IDbContext _context; public BusinessRule(IDbContext context) { _context = context; } public Rule GetRule(string ruleName) { _context.GetRuleByName(ruleName); } } public class DbContext : IDbContext { public DbContext() { } public Rule GetRuleByName(string name) { return new Rule(new {Name = \u0026#34;Allow All The Things\u0026#34;, Allow = false}) } } With the above change, the DbContext can be any class as long as it inherits from the IDbContext interface and has a method with a signature of Rule GetRuleByName(string name).\nThe above is demonstrative of the 2nd rule; do not depend on the detail. As you can see, in the example above, we\u0026rsquo;re depending on an interface method contract and the actual implementational detail is being dealt with by the Lower-level class.\nThe above example includes Dependency Injection. Although you can accomplish IoC with DI, they are not the same thing. IoC does not mention anything about the direction of the dependency.\nGeneralization restrictions The presence of interfaces to accomplish the Dependency Inversion Pattern (DIP) has other design implications in an object-oriented program:\n All member variables in a class must be interfaces or abstracts All concrete class packages must connect only through interface or abstract class packages No class should derive from a concrete class No method should override an implemented method All variable instantiation requires the implementation of a creational pattern such as the factory method or the factory pattern, or the use of a dependency-injection framework.  Testing (unit/functional, including concepts like TDD \u0026amp; BDD and frameworks)\nFor testing to be a success, the details are key. These details will come in the form of a specification or from a verbal conversation (always to be confirm in writing later). If you\u0026rsquo;re lucky, these test cases will be included as ACs (Acceptance Criteria) in the Scrum Story Description.\nTaking a test driven development approach to writing code often results in:\n a reduction in verbose code less post-deployment bug fixing succinct (do no more, no less than is required), structure and logic.  Testing is important. Obviously! I often refer to testing as \u0026lsquo;having your back\u0026rsquo;. It ensures you don\u0026rsquo;t break existing functionality when implementing new functionality or dealing with tech debt. It also protects new engineers from breaking things as well as extant engineers who may have touched this repository many times before.\nTests aren\u0026rsquo;t just for new functionality either. If you change extant functionality or class responsibilities you must modify extant tests or create new tests. Ideally, your CI build pipeline should run tests every time time a commit(s) is pushed to a PR or Draft PR. This last step is here, to again, have your back and to safeguard against erroneous code poluting your codebases and getting into production.\nIn the .NET world, there are many testing frameworks available; xUnit, NUnit, MSTest to name a few. There are also many mocking frameworks available; Moq, NSubstitute, JustMock, again, to name a few. Frameworks like these help make the testing process and overall experience less painful and cumbersome and some might even say it makes this part of development, pleasurable!\nMy .NET Core testing and mocking preferences are xUnit \u0026amp; Moq and my javascript (including node.js) testing framework preference is Jest.\nThis code sample shows how both a testing and mocking frameworks compliment each other:\nusing Moq; using Xunit; namespace BasicAAATestExample { public interface IUser { string GetFullname(); string Firstname { get; set; } string Lastname { get; set; } } public class User : IUser { public string Firstname { get; set; } public string Lastname { get; set; } public string GetFullname() { return $\u0026#34;{Firstname} {Lastname}\u0026#34;; } } public class Notify { private IUser _user; public Notify(IUser user) =\u0026gt; _user = user; public string GetMessage() =\u0026gt; $\u0026#34;{_user.GetFullname()} has been notified\u0026#34;; } public class NotifyTests { [Theory] [InlineData(\u0026#34;Garrard\u0026#34;, \u0026#34;Kitchen\u0026#34;, \u0026#34;Garrard Kitchen has been notified\u0026#34;)] [InlineData(\u0026#34;Charles\u0026#34;, \u0026#34;Kitchen\u0026#34;, \u0026#34;Charles Kitchen has been notified\u0026#34;)] public void GivenGetMessageIsCalled_WhenFirstAndLastNameExist_ThenReturnsANotificationMessage(string firstname, string lastname, string expected) { // arrange  var mockUser = new Mock\u0026lt;IUser\u0026gt;(); mockUser.Setup(x =\u0026gt; x.GetFullname()).Returns($\u0026#34;{firstname} {lastname}\u0026#34;); var sut = new Notify(mockUser.Object); // act  string message = sut.GetMessage(); // assert  Assert.Equal(expected, message); mockUser.Verify(x =\u0026gt; x.GetFullname(), Times.Once); } } } The single unit test above follows the AAA (Arrange, Act, Assert) pattern and is a common way of writing unit tests for a method under test:\n the Arrange section of a unit test method initializes objects and sets the value of the data that is passed to the method under test the Act section invokes the method under test with the arranged parameters the Assert section verifies that the action of the method under test behaves as expected.  There are a few standards I adhere to when it comes to writing tests. In the sample unit test above these standards include:\n the method name (GWT) the comment blocks of arrange, act and assert the name of the mock instantiated object (mock\u0026lt;Class\u0026gt;) the class name of the SUT - system under test - (sut).  YAGNI (you ain\u0026rsquo;t going to need it)\nDo no more, and no less than is required. You do not want to have to maintain code that is never used or produce code that others have to maintain unwittingly. It\u0026rsquo;s very difficult to future proof your code if you do not know what\u0026rsquo;s going to happen, let alone without a specification! It\u0026rsquo;s a guess at best so don\u0026rsquo;t waste your time or others. Keeps things concise, succinct and simple.\n My pattern discovery I\u0026rsquo;m a huge fan of patterns, especially cloud architectural patterns but sometimes, they add unnecessary complicity so beware!\nWhen I first started learning about patterns - some 18 years ago - I went through a few codebases I was involved with at the time to see if I\u0026rsquo;d subconsciously been repeatedly using a pattern \u0026hellip; and I had! It was the lazy loading pattern\u0026hellip;which I continue to use regularly today!\nBeing a Principal Engineer As a Principal Engineer, I consider the above as the foundation for writing quality code. The objective of this list, in conjunction with the message I propagate via this list, during discussions, evidence from my own work and by leading from the front within my role, is one of a reminder to me and my colleagues of best practice and commitment to quality and good practice. As with all foundations, it forms the base from which more advanced concepts or approaches can be learned. An important part of this practice is heuristic - enabling a person to discover or learn something by themselves. So, how do I go about doing this?\nThese are some of the activities I execute to embed good engineering principles:\n 1-2-1 Group conversations Advocate online learning platforms such as Pluralsight or Udemy. For the more keen Engineer, I also recommend certification. YouTube is another favourite of mine. With YouTube, you can tag recordings, therefore building up a catalogue of materials that you can make public Workshops Brown bags Capture How To Do\u0026rsquo;s in wikis or similar Coding advice/tips (e.g. when to use Task instead of an Async method) Take the time to explain questions about implementation reasons in DVCS Pull Requests Share blog posts \u0026amp; other materials across multiple channels Compile a learning profile for an individual  The coding advice/tips above are interesting ones. As professionals, we always want to improve our ability to code, how we approach problems, etc\u0026hellip;, and in doing so we want our colleagues to benefit from our experience. I recently became reacquainted with coding katas. As a black belt in Ju-Jitsu I am well versed in what a kata is. Katas can also be used to remind, stretch and improve our core coding capability. The last time I used a kata in programming was 10+ years ago. This was when I was first introduced to TDD. A favourite development book of mine is \u0026lsquo;The Art of Unit Testing\u0026rsquo; by Roy Osherove. It is the first edition. For many years I had it as a click-thru purchase option on a previous blog site of mine. I\u0026rsquo;ve not really participated in many katas since. I have written a few though and now having been reintroduced to them and reminded of their potential, as a Principal Engineer I can see it as an invaluable tool. One thought I\u0026rsquo;ve had is to use it as a framework to assess an Engineer\u0026rsquo;s current capability and then use during pair programming to help share coding techniques, approaches and standards.\nPair programming is a wonderful technique for propagating developer skills (think how to use cloud services), approaches to coding (think TDD and problem solving), embed team coding standards and code review in realtime. Pair Programming is an Extreme Programming technique. It is not a mentoring or coaching technique but some do use it for this. Quite often, I find I only participate in pair programming is one of two use cases. (1) if the subject I\u0026rsquo;m investigating is new (important to have shared knowledge) and (2) when I\u0026rsquo;m helping an Engineer to overcome an esoteric issue. You know what they say? \u0026hellip;a problem shared is a problem halved! However, now, I\u0026rsquo;ll be including Pair Programming in conjunction with katas as part of my techniques to stretch the Engineer\u0026rsquo;s muscle memory (including mine!).\nI love hacking away at code as much as the next Engineer. Hacking code is a great way to experiment and to get out of the starting gate. However, when it comes to pair programming I do like to give it the necessary due diligence. By this I am referring to allowing for a few moments up front to deliberate and agree on what\u0026rsquo;s required. This checklist guides me and ensures up front I set off in the right frame of mind:\n the objective of the pair programming exercise (think the desired outcome - you might even want to frame this with a Story Description; AS A, I WANT, SO THAT) what libraries (3rd party) will we need (think cloud provider SDKs and vendor client APIs) how are we going to test the code we write (think unit tests, integration, functional [e2e] as well as your approach e.g. TDD).  After the session has finished I like to perform one final task. This is to document findings/learnings and areas that require further investigation. This is normally helped by capturing notes as we go along.\nAs a side note to TDD, with modern compilers (think Roslyn in the .NET world) and even linting to a certain extent, you know if something will fail - if a reference type (.NET) does not exist yet - as your IDE will be screaming at you, so I don\u0026rsquo;t run tests that are missing these reference types (think classes and interfaces in the .NET world).\nDiscussion point I\u0026rsquo;m sure I\u0026rsquo;m not alone here when I say, having the time available for 2 Engineers to code together for skills transfer etc is a challenging one. An agile sprint doesn\u0026rsquo;t facilitate this. This is something that I often refer to as having the \u0026lsquo;space to learn\u0026rsquo;. The pressures of a sprint often, sadly, works against this. This is doubly as difficult, if your sprint is made up of technical debt, BAU, Ad-hoc etc\u0026hellip; Timeboxing \u0026lsquo;effort\u0026rsquo; into percentages doesn\u0026rsquo;t always present an obvious education path for your Engineers either. Having a day (developer day or similar) dedicated to learning also never really quite works out the way it\u0026rsquo;s meant too, plus, \u0026lsquo;a day\u0026rsquo;?! In my experience, this, and trying to cram genius into a time box also never quite works either. After all, you can\u0026rsquo;t schedule genius, in the same way, you can\u0026rsquo;t guarantee that the best Engineers are in your locality, or that the best time for Engineers to work is between 9-5.\nWhat is the answer? A mixture of all the above, at hock and at scheduled times, to ensure quality and advancement of skills.\nWhen I do speak out regarding the above, I inevitably also lead this conversation into Engineering not having the kit [hardware \u0026amp; software] they need. Engineers require the software and hardware they deem as necessary to be effective in their role. I once gave an analogy of, not giving Engineers the right kit is like giving a roller brush and a Pogo stick to Michelangelo to paint the Sistine Chapel ceiling. He\u0026rsquo;ll manage it \u0026hellip; eventually, but the attention to detail and accuracy will be woefully inadequate.\n Written mainly for me, I do hope you\u0026rsquo;ve found something useful here, and who knows, it may even help you with your engineering journey too.\n References   The pragmatic programmer\n  YAGNI\n  XP\n  Dependency inversion principle\n  OOP design patterns\n  Inversion of Control Containers and the Dependency Injection pattern\n  Write your tests\n  "});index.add({'id':5,'href':'/posts/','title':"Blog",'content':"This section contains articles form my old blogging site. Not all have been ported so will look sparse\n"});})();